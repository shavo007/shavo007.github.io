<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Tech Blog (mainly!)]]></title><description><![CDATA[Thoughts, stories and ideas.]]></description><link>https://blog.shanelee.name/</link><generator>Ghost 0.8</generator><lastBuildDate>Sat, 24 Jan 2026 20:34:06 GMT</lastBuildDate><atom:link href="https://blog.shanelee.name/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Dependency management with dependabot]]></title><description><![CDATA[How to shift security left and manage dependencies via dependabot and reduce the no. of security vulnerabilities.]]></description><link>https://blog.shanelee.name/2023/02/28/dependency-management-with-dependabot/</link><guid isPermaLink="false">d169802e-3e11-47a5-81ad-73cfcd7c3ff0</guid><category><![CDATA[github]]></category><category><![CDATA[devsecops]]></category><category><![CDATA[dependabot]]></category><category><![CDATA[owasp]]></category><category><![CDATA[security]]></category><category><![CDATA[culture]]></category><category><![CDATA[leadership]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 28 Feb 2023 03:30:31 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2023/02/guerrillabuzz-blockchain-pr-agency-SYofhg_IX3A-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.shanelee.name/content/images/2023/02/guerrillabuzz-blockchain-pr-agency-SYofhg_IX3A-unsplash.jpg" alt="Dependency management with dependabot"><p>We all know how vital shifting security left is and how engineering needs to work more closely with security. According to the Cloud Security Alliance (CSA), 70% of <a href="https://cloudsecurityalliance.org/artifacts/secure-devops-and-misconfigurations-survey-report/">security professionals and engineering teams</a> struggle to <strong>‚Äúshift left,‚Äù</strong> with many unable to recognize the formation of anti-patterns and understand/appreciate the wider impact to development, cost, governance, culture, etc.</p>

<h2 id="recognizingtheparadigmshift">Recognizing the Paradigm Shift</h2>

<p>Engineering teams that implement DevSecOps practices and automate security tooling will <mark>discover security risks earlier, saving developers time, accelerating release cycles, and shipping more secure and compliant code</mark>.</p>

<p>As I have previously <a href="https://blog.shanelee.name/2022/11/04/shifting-security-left/">discussed</a>, dependency management is an important part of securing your software supply chain. This post covers how to roll out Dependabot internally within your company to keep your dependencies up to date.</p>

<p><mark>NB: There are other tools such as Renovate and Snyk, but as Dependabot is native to the GitHub platform I am focusing the talk on this tool.</mark></p>

<p>Keeping your dependencies up to date is one of the easiest ways to keep your systems secure. The issue of supply chain security has become increasingly obvious in the past number of years, from the malicious <code>flatmap-stream</code> package to the most recent <code>log4shell</code> vulnerabilities. <a href="https://docs.github.com/en/code-security/dependabot/dependabot-alerts/about-dependabot-alerts">Dependabot</a> will alert developers when a repository is using a software dependency with a known vulnerability. By rolling out Dependabot internally to all of your repositories, you can measure, and significantly reduce, your usage of software dependencies with known vulnerabilities.</p>

<p>Everyone will remember the security breach at <a href="https://www.infoq.com/news/2017/09/struts/">Experian</a> in 2017. Hackers stole the personal details of <strong>143 million Americans</strong> from the Equifax credit report company after exploiting a security flaw in the Apache Struts framework. That is why it is so important to keep track of your dependencies and any known vulnerabilities.</p>

<p>OWASP published last year their top ten, and vulnerable and outdated components moved up to <a href="https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components/">sixth position</a>. Every organization must ensure an ongoing plan for monitoring, triaging, and applying updates or configuration changes for the lifetime of the application or portfolio. There are over 20,000 common vulnerabilities and exposures (CVEs) discovered per year in open-source and third-party code.</p>

<p><img src="https://blog.shanelee.name/content/images/2023/02/owasp.png" alt="Dependency management with dependabot"></p>

<h2 id="empoweringdeveloperstobesecurityminded">Empowering developers to be security minded</h2>

<p>According to the SANS 2022 DevSecOps Survey: <a href="https://www.deepfactor.io/sans-2022-devsecops-survey/">Creating a Culture to Significantly Improve Your Organization‚Äôs Security Posture</a>, ‚Äúmanagement buy-in‚Äù was the number one factor contributing to DevSecOps security programs‚Äô success.</p>

<p><img src="https://blog.shanelee.name/content/images/2023/02/image2.png" alt="Dependency management with dependabot"></p>

<h2 id="whatisdependabot">What is Dependabot?</h2>

<p><img src="https://blog.shanelee.name/content/images/2023/02/depend.png" alt="Dependency management with dependabot"></p>

<p>There are a number of guiding principles when evaluating tools and designing a rollout plan. For example, Does the security benefit of this new process outweigh the impact on engineering teams? How do we roll this out incrementally and gather feedback? What are our expectations for engineers, and how do we clearly communicate these expectations?</p>

<p>For Dependabot in particular, some of these questions are easy to answer. Dependabot is a native feature of GitHub, meaning, that it integrates with your engineers‚Äô current workflows on GitHub.com. By better tracking the security of your software supply chain, you will keep your software secure, which outweighs any potential impact on engineering teams.</p>

<p>When Dependabot raises pull requests, these pull requests could be for <code>security</code> or <code>version</code> updates:</p>

<ul>
<li><strong>Dependabot security updates</strong> are automated pull requests that help you update dependencies with known vulnerabilities. </li>
<li><strong>Dependabot version updates</strong> are automated pull requests that keep your dependencies updated, even when they don‚Äôt have any vulnerabilities. To check the status of version updates, navigate to the Insights tab of your repository, then Dependency Graph, and Dependabot.</li>
<li>Dependabot security updates may include <strong>compatibility scores</strong> to let you know whether updating a dependency could cause breaking changes to your project.</li>
</ul>

<h3 id="caveats">Caveats</h3>

<ul>
<li><strong>Dependabot alerts</strong> - Owners of private repositories, or people with admin access, can enable Dependabot alerts by enabling the dependency graph and Dependabot alerts for your repositories. </li>
<li><strong>Secrets</strong> - When a Dependabot event triggers a workflow, the only secrets available to the workflow are Dependabot secrets. The simplest solution is to store the token with the permissions required in an action and in a Dependabot secret with <em>identical names</em>.</li>
<li><strong><a href="https://semver.org/">Semantic versioning</a></strong> ‚Äì don‚Äôt trust that every third party dependency aligns with this!</li>
<li><strong>Vulnerability database</strong> -  GitHub's security features do not claim to catch all vulnerabilities and malware. They actively maintain <em>GitHub Advisory Database</em> and generate alerts with the most up-to-date information. However, they <em>cannot catch everything</em> or tell you about known vulnerabilities within a guaranteed time frame. These features are not substitutes for a human review of each dependency for potential vulnerabilities or any other issues, and they recommend consulting with a security service or conducting a thorough dependency review when necessary.</li>
</ul>

<h3 id="bewareofdependencyfatigue">Beware of dependency fatigue</h3>

<p>You may have heard of <a href="https://www.pagerduty.com/blog/cutting-alert-fatigue-modern-ops/">alert fatigue</a> before when you are on a support roster. Well, the same can be said for dependency management: I call it <mark>dependency fatigue</mark>! The last thing you want is for teams to not configure dependabot properly and start to ignore the PRs and alerts created or turn them off completely. So what can we do to help alleviate this? Below I provide some tips and tricks I have used in the past and some sample configs.</p>

<h3 id="tips">Tips</h3>

<ul>
<li>Design config with your team - Get the team to review the configuration and agree on the setup.</li>
<li>Understand what <code>package-ecosystem(s)</code> to enable (docker, maven, actions, terraform etc)</li>
<li>Add <code>labels</code> to your PRs to allow easier filtering</li>
<li>Prioritise the order and schedule</li>
<li>Assign ownership to who is on call</li>
<li>If your dependencies are very outdated, you might want to start with a daily schedule until the dependencies are up-to-date, and then drop back to a weekly schedule.</li>
<li>Review the changelog (look for any breaking changes announced) and make sure checks pass on your branch</li>
<li>It's good practice to have automated tests and acceptance processes in place so that checks are carried out before the pull request is merged.</li>
<li>Test and learn! üòÖ</li>
</ul>

<h3 id="sampleconfig">Sample config</h3>

<p>You can place this file <code>dependabot.yml</code> under your <code>.github</code> directory in your repo.</p>

<p>This example below showcases a repo that is interested in two eco-systems: <code>docker</code> and <code>actions</code>. I have added in a schedule for docker updates in my local timezone of Melbourne. And I have added some labels. This helps to filter your PRs if you want to focus on specific updates. The <code>open-pull-requests-limit</code> default is 5, so in this instance, I have reduced it to 2. </p>

<p><img src="https://blog.shanelee.name/content/images/2023/02/code.png" alt="Dependency management with dependabot"></p>

<p>The next example showcases a sample NodeJS project. Again you are interested in updates from two ecosystems: <code>npm</code> and <code>actions</code>. What this config is also showcasing is how you can ignore certain dependencies. You can see that it ignores packages that start with <code>aws</code>, ignores updates to <code>express</code> and patch updates for <code>all</code> dependencies. </p>

<p><img src="https://blog.shanelee.name/content/images/2023/02/code1.png" alt="Dependency management with dependabot"></p>

<p>Lastly, is a sample config for a JVM-based project. Here we are covering three package ecosystems as it's a cloud-native microservice. It covers updates to <code>maven</code>, <code>docker</code> and <code>actions</code>. </p>

<p><img src="https://blog.shanelee.name/content/images/2023/02/code3.png" alt="Dependency management with dependabot"></p>

<p>Below is a sample PR that was created by dependabot based on the config above. You can see it has added the labels of <code>dependencies</code> and <code>maven</code>. It provides the release notes and changelog for you to review. </p>

<p><img src="https://blog.shanelee.name/content/images/2023/03/Screen-Shot-2023-03-02-at-10-06-07-am.png" alt="Dependency management with dependabot"></p>

<p>If you want to know more about the configuration options for dependabot, check out the doc <a href="https://docs.github.com/en/code-security/dependabot/dependabot-version-updates/configuration-options-for-the-dependabot.yml-file">here</a></p>

<h2 id="rollout">Rollout</h2>

<p>Dependabot, like other GitHub Advanced Security features, can be enabled for all repositories within an organization from the organization‚Äôs administration page. Depending on how you create new GitHub repositories (infra as code anyone??), you can also enable upon creation by <a href="https://docs.github.com/en/organizations/keeping-your-organization-secure/managing-security-settings-for-your-organization/managing-security-and-analysis-settings-for-your-organization#enabling-or-disabling-a-feature-automatically-when-new-repositories-are-added">default</a>. That way there is no major impact on teams and a staged rollout is not needed. Frequent company-wide comms are recommended.</p>

<p>The aim is to answer, clearly and succinctly, the most important questions in your communications: <code>What are we doing? Why are we doing this? When are we doing this? Lastly, what do I need to do?</code> The last question was key. Make it clear that you are rolling out Dependabot organization-wide to understand your current risk and that, while we encourage service owners to upgrade dependencies, you are not expecting every Dependabot alert to be fixed right away.</p>

<p>Running an engineering workshop on dependency management and dependabot can help educate teams on the importance of this initiative and shifting security left. Showcasing how to manage security and version updates for your services via dependabot will set the foundations and kick-start the conversation. Like any change management framework, it is important to <code>communicate/educate</code>, <code>build the capability</code> and <code>change the mindsets</code>.</p>

<h2 id="remediation">Remediation</h2>

<p>Once you had Dependabot enabled for all your GitHub repositories, you could measure the general trend of Dependabot alerts across the company. At an org level under the <code>insights</code> tab, you have a bird's eye view of dependencies and open security advisories. You can filter by most critical and then switch your focus from measuring the current state to working with repository owners to upgrade their dependencies.  By leveraging insights, you can ensure that you focus your remediation efforts on repositories that are running in production, where a vulnerable dependency could present a risk to your company and your users. As I mentioned earlier, <strong>"management buy-in"</strong> is extremely important in incorporating dependency management into your security program. Otherwise, you will end up competing with feature delivery and it will fall further down the pecking order. Security trumps customer centricity.</p>

<p><img src="https://blog.shanelee.name/content/images/2023/03/Screenshot-2023-02-28-at-3-33-51-pm.png" alt="Dependency management with dependabot"></p>

<p>If you have a platform engineering team you could go even further and build or use a nice internal service catalog tool like <a href="https://backstage.spotify.com/">backstage</a> and hook into GitHub graphQL API to track this. As part of measuring system health and running an internal security program, you can work with your stream-aligned teams to define <em>SLOs</em> and <em>SLIs</em> regarding timeframes to resolve. An example could be: <br>
<strong>"Critical and high severity vulnerable dependencies are fixed within 30 days"</strong>.</p>

<p><img src="https://blog.shanelee.name/content/images/2023/02/Screenshot-2023-02-28-at-11-42-52-am.png" alt="Dependency management with dependabot"></p>

<p>It is important to acknowledge that <code>not all Dependabot alerts</code> can be actioned immediately. Instead, assign a realistic grace period for service owners to remediate Dependabot alerts before marking a metric as failing.</p>

<h2 id="outcomes">Outcomes</h2>

<p>The goal here is to reduce the no. of Dependabot alerts over time for your services. But do not regard this as a <strong>once-off task</strong>. Rather, you need to follow the dependabot alert metrics continuously over time and intervene if you see them trending in the wrong direction. This will help you have a better picture of your system health across the company and prioritise work.</p>

<p>Happy updating! üòÄ</p>]]></content:encoded></item><item><title><![CDATA[Shifting security left]]></title><description><![CDATA[<p>As part of adopting CD (Continuous Delivery) and embracing a DevOps culture, shifting security left is one of the capabilities needed to drive high software delivery and organisational performance.</p>

<p>Tech companies have identified that security is EVERYONE's responsibility. ie. Security is a distributed ownership model. What <strong>shifting left</strong> means is</p>]]></description><link>https://blog.shanelee.name/2022/11/04/shifting-security-left/</link><guid isPermaLink="false">fedd7be5-31ee-4340-9199-bc0eb054c558</guid><category><![CDATA[github]]></category><category><![CDATA[culture]]></category><category><![CDATA[aws]]></category><category><![CDATA[devops]]></category><category><![CDATA[devsecops]]></category><category><![CDATA[security]]></category><category><![CDATA[owasp]]></category><category><![CDATA[dependabot]]></category><category><![CDATA[renovate]]></category><category><![CDATA[team-topologies]]></category><category><![CDATA[checkov]]></category><category><![CDATA[actions]]></category><category><![CDATA[tflint]]></category><category><![CDATA[terraform]]></category><category><![CDATA[tfsec]]></category><category><![CDATA[devX]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Fri, 04 Nov 2022 08:43:55 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2022/11/matthew-henry-fPxOowbR6ls-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.shanelee.name/content/images/2022/11/matthew-henry-fPxOowbR6ls-unsplash.jpg" alt="Shifting security left"><p>As part of adopting CD (Continuous Delivery) and embracing a DevOps culture, shifting security left is one of the capabilities needed to drive high software delivery and organisational performance.</p>

<p>Tech companies have identified that security is EVERYONE's responsibility. ie. Security is a distributed ownership model. What <strong>shifting left</strong> means is including concerns such as security earlier in the development lifecycle.</p>

<blockquote>
  <p>The principles of shifting left also apply to security, not only to operations. It‚Äôs critical to prevent breaches before they can affect users, and to move quickly to address newly discovered security vulnerabilities and fix them.</p>
</blockquote>

<p>In software development, there are at least these four activities: design, develop, test, and release. In a traditional software development cycle, testing (including security testing), happens after development is complete. This typically means that a team discovers significant problems, including architectural flaws, that are expensive to fix.</p>

<p>After defects are discovered, developers must then find the contributing factors and how to fix them. In complex production systems, it's not usually a single cause; instead, it's often a series of factors that interact to cause a defect. Defects involving security, performance, and availability are expensive and <a href="https://linearb.io/blog/engineering-metrics-benchmarks-what-makes-elite-teams/#:~:text=PR%20review%20process.-,Rework%20Rate,-%3A">time-consuming</a> to remedy; they often require architectural changes. The time required to find the defect, develop a solution, and fully test the fix are unpredictable. This can further push out delivery dates.</p>

<p>Research from <a href="https://www.infoq.com/news/2022/10/google-devops-2022/">Accelerate State of DevOps Research and Assessment 2022</a> (DORA) shows that teams can achieve better outcomes by making security a part of everyone's daily work, instead of testing for security concerns at the end of the process. <mark>One key finding is that the largest predictor of an organization's software security practices was not technical but instead <code>cultural</code>.</mark> Leveraging Westrum's organizational topology, high-trust, low-blame cultures focused on performance were significantly more likely to adopt emerging security practices than low-trust, high-blame cultures that focused on power or rules. For more on organisational culture and my assessment of the book Accelerate check out my previous <a href="https://blog.shanelee.name/2022/05/15/book-review-accelerate-and-my-experience-in-high-performing-organisations/">blog post</a></p>

<h2 id="howtoimplementimprovedsecurityquality">How to implement improved security quality</h2>

<h3 id="getinfosecinvolvedinsoftwaredesign">Get Infosec involved in software design</h3>

<p>The Infosec team should get involved in the design phase for all projects. As you can imagine, the ratio of engineers/builders to security within a company rates pretty high. So companies these days need to rethink how to approach shifting security left. I found this great article on how a company adopted the team's topology approach and looked to re-frame security as an <a href="https://www.securitydifferently.com/minimum-viable-security-knowledge-and-team-topologies-for-security/">enablement team</a>.</p>

<blockquote>
  <p>This mode helps reduce gaps in capabilities and is suited to situations where one or more teams would benefit from the active help of another team facilitating or coaching an aspect of their work. This is the primary operating mode of an enabling team and provides support to many other teams. It can also help discover gaps or inconsistencies in existing components and services used.</p>
</blockquote>

<p>Educating teams on <a href="https://thenewstack.io/the-latest-owasp-top-10-looks-a-lot-like-the-old-owasp/">OWASP 10</a> and <a href="https://apisecurity.io/encyclopedia/content/owasp/owasp-api-security-top-10.htm">API security</a> is extremely important. Understanding AuthZ, excessive data exposure and insufficient logging and monitoring remain high on the list. </p>

<h3 id="educationprograms">Education Programs</h3>

<p>Companies like AWS have created a guardian program whereby every stream-aligned team needs a security guardian (or what I like to call a security ‚Äúchampion‚Äù). So you cannot deliver value to your customers (release to prod) until these criteria are met. No guardian, no shipping software! üòõ</p>

<p>The program consists of workshops and training from the security team (OWASP, threat modelling, AWS Well-Architected framework (security pillar) etc). Due to the ratio of engineers to security, they regard the security team as an <code>enablement</code> function. This ties back to team topologies again and its four-team types (enablement/stream aligned/platform/complicated sub-system).</p>

<blockquote>
  <p>Instead of ‚Äúus versus them,‚Äù make security part of development from the start, and encourage day-to-day collaboration between both teams.</p>
</blockquote>

<h2 id="securityandcompliance">Security and compliance</h2>

<h3 id="threatmodelling">Threat modelling</h3>

<p><img src="https://blog.shanelee.name/content/images/2022/11/cupcake-256x256.png" alt="Shifting security left"></p>

<p>I have observed companies adopting a threat modelling framework as part of the assessment. <a href="https://www.appsecengineer.com/blog/what-is-threat-modeling-how-do-you-learn-it">Frameworks</a> such as <code>STRIDE</code> and <code>PASTA</code> are quite popular. OWASP for example has a modelling tool called <a href="https://owasp.org/www-project-threat-dragon/">Threat Dragon</a> that supports frameworks like <code>STRIDE</code>.</p>

<p>Threat modelling is a set of techniques to help you identify and classify potential threats during the development process ‚Äî but I want to emphasize that this is not a <strong>one-off activity</strong> only done at the start of projects.</p>

<p>This is because throughout the lifetime of any software, new threats will emerge and existing ones will continue to evolve thanks to external events and ongoing changes to requirements and architecture. This means that threat modelling needs to be <strong>repeated periodically</strong> ‚Äî the frequency of repetition will depend on the circumstances and will need to consider factors such as the cost of running the exercise and the potential risk to the business. When used in conjunction with other techniques, such as establishing cross-functional security requirements to address common risks in the project's technologies and using automated security scanners, threat modelling can be a powerful asset.</p>

<p>AWS provides <a href="https://catalog.workshops.aws/threatmodel/en-US">workshops</a> on threat modelling and educating teams on the approach. GitHub talks about its threat modelling <a href="https://github.blog/2020-09-02-how-we-threat-model/">process</a> and how it brings more improved communication between security and the engineering teams and how they are able to "shift left".</p>

<h3 id="bugbountyprograms">Bug bounty programs</h3>

<p><img src="https://blog.shanelee.name/content/images/2022/11/download.jpeg" alt="Shifting security left"></p>

<p>Many companies are now adopting and investing in bug bounty programs. Bug bounty programs give you continuous, real-time vulnerability insights across your expanding digital attack surface so you can eliminate critical threat ‚Äúblind spots‚Äù and strengthen your security posture.</p>

<p>Check out this interview with one of the security researchers in the <a href="https://github.blog/2022-10-28-cybersecurity-spotlight-on-bug-bounty-researcher-ahacker1/">Github Security Bug Bounty program</a> to understand more about their background and how they keep up with the latest vulnerability trends. One program I have used in the past is <a href="https://www.bugcrowd.com/">BugCrowd.</a></p>

<p><a href="https://en.wikipedia.org/wiki/Capture_the_flag_(cybersecurity)">CTF</a> (<strong>Capture The Flag</strong>) competitions are also a great way to keep engineers engaged (security mindset).</p>

<p>Adopting an <em>NFR (Non-Functional Requirements) checklist</em> in the design phase helps to deliver quality at speed; incorporating performance, security and reliability. One example of compliance is the <strong>4-eyes principle</strong> where at least <em>two</em> approvers are needed before merging the PR. <a href="https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-code-owners">CODEOWNERS</a> helps automatically assign individuals (or teams) to review PRs. Below I will elaborate further on the tooling that can enable and automate compliance as part of the development lifecycle early.</p>

<h2 id="securityapprovedtools">Security-approved tools</h2>

<p>Security-approved tools help to integrate and automate many tooling into your day-to-day life as an engineer. I have noticed over time the rapid rise of Devsecops and tooling that can help achieve this. The areas that I used and have observed include (<em>non-exhaustive list</em>):</p>

<h3 id="infraascodescanning">Infra as code scanning</h3>

<ul>
<li><a href="https://github.com/aquasecurity/tfsec">Tfsec</a> - Uses static analysis of your terraform code to spot potential misconfiguration. Can be easily incorporated into your CI pipeline.</li>
</ul>

<p><img src="https://blog.shanelee.name/content/images/2022/11/example-github-pr-check.png" alt="Shifting security left"></p>

<ul>
<li><a href="https://www.checkov.io/">Checkov by Bridgecrew</a> - Can scan results across platforms such as Terraform, CloudFormation, Kubernetes, Helm, ARM Templates and Serverless framework.</li>
<li><a href="https://snyk.io/product/infrastructure-as-code-security/">Snyk</a> supports many formats and can also detect drifts post-deployment.</li>
<li>Policy enforcement for terraform using <a href="https://www.conftest.dev/">conftest</a> - Doordash have a great <a href="https://doordash.engineering/2022/09/20/how-doordash-ensures-velocity-and-reliability-through-policy-automation/">article</a> on how they ensure reliability and velocity through conftest and Atlantis. </li>
</ul>

<h3 id="bots">Bots</h3>

<p><img src="https://blog.shanelee.name/content/images/2022/11/1_MARssNdoQ3kG1mdZEDV3iA.png" alt="Shifting security left"></p>

<p>I am a big fan of chatbots! I watched a presentation from a company recently that built multiple chatbots on shifting security left and helping with remediation. </p>

<p>They had an intern with an interest in python and he created some very nifty chatbots. <br>
Behind the scenes, they use Cloud conformity from Trend Micro. Now when an engineer is naughty and maybe clickops and opens a security group to the world or an s3 bucket, the bot will alert the channel and notify of the issue in real-time (they only broadcast on high and mediums). You can then ask the bot also who made the change. It will then scan Cloudtrail, find the IAM user in question and link it back to the slack handle of the user and post back to the channel who's to ‚Äúblame‚Äù (more in the sense of git blame than to literally blame!). The bot could also perform auto-remediation. This is just an example of the opportunities that exist with chatbots.</p>

<h3 id="lintingcompliance">Linting (Compliance)</h3>

<p><img src="https://blog.shanelee.name/content/images/2022/11/icon-512.png" alt="Shifting security left"></p>

<p>Linters are tools used to flag programming errors, bugs, stylistic errors and suspicious constructs. Examples include:</p>

<ul>
<li><a href="https://eslint.org/">Eslint</a> - is an open-source project that helps you find and fix problems with your JavaScript code.</li>
<li><a href="https://github.com/terraform-linters/tflint">TFlint</a> is a pluggable terraform linter that finds possible errors (like invalid instance types) for Major Cloud providers (AWS/Azure/GCP) and enforces best practices.</li>
<li><a href="https://github.com/rhysd/actionlint">Actionlint</a> is a static checker for GitHub workflow files. I use the VSCode extension extensively.</li>
<li><a href="https://github.com/adrienverge/yamllint">yamllint</a> is a linter for YAML files.</li>
<li><a href="https://stoplight.io/open-source/spectral">oaslint</a> - Spectral is an OAS linter to enforce best practices for API design of Open API Specs. I have introduced this linter before at organisations to align with companies' API guidelines and catch any inconsistencies early in the development lifecycle.</li>
</ul>

<p>If you are using GitHub actions check out this awesome list of <a href="https://github.com/sdras/awesome-actions#linting">linters</a> that the community recommends.</p>

<h3 id="securityversionupdates">Security &amp; Version updates</h3>

<p><img src="https://blog.shanelee.name/content/images/2022/11/dependabot.png" alt="Shifting security left"></p>

<p>OWASP published last year their top ten, and <strong>vulnerable and outdated components</strong> moved up to <a href="https://owasp.org/Top10/A06_2021-Vulnerable_and_Outdated_Components">sixth position</a></p>

<p><a href="https://docs.github.com/en/code-security/dependabot/dependabot-alerts/about-dependabot-alerts">Dependabot</a> and <a href="https://docs.renovatebot.com/#why-use-renovate">Renovate</a> are two tools I have used in the past for dependency updates and vulnerability management. Dependabot is part of the GitHub ecosystem now and is maturing every day. But I do miss the nice presets that Renovate has and how you can group certain dependencies too.</p>

<p>One lesson I have learnt from automating this process is to be aware of <em>dependency fatigue</em>. Work with the team on your schedule and what dependencies are the highest priority. Otherwise, you could end up becoming overwhelmed with PRs and start to ignore updates!</p>

<p><a href="https://github.blog/2022-05-25-how-we-use-dependabot-to-secure-github/">Github</a> talks about how they use dependabot internally to secure their own platform.</p>

<p>OWASP <a href="https://owasp.org/www-project-dependency-check/">dependency checker</a> is also a useful tool to run as part of your CI pipeline to identify security vulnerabilities.</p>

<h3 id="codescanningsast">Code scanning (SAST)</h3>

<p><img src="https://blog.shanelee.name/content/images/2022/11/dev-cycle.png" alt="Shifting security left"></p>

<p><a href="https://www.sonarqube.org/">SonarQube</a> is a self-managed, automatic code review tool that systematically helps you deliver Clean Code. As a core element of their solution, SonarQube integrates into your existing workflow and detects issues in your code to help you perform continuous code inspections of your projects. The tool analyses <em>30+ different programming languages</em> and integrates into your CI pipeline and DevOps platform to ensure that your code meets high-quality standards.</p>

<h3 id="containerscanning">Container scanning</h3>

<ul>
<li><p><a href="https://docs.paloaltonetworks.com/prisma/prisma-cloud/prisma-cloud-admin-compute/tools/twistcli">Twistlock</a> is now part of Palo Alto‚Äôs Prisma Cloud offering and is one of the leading container security scanning solutions.</p></li>
<li><p><a href="https://docs.snyk.io/products/snyk-container/how-snyk-container-works">Snyk</a> also provides a similar product offering to scan images.</p></li>
</ul>

<p>I hope you have found this list of security tools useful. Head over to <a href="https://landscape.cncf.io/card-mode?category=security-compliance,security">CNCF</a> to find more.</p>

<p>Suffice it to say are we still shifting left? Is it realistic to expect developers to take on the burdens of security and infrastructure provisioning, as well as writing their applications? Is platform engineering the answer to saving the DevOps dream?? I will leave you with that thought! üòâ</p>]]></content:encoded></item><item><title><![CDATA[Book review: Accelerate]]></title><description><![CDATA[Review of Accelerate, high performing organizations and the four key metrics of change lead time, deployment frequency, mean time to recover, and change failure rate.]]></description><link>https://blog.shanelee.name/2022/05/15/book-review-accelerate-and-my-experience-in-high-performing-organisations/</link><guid isPermaLink="false">9d648e15-7d5f-473f-bc43-bcb2d8e354d8</guid><category><![CDATA[accelerate]]></category><category><![CDATA[devops]]></category><category><![CDATA[culture]]></category><category><![CDATA[leadership]]></category><category><![CDATA[CD]]></category><category><![CDATA[continuous delivery]]></category><category><![CDATA[high-performing]]></category><category><![CDATA[transformation]]></category><category><![CDATA[tracing]]></category><category><![CDATA[docker]]></category><category><![CDATA[github]]></category><category><![CDATA[aws]]></category><category><![CDATA[cloud]]></category><category><![CDATA[google]]></category><category><![CDATA[CI]]></category><category><![CDATA[fourkeymetrics]]></category><category><![CDATA[lean]]></category><category><![CDATA[agile]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 15 May 2022 07:27:53 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2022/05/jacek-dylag-fZglO1JkwoM-unsplash--1-.jpg" medium="image"/><content:encoded><![CDATA[<h1 id="overview">Overview</h1>

<img src="https://blog.shanelee.name/content/images/2022/05/jacek-dylag-fZglO1JkwoM-unsplash--1-.jpg" alt="Book review: Accelerate"><p>In the book, <em>Accelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology Organizations</em>, Dr. Nicole Forsgren, Jez Humble, and Gene Kim studied what made separated strong performing technology organizations from their less effective counterparts.</p>

<p>The book summarises <strong>4 years</strong> of rigorous research from years of State of DevOps Reports, built upon <strong>23,000 datasets</strong> from over <strong>2,000 unique companies</strong> all around the world. The organizations studied included start-ups and enterprises, profit and not-for-profit organisations, and companies that were born digital alongside those that had to undergo digital transformation.</p>

<h2 id="fourkeymetrics">Four key metrics</h2>

<p>The research identified that just <strong>Four Key Metrics</strong> distinguish the performance of various organisations. These ‚ÄúNorth Star‚Äù metrics serve as indicators of overall software engineering health. <br>
These metrics aren‚Äôt ‚ÄúThe Goal‚Äù of a business, but organisations that did well against these metrics had higher rates of <strong>profitability üíµ, market share, and customer satisfaction</strong>. In other words; they allowed organisations to <strong>experiment faster üöÄ, ship reliably, and prevent burnout</strong>.</p>

<p>The Four Key Metrics were as follows:</p>

<ul>
<li><mark>Cycle Time (Change Lead Time)</mark> - Time to implement, test, and deliver code for a feature (measured from first commit to deployment)</li>
<li><mark>Deployment Frequency</mark> - Number of deployments in a given duration of time</li>
<li><mark>Change Failure Rate (CFR)</mark> - Percentage of deployments which caused a failure in production</li>
<li><mark>Mean Time to Recovery (MTTR)</mark> - Mean time it takes to restore service after production failure</li>
</ul>

<p><img src="https://blog.shanelee.name/content/images/2022/05/Screen-Shot-2022-05-15-at-6-10-07-pm.png" alt="Book review: Accelerate"></p>

<h2 id="whythesemetrics">Why these metrics</h2>

<p>Where is the evidence that these metrics help organisational performance though? How do you convince your boss that these things matter??</p>

<h3 id="martinfowleronacceleratemetrics">Martin Fowler on Accelerate metrics</h3>

<p>The picture they paint is compelling. They describe how effective IT delivery organizations take about an hour to get code from <strong>"committed to mainline"</strong> to <strong>"running in production"</strong>, a journey lesser organizations take months to do. They, thus, update their software many times a day instead of once every few months, increasing their ability to use software to explore the market, respond to events, and release features faster than their competition. This huge increase in responsiveness does not come at a cost in stability, since these organizations find their updates cause failures at a fraction of the rate of their less-performing peers, and these failures are usually fixed within the hour. Their evidence refutes the bimodal IT notion that you have to choose between <strong>speed and stability</strong> instead, speed <strong>depends</strong> on stability, so good IT practices give you both.</p>

<p>So, as you may expect, I'm delighted that they've put this book into production, and I will be recommending it willy-nilly over the next few years. (I've already been using many bits from its drafts in my talks.) However, I do want to put in a few notes of caution. They do a good job of explaining why their approach to surveys makes them a good basis for their data. However, they are still surveys that capture subjective perceptions, and I wonder how their population sample reflects the general IT world. I'll have more confidence in their results when other teams, using different approaches, are able to confirm their reasoning. The book already has some of this, as the work done by Google on team cultures provides further evidence to support their judgment on how important a <strong>Westrum-generative organizational culture</strong> is for effective software teams.</p>

<h3 id="nicoleforsgrenonhowdevopsmetricscorrelatewithorganisationalperformance">Nicole Forsgren On How DevOps Metrics Correlate With Organisational Performance</h3>

<p>Rigorous appraisal of these Four Key Metrics has shown that higher performers are <strong>2x more likely</strong> to meet their <strong>commercial goals</strong> <em>(productivity, profitability, market share, number of customers)</em> and their <strong>non-commercial goals</strong> <em>(quantity of products or services, operating efficiency, customer satisfaction, quality of products or services and achieving organisational or mission goals)</em>. Indeed, companies which do well under these DevOps metrics have a <strong>50%</strong> higher market cap growth over 3 years. üìà</p>

<p><strong>NOTE</strong>: These findings will apply whether you‚Äôre using a traditional ‚Äúwaterfall‚Äù methodology (also known as <em>gated, structured, or plan-driven</em>) and just beginning your technology transformation, or whether you have been implementing Agile and DevOps practices for years.</p>

<h2 id="flawsinpreviousattemptstomeasureperformance">Flaws in previous attempts to measure performance</h2>

<p>Before I jump into what <em>capabilities</em> a company needs to focus on to achieve these metrics, I wanted to explain how companies have in the past measured performance.</p>

<p>There have been many attempts to measure the performance of software teams. Most of these measurements focus on productivity. In general, they suffer from <strong>two</strong> drawbacks.</p>

<ul>
<li>First, they focus on <strong>outputs rather than outcomes</strong>.</li>
<li>Second, they focus on <strong>individual</strong> or <strong>local measures</strong> rather than a team or global ones.</li>
</ul>

<h3 id="loclinesofcode">LOC (lines of code)</h3>

<p>Measuring productivity in terms of lines of code has a long history in software. Some companies even required developers to record the lines of code committed per week. However, in reality, we would prefer a <em>10-line solution</em> to a <em>1,000-line solution</em> to a problem. Rewarding developers for writing lines of code leads to bloated software that incurs <strong>higher maintenance costs</strong> and <strong>higher costs of change</strong>. </p>

<p>Ideally, we should reward developers for solving business problems with the minimum amount of code and it's even better if we can solve a problem without writing code at all or by deleting code (perhaps by a business process change). However, minimizing lines of code isn't an ideal measure either. At the extreme, this too has its drawbacks. Accomplishing a task in a single line of code that no one else can understand is less desirable than writing a few lines of code that are easily understood and maintained.</p>

<h3 id="velocity">Velocity</h3>

<p>With the advent of Agile software development came a new way to measure productivity: <strong>velocity</strong>. In many schools of Agile, problems are broken down into stories. Stories are then estimated by developers and assigned a number of "points" representing the relative effort expected to complete them. At the end of an iteration, the total number of points signed off by the customer is recorded; this is the team's velocity. </p>

<p>Velocity is designed to be used as a capacity planning tool; for example, it can be used to extrapolate how long it will take the team to complete all the work that has been planned and estimated. However, some managers have also used it as a way to measure team productivity, or even to compare teams.</p>

<p>Using velocity as a productivity metric has several flaws. First, velocity is a relative and team-dependent measure, not an absolute one. Teams usually have significantly different contexts which render their velocities incommensurable. Second, when velocity is used as a productivity measure, teams inevitably work to game their velocity. They inflate their estimates and focus on completing as many stories as at the expense of collaboration with other teams (which might decrease their velocity and increase the other team's velocity, making them look bad). Not only does this destroy the utility of velocity for its intended purpose, but it also inhibits collaboration between teams.</p>

<p>I have seen this in the past myself, <strong>"gaming"</strong> the system and engineers aiming to deliver as my story points as you can within a sprint. Teams celebrating burn down charts and how many points they delivered compared to other squads...</p>

<p>This obfuscates the underlying quality of the code. And what tech debt or improvements you could have made but decided not to. If you don't have the right leadership in place or a peer review process, then this can unfold very quickly. And not being a <a href="https://www.stepsize.com/blog/how-to-be-an-effective-boy-girl-scout-engineer">good girl scout or boy scout</a>. They talk about this more in "system thinking" and not having that <a href="https://medium.com/@Smrimell/it-s-a-trap-systems-traps-in-software-development-dc6341022795#:~:text=you%20actually%20incentivising%3F-,Drift%20to%20low%20performance,-Drift%20to%20low">growth mindset</a>.</p>

<h3 id="utilisation">Utilisation</h3>

<p>Finally, many organizations measure utilization as a proxy for productivity. The problem with this method is that <strong>high utilization</strong> is only good up to a point. Once utilization gets above a certain level, there is no spare capacity (or "slack") to absorb unplanned work, changes to the plan, or improvement work. This results in longer lead times to work. Queue theory in math tells us that as utilization approaches 100%, lead times approach infinity-in other words, once you get to very high levels of utilization, it takes teams exponentially longer to get anything done. Since lead time is a measure of how fast work can be completed, and is a productivity metric that doesn't suffer from the drawbacks of the other metrics we've seen, it's essential that we manage utilization to balance it against lead time in an economically optimal way.</p>

<h2 id="performancemetrics">Performance metrics</h2>

<p>Below you can see the different groupings of teams and how they perform. You can see the elite teams, they're able to deploy on-demand. Their Cycle Time is less than one day. Their MTTR, mean time to restore, is less than one hour. Change failure rate is in the zero to 15% range. You can really see how the Cycle Time, in particular, the lead time for changes, how varies throughout those four columns. It's less than one day for the elite teams. For the high-performance teams, it goes down to that one day and one week. Medium teams one week and one month and the low performing teams are really between that one month and six months mark.</p>

<p><img src="https://blog.shanelee.name/content/images/2022/05/Screen-Shot-2022-05-15-at-6-14-09-pm.png" alt="Book review: Accelerate"></p>

<p><img src="https://blog.shanelee.name/content/images/2022/05/Screen-Shot-2022-05-15-at-6-08-18-pm.png" alt="Book review: Accelerate"></p>

<p>I will highlight next the main capabilities and provide real use-cases that I have observed in my experience:</p>

<h3 id="capabilities">Capabilities</h3>

<p><img src="https://blog.shanelee.name/content/images/2022/05/Accelerate--1-.jpg" alt="Book review: Accelerate"></p>

<p><img src="https://blog.shanelee.name/content/images/2022/05/Screen-Shot-2022-05-15-at-6-23-34-pm.png" alt="Book review: Accelerate"></p>

<h2 id="thoughtsandfeedback">Thoughts and feedback</h2>

<p>As highlighted above, there is a lot of investment needed to achieve CD (Continuous Delivery) and become high-performing. Whenever you are asked about what makes a high-performing team, people say trust and a sense of purpose are important. And they are, don't get me wrong. But you also need engineers with a growth mindset, discipline, and technical acumen to make it a reality.</p>

<p>But regarding trust and a sense of purpose, there are numerous ways to achieve this within your team. Team topologies go in-depth into this discussion. They talk about <em>Dunbar's trust number</em> and having <em>two size pizza teams</em> (normally 5-8 depending on the complexity of the domain). </p>

<p><img src="https://blog.shanelee.name/content/images/2022/05/Screen-Shot-2022-05-18-at-11-48-54-am.png" alt="Book review: Accelerate"></p>

<p>Then there is also <em>Tuckman's model approach</em> to team development: <strong>forming-storming-norming-performing</strong>. </p>

<p><img src="https://blog.shanelee.name/content/images/2022/05/Screen-Shot-2022-05-16-at-2-35-06-pm.png" alt="Book review: Accelerate"></p>

<p>Regular social events with your team and <a href="https://www.atlassian.com/team-playbook/plays">team playbooks</a> are a great way to break the ice and start to understand eachothers personalities. Especially when you work in a culturally diverse environment. Working in Europe and the Asia Pacific, I have worked with many nationalities and am always keen to understand more about their culture. </p>

<p><strong>Sense of purpose</strong> - having that clear vision and objective for the team and getting alignment is crucial to the success of the team. Running exercises such as <a href="https://miro.com/guides/team-charter/">team charter</a> and <a href="https://miro.com/templates/raci-matrix/">RACI model</a> help with this.</p>

<p>But now onto the fun part; the technical aspects. But I will let you in with a little secret first:</p>

<blockquote>
  <p>Engineers love shipping software! üòõ</p>
</blockquote>

<p><img src="https://blog.shanelee.name/content/images/2022/05/Screen-Shot-2022-05-13-at-10-41-01-am.png" alt="Book review: Accelerate"></p>

<p>I remember years ago working in London for a mobile apps company. There was an engineer who I worked with that hadn't shipped code in over six months! (due to indecisiveness on the client's part). I could see how demoralized she looked not able to see her hard work go live. So always remember, engineers, <strong>love to ship software</strong>. </p>

<p>I have worked in high-performing teams in the past where <strong>change lead time</strong> was <strong>&lt; 90 mins</strong> and <strong>deployment frequency daily</strong>. I have seen firsthand how it has a direct impact on job satisfaction.  Having engaged employees doing meaningful work drives <strong>business value</strong>.</p>

<h3 id="deploymentpipeline">Deployment pipeline</h3>

<p>Continuous delivery does not happen overnight. You need to methodically design your CI pipeline, and focus on your technical capabilities to make it a reality. Working with CI servers such as buildkite, GitHub actions or circleCI allows you to design your pipeline as code. We have moved on from the clickops days of TeamCity or bamboo where engineers are not across how CI/CD works. To be able to visualise it as code, can really help even new team members understand and get up to speed so much faster. </p>

<p>A lot of these CI servers now support plugins and integrations to expedite your delivery and simplify the work needed in the pipeline. Accessing secrets from secret manager, running docker-compose, generating test reports, and more. Continually looking to optimise and improve your pipeline to run as fast as possible through certain software techniques/enhancements or agent upgrades is an ongoing task.</p>

<h3 id="testautomation">Test Automation</h3>

<p>Software quality and test automation are the next phases. Working in cross-functional teams can be a mindset change for a lot of engineers. It is well publicised now the mandate at AWS of decoupling architecture and building <em>two size pizza teams</em> that build, run, and support your workloads. Working on teams with <strong>24/7 support</strong>, your mindset changes into how you perceive the quality of your code. Getting a change "out the door" to achieve a deadline is frowned upon. And those traditional measures of <em>velocity</em> and <em>high utilisation</em>.</p>

<p>If you are on 24/7 support, you don't want to be woken up at 3 AM in the morning right! So your mindset changes. You are more aware of the quality of your code and of others. Working on trunk-based development makes engineers' life also easier when reviewing PRs.</p>

<p>There is a lot of talk about shifting left and specifically shifting testing left. Having a healthy automated test suite and being aware of anti-patterns such as ice cream cones and <a href="https://www.thoughtworks.com/en-au/insights/blog/introducing-software-testing-cupcake-anti-pattern">cupcake</a> is very important. </p>

<p>You cannot achieve CD by having a <strong>heavy</strong> layer of e-2-e tests. This will delay your change lead time and overtime those tests will become <a href="https://www.thoughtworks.com/en-au/insights/blog/transitioning-conventional-shift-left-testing">flaky</a> and less reliable. </p>

<p><img src="https://blog.shanelee.name/content/images/2022/05/ice_cream_cone.png" alt="Book review: Accelerate"></p>

<p>Shifting left to integration tests (that can run on your local machine) is the key. QAs focus less on manual tests and free them up to look at exploratory testing or chaos engineering for example. But many high-performing teams these days hire more engineers to focus on test automation and shifting testing and performance left rather than hire QAs. Working with the teams to educate them on the benefits of shifting left and changing that mindset of how to test a feature is extremely valuable.</p>

<p>Now companies are given the space to revisit their environments and look to <a href="https://www.infoq.com/news/2022/05/removing-staging-environments/">consolidate</a>. Not only does this improve change lead time and MTTR. But also the maintenance overhead of trying to keep envs in <a href="https://12factor.net/dev-prod-parity">parity</a>. Cost savings on your cloud provider can be significant too.</p>

<h3 id="featuretoggles">Feature toggles</h3>

<p>Feature toggles are a great way to separate deploy from release when looking to consolidate envs. There are several commercial options out there such as launch darkly and other <a href="https://github.com/Unleash/unleash">open source</a> options to help with this. <a href="https://github.blog/2021-04-27-ship-code-faster-safer-feature-flags/">Github</a> provides a great example of how they ship frequently and safely to production with the use of feature toggles.</p>

<h3 id="cloud">Cloud</h3>

<p>Cloud infrastructure is adopted quite heavily now by companies. And there is an expectation now for engineers to upskill in cloud providers such as AWS or Google Cloud. Working in cross-functional teams in the past we managed our own AWS accounts and infrastructure. There are many tools of choice for infra as code such as cloudformation, terraform, AWS CDK, and pulumi. </p>

<p>Companies also these days are identifying areas of the platform to design and build to provide self-service capabilities to the delivery teams. I have worked with teams that treated the platform as a product and its internal customers as delivery teams and looked to streamline their development process by providing custom tooling with well-documented APIs and support. </p>

<p><img src="https://blog.shanelee.name/content/images/2022/05/Screen-Shot-2022-05-16-at-5-57-35-pm.png" alt="Book review: Accelerate"></p>

<p>Deploying static sites and deploying app workloads to Kubernetes are examples I have seen. The key is to balance <strong>autonomy with alignment</strong> and still allow teams to choose their own tooling when needed. They call it providing a "thinnable viable" platform.</p>

<p>Slack has a great article on how they adopted this <a href="https://slack.engineering/applying-product-thinking-to-slacks-internal-compute-platform/">approach</a>.</p>

<h3 id="proactivemonitoring">Proactive monitoring</h3>

<p>Once you are happy with your pipeline, code quality, and test automation, the next area to invest in is monitoring. I am not going into the specifics here but you have many tools at your disposal now for logging and monitoring namely Splunk, sumologic, datadog, new relic, and appdymanics. </p>

<p>In a loosely coupled architecture having distributed tracing and a common log format really helps when troubleshooting. Investing time in an incident management playbook and alert notifications too. <mark>Just be aware of <a href="https://www.pagerduty.com/blog/reduce-alert-fatigue/!">alert fatigue</a></mark></p>

<h3 id="leanmanagementandproductdevelopment">Lean Management and product development</h3>

<p>With this investment in CD, your team can now afford to streamline its change approval process. Gone are the days that you need to present to CAB for production deployments. This allows the team to adopt a peer-review process via pair programming or intra-team code review without sacrificing speed or quality. </p>

<p>One common requirement I hear from companies is segregation of duties which is a common regulatory requirement. This can now be achieved by using peer review to meet the goal of segregation of duties, with <strong>reviews</strong>, <strong>comments</strong>, and <strong>approvals</strong> captured in the team's development platform as part of the development process.</p>

<p>The time and attention of those in leadership and management positions are freed up to focus on more strategic work. This transition, from <em>gatekeeper</em> to <em>process architect and information beacon</em>, is consistent with the practices of organizations that excel at software delivery performance.</p>

<blockquote>
  <p>CAB idea is a form of risk management theater: we check boxes so that when something goes wrong, we can say at least we followed the process. At best, the process only introduces time delays and handoffs.</p>
</blockquote>

<p>Lean product management and visibility of work from the business all the way through to customers is a new approach that I have seen. Team experimentation, visibility into customer feedback, and <a href="https://www.producttalk.org/continuous-discovery/">continous discovery</a> are ways for the team and engineers to feel empowered to contribute on what opportunities to work on next, and become more engaged in product discovery and engaging with customers.</p>

<h3 id="shiftingsecurityleft">Shifting security left</h3>

<p>Security is another CFR that can be shifted left. Areas I have worked on in the past are the automated management of dependencies via tools such as <a href="https://github.blog/2020-06-01-keep-all-your-packages-up-to-date-with-dependabot/">dependabot</a>, <a href="https://snyk.io/">snyk</a> or <a href="https://www.whitesourcesoftware.com/free-developer-tools/renovate/">renovate</a>. </p>

<p>We devised a schedule on what frequency and priority we needed to be notified of, for our repos to prevent dependency fatigue. Having multiple automated PRs created every day for patches is not an efficient use of time! Tools such as snyk also provide infra scanning and container scanning. Engaging teams such as architecture and security earlier in the development lifecycle is also recommended.</p>

<h3 id="cultureandtransformativeleadership">Culture and transformative leadership</h3>

<p>And lastly, but most importantly <strong>culture</strong>. Creating a climate for learning is a key investment within a company and investing in your people. <br>
Some steps that I have advocated for and adopted in the past include:</p>

<ul>
<li>Creating a training budget</li>
<li>Ensuring the team has the resources to engage in informal learning and space to explore ideas</li>
<li>Blameless culture and making it safe to fail</li>
<li>Creating opportunities and spaces to share information through lightning talks or guilds</li>
<li>Make resources available for continued education such as meetups and attending conferences</li>
</ul>

<p>As this survey has highlighted, investing in culture that values <em>learning</em> contributes significantly to software delivery performance.</p>

<p>As a leader, it is important to really invest in culture and be able to measure it. Your work is never done, always looking to continually improve. Providing the <strong>technical vision</strong> and <strong>where the company is going</strong>, <strong>supportive leadership</strong>, and <strong>challenging team members</strong> (intellectual stimulation) are the areas I continually focus on.</p>

<h2 id="pitfallstowatchoutfor">Pitfalls to watch out for</h2>

<ul>
<li><p><strong>Focus on capabilities and not maturity</strong> The key to successful change is measuring and understanding the right things with a focus on capabilities - not on maturity.
Maturity models focus on helping an org <code>"arrive"</code> at a mature state and declare themselves done with their journey. Alternatively, capability models focus on helping an org continually improve and progress, realising that the technological and business landscape is ever-changing.</p></li>
<li><p><strong>Treating transformation as a one-time project.</strong> in high-performing organizations, getting better is an ongoing effort and part of everybody's daily work. However, many transformation programs are treated as large-scale, one-time events in which everyone is expected to rapidly change the way they work and then continue on with business as usual. Teams are not given the capacity, resources, or authority to improve the way they work, and their performance gradually degrades as the team's processes, skills, and capabilities become an ever poorer fit for the evolving reality of the work. You should think of technology transformation as an important value-delivery part of the business, one that you won't stop investing in. After all, do you plan to stop investing in customer acquisition or customer support?</p></li>
<li><p><strong>Treating transformation as a top-down effort.</strong> In this model, organizational reporting lines are changed, teams are moved around or restructured, and new processes are implemented. Often, the people who are affected are given little control of the changes and are not given opportunities for input. This can cause stress and lost productivity as people learn new ways of working, often while they are still delivering on existing commitments. When combined with the poor communication that is frequent in transformation initiatives, the top-down approach can lead to employees becoming unhappy and disengaged. It's also uncommon for the team that's planning and executing the top-down transformation to gather feedback on the effects of their work and to make changes accordingly. Instead, the plan is implemented regardless of the consequences.</p></li>
</ul>

<h2 id="googleproject">Google project</h2>

<p>Google provides a very useful project on <a href="https://github.com/GoogleCloudPlatform/fourkeys">github</a> showcasing how to capture these metrics. They use services on GCP such as Pub/Sub and BigQuery to capture this data in real-time. I would love to see an example of using Kafka and ksqlDB to mirror this approach (hack day idea!)</p>

<p><img src="https://raw.githubusercontent.com/GoogleCloudPlatform/fourkeys/main/images/dashboard.png" alt="Book review: Accelerate"></p>

<p>To find out where your company sits, run this <a href="https://www.devops-research.com/quickcheck.html">quick survey</a></p>

<p>I hope you have found this post insightful and helped create a conversation in your organization about where you stand today. And where you want to go.</p>

<p>As my main man Kelsey said:</p>

<p><img src="https://blog.shanelee.name/content/images/2022/05/Screen-Shot-2022-05-15-at-5-38-24-pm.png" alt="Book review: Accelerate"></p>]]></content:encoded></item><item><title><![CDATA[Performance testing with k6]]></title><description><![CDATA[<p>I started looking into API performance testing again recently. As part of delivering an API to production, performance testing is crucial to see how your API performs under load ( more on that shortly)</p>

<p>You might have heard of the ridiculous load that Shopify handled over <strong>BFCM</strong> (Black Friday/Cyber Monday)</p>]]></description><link>https://blog.shanelee.name/2021/12/15/performance-testing-with-k6/</link><guid isPermaLink="false">35b5397d-d54f-4305-9f95-32975623f93f</guid><category><![CDATA[testing]]></category><category><![CDATA[oas]]></category><category><![CDATA[grafana]]></category><category><![CDATA[docker]]></category><category><![CDATA[api]]></category><category><![CDATA[k6]]></category><category><![CDATA[perf]]></category><category><![CDATA[apifirst]]></category><category><![CDATA[typescript]]></category><category><![CDATA[performance]]></category><category><![CDATA[api first]]></category><category><![CDATA[javascript]]></category><category><![CDATA[golang]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Wed, 15 Dec 2021 06:03:00 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2021/12/marc-olivier-jodoin-NqOInJ-ttqM-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.shanelee.name/content/images/2021/12/marc-olivier-jodoin-NqOInJ-ttqM-unsplash.jpg" alt="Performance testing with k6"><p>I started looking into API performance testing again recently. As part of delivering an API to production, performance testing is crucial to see how your API performs under load ( more on that shortly)</p>

<p>You might have heard of the ridiculous load that Shopify handled over <strong>BFCM</strong> (Black Friday/Cyber Monday) weekend. They executed and ran performance testing on their systems well in advance as far back as July to make sure they were well prepared for <code>"game-day"</code>.</p>

<p><blockquote class="twitter-tweet"><p lang="en" dir="ltr">‚è™ Rewinding for a moment ‚Äî preparing for an event of this scale doesn&#39;t happen overnight. In anticipation of BFCM 2021 we began load testing back in July! To better simulate real global traffic we spread out our load generation across <a href="https://twitter.com/googlecloud?ref_src=twsrc%5Etfw">@GoogleCloud</a>&#39;s global network. <a href="https://t.co/5oXqFOadae">pic.twitter.com/5oXqFOadae</a></p>&mdash; Shopify Engineering (@ShopifyEng) <a href="https://twitter.com/ShopifyEng/status/1465806698954772489?ref_src=twsrc%5Etfw">November 30, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p>Now there are many tools that are out there in the wild. I have worked with <code>JMeter</code> quite extensively, <code>Gatling</code> and <code>Artillery</code>. But as always, I like to do some research and see what is happening in that space.</p>

<p>Enter <code>k6</code>!</p>

<p>I follow ThoughtWorks <a href="https://www.thoughtworks.com/radar/tools?blipid=202010078">tech radar</a> and they identified this new tool. So I decided to have a play around.</p>

<h2 id="k6">K6</h2>

<p><img src="https://raw.githubusercontent.com/grafana/k6/master/assets/logo.svg" alt="Performance testing with k6"></p>

<ul>
<li>Created in 2016 by loadImpact</li>
<li>Acquired by Grafana in 2021</li>
<li>14.6k Github stars ‚≠êÔ∏è</li>
<li>Built in golang, test scripts using <del>Javascript</del> Typescript üòâ</li>
<li>Multiple choices for storage (datadog/kafka/cloudwatch/Prometheus/JSON/CSV)</li>
<li>Converters (HAR/Postman/OAS)</li>
<li>Github action</li>
<li>Test builder/browser recorder (low code options)</li>
<li>Aligns with ‚ÄúAPI first‚Äù approach</li>
</ul>

<p><mark>I mention "API first" here as that is a common practice for organizations now.</mark> In my previous post, I talked about another API first tool to use when <a href="https://blog.shanelee.name/2021/08/29/mocking-a-rest-api-the-api-first-way-with-mockoon/">mocking APIs</a>.</p>

<h3 id="installation">Installation</h3>

<p>There are several ways to install k6 depending on your operating system or system environment. But in this article, I will only touch on two of them, MacBook and Docker.</p>

<p>You can install k6 on a MacBook by using brew</p>

<pre><code class="language-bash">brew install k6
</code></pre>

<p>or via docker</p>

<pre><code class="language-bash">docker pull loadimpact/k6  
</code></pre>

<p>You can find the full explanation here <a href="https://k6.io/docs/getting-started/installation/">https://k6.io/docs/getting-started/installation/</a></p>

<h3 id="typescripttemplate">Typescript template</h3>

<p>As I mentioned above, the test scripts can be defined in javascript. But I have decided from the beginning to use typescript instead for static type checking. I have created a typescript template on <a href="https://github.com/shavo007/k6-demo">Github</a> that can be easily re-used by anyone. It is built on the good work that k6 did with some enhancements.</p>

<p><img src="https://blog.shanelee.name/content/images/2021/12/Screen-Shot-2021-12-16-at-5-23-12-pm.png" alt="Performance testing with k6"></p>

<h4 id="rationale">Rationale</h4>

<p>While JavaScript is great for a myriad of reasons, one area where it falls short is type safety and developer ergonomics. It's perfectly possible to write JavaScript code that will look <code>OK</code> and behave <code>OK</code> until a certain condition forces the executor into a faulty branch.</p>

<p>While it, of course, still is possible to shoot yourself in the foot with TypeScript as well, it's significantly harder. Without adding much overhead, TypeScript will:</p>

<ul>
<li>Improve the ability to safely refactor your code.</li>
<li>Improve readability and maintainability.</li>
<li>Allow you to drop a lot of the defensive code previously needed to make sure consumers are calling functions properly.</li>
</ul>

<h3 id="lifecycle">Lifecycle</h3>

<p>The four distinct life cycle stages in a k6 test are <code>"init"</code>, <code>"setup"</code>, <code>"VU"</code> and <code>"teardown"</code></p>

<h4 id="initandvustages">Init and VU stages</h4>

<p>Scripts must contain, at the very least, a default function - this defines the entry point for your VUs, similar to the <code>main()</code> function in many other languages.</p>

<p>Code inside default is called "VU code", and is run over and over for as long as the test is running. Code outside of it is called "init code", and is run only once per VU (Virtual User).</p>

<p>A VU will execute the default function from start to end in sequence. Nothing out of the ordinary so far, but here's the important part; once the VU reaches the end of the default function it will loop back to the start and execute the code all over.</p>

<h4 id="setupandteardownstages">Setup and teardown stages</h4>

<p>Beyond the required <code>init</code> and <code>VU</code> stages, which is code run for each VU, k6 also supports test-wide setup and teardown stages, like many other testing frameworks and tools. The setup and teardown functions, like the default function, need to be exported functions. But unlike the default function setup and teardown are only called once for a test. setup is called at the beginning of the test, after the init stage but before the VU stage (default function), and teardown is called at the end of a test, after the VU stage (default function).</p>

<p>You might have noticed the function signature of the default function and teardown function takes an argument, which we here refer to as data.</p>

<p>This data will be whatever is returned in the setup function, so a mechanism for passing data from the setup stage to the subsequent VU and teardown stages.</p>

<p><em>Further below I will showcase how in <code>setup</code> you can inject in an access token to the default function for an API secured by OAuth2</em>.</p>

<h3 id="basicexample">Basic example</h3>

<pre><code class="language-javascript">import { sleep, check } from "k6";  
import { Options } from "k6/options";  
import http, { Response } from "k6/http";  
import { padStart } from "lodash";  
import { textSummary } from "./helper";  
import { Trend } from "k6/metrics";

// 1. init code

console.log(padStart("Hello TypeScript!", 20, " "));

//custom define metric
const durationInSeconds = new Trend("duration_in_seconds");

export let options: Options = {  
  vus: 5, //no. of concurrent virtual users
  duration: "5s",
  discardResponseBodies: true, //discard response bodies to improve perf
  //if you want to fail the whole load test use thresholds
  thresholds: {
    http_req_failed: ["rate&lt;0.01"], // http errors should be less than 1%
    http_req_duration: ["p(95)&lt;350"], // 95% of requests should be below 350ms
  },
  // httpDebug: "true",
  userAgent: "K6GreetingsDemo/1.0",
};

export function setup() {  
  //setup is called once off
  // 2. setup code
}

export default () =&gt; {  
  // 3. VU code

  let baseUrl = __ENV.BASE_URL ?? "http://localhost:8090";
  let url = `${baseUrl}/greetings`;
  const res: Response = http.get(url, {
    tags: { team: "team-label", api: "greetings" },
  });
  check(res, {
    "status is 200": () =&gt; res.status === 200
  });
  // we know that the duration is in millisecond
  // but for demonstration purposes, we convert it to second
  durationInSeconds.add(res.timings.duration / 1000);
  sleep(1);
};

export function handleSummary(data: any) {  
  console.log("Preparing the end-of-test summary...");

  // Send the results to some remote server or trigger a hook

  return {
    stdout: textSummary(data, { indent: " ", enableColors: true }), // Show the text summary to stdout...
  };
}
</code></pre>

<p>First, we imported the dependencies on top. k6 has types (<code>@k6/types</code>) that are nice when developing in typescript. </p>

<p><em>Note that in the background, k6 doesn‚Äôt run in NodeJS, since in general JavaScript is not well suited for high performance. It‚Äôs written in <code>Go</code> to achieve the desired high-performance testing.</em></p>

<p>The test itself is running inside the exported <code>default</code> function. This part of the code is what we usually called as <code>VU Code</code>. So, the test is running once and uses only one virtual user (think of this as a real user, but simulated) by default, but you can change that using options. <em>We discussed the lifecycle earlier</em>.</p>

<p>So in the example above, we are simulating 5 users over 5 secs. We have set thresholds for the test in relation to response time and error rate. We have added in an additional <code>trend</code> to showcase how that is defined. No setup is involved here. We have one check that the status is <code>200</code>.</p>

<p>To run this we can run the script <code>yarn bundle</code>. This will transpile to JS using babel and bundled with webpack. The outputted file will be located in the <code>dist</code> dir. <br>
Now run locally:</p>

<pre><code class="language-bash">k6 run dist/greetings.js  
</code></pre>

<p>You‚Äôll see the result of the test right away on the terminal. Something similar to this.</p>

<p><img src="https://raw.githubusercontent.com/shavo007/k6-demo/main/assets/greeting-test-results.png" alt="Performance testing with k6"></p>

<p>You can see the built-in metrics that were outputted and also your custom metrics <code>duration_in_seconds</code>. To read more about the metrics go to <a href="https://k6.io/docs/using-k6/metrics/">https://k6.io/docs/using-k6/metrics/</a></p>

<p>Now I have only outputted this to stdout. But you may decide you want to pipe it into a time series database like influxdb, APM such as datadog or appdynamics which is all possible. For more on the supported outputs check out <a href="https://k6.io/docs/getting-started/results-output/">https://k6.io/docs/getting-started/results-output/</a>. If you want to look at an example of using influxdb and grafana check out <a href="https://github.com/shavo007/k6-demo#load-testing-with-influxdb-and-grafana">here</a>.</p>

<h3 id="apifirsttools">API first tools</h3>

<p>Now I mentioned earlier that it supports the "api first" approach. In the past, I have looked to use tools such as open api generator to generate server stubs based on the OAS. </p>

<p><em><a href="https://slack.engineering/how-we-design-our-apis-at-slack/">Slack</a> have talked about this approach recently regarding their APIs. And <a href="https://blog.stoplight.io/api-first-vs.-api-design-first-a-comprehensive-guide">stoplight</a> has some great resources on the API first approach too.</em> So I looked into open api generator and saw they actually supported k6. Let's try that out next.</p>

<h4 id="oask6">OAS + k6</h4>

<p>I have designed a sample <a href="https://github.com/shavo007/k6-demo/blob/main/oas3.yaml">OAS</a> already, called <code>greetings API</code> and I wanted to generate the k6 script using open api generator. You can follow along by cloning this <a href="https://github.com/shavo007/k6-demo">repo</a></p>

<pre><code class="language-bash">docker pull openapitools/openapi-generator-cli

docker run --rm -v ${PWD}:/local openapitools/openapi-generator-cli generate \  
    -i /local/oas3.yaml \
    -g k6 \
    -o /local/k6-oas3/ \
    --skip-validate-spec
</code></pre>

<p>Here I pull down the docker image and then run the cmd by mounting my oas file and using the generator type <code>k6</code>. This then generates some boilerplate test script code inside the dir <code>k6-oas3</code>.</p>

<p><mark><strong>NB:</strong> This will need to be refined and enhanced afterward but is a great starting point</mark>.</p>

<h4 id="openapitotypescript">open api to typescript</h4>

<p>Another tool I found useful is this node module <a href="https://www.npmjs.com/package/openapi-typescript">open api typescript</a>. </p>

<p>I read an article recently on how <a href="https://codeascraft.com/2021/11/08/etsys-journey-to-typescript/">etsy</a> migrated from JS to TS and they used this tool to generate the types which can save a lot of time and aligns with the "API first" approach.</p>

<p>So in the example below I will showcase how to use the types when running a load test against <a href="https://developer.bpaygroup.com.au/validate-bpay-payment">bpay API</a>. This API is secured by <code>OAuth2 client credentials</code> grant type also. You will see that I use the <code>setup</code> function to inject in the access_token that's needed when running the test.</p>

<p>As I am defining my script in typescript and this API method is a post, I can use <code>openapi-typescript</code> to generate the types for me from the OAS. This saves me a lot of time not having to define my own interfaces.</p>

<p>To generate the types:</p>

<pre><code class="language-bash">npx openapi-typescript bpay/oas3.yaml --output src/bpay/schema.ts  
</code></pre>

<p>This generates the types in the <code>bpay</code> dir.</p>

<p>Now let's have a look at the test script for bpay API.</p>

<pre><code class="language-javascript">import http from "k6/http";  
import { group, check, sleep } from "k6";  
import { PaymentPaymentMethodEnum } from "./bpay";  
import { Options } from "k6/options";  
import { getToken, Options as BpayOptions } from "./helper";  
import { components } from "./bpay";

type Payment = components["schemas"]["Payment"];  
type PaymentItem = components["schemas"]["PaymentItem"];

const BASE_URL = "https://sandbox.api.bpaygroup.com.au/payments/v1";  
// Sleep duration between successive requests.
// You might want to edit the value of this variable or remove calls to the sleep function on the script.
const SLEEP_DURATION = 0.1;  
// Global variables should be initialized.

export let options: Options = {  
  vus: 1, //no. of concurrent visual users
  duration: "1s",
  httpDebug: "true",
};

export function setup() {  
  const options: BpayOptions = {
    clientId: `${__ENV.CLIENT_ID}`,
    clientSecret: `${__ENV.CLIENT_SECRET}`,
    domain: "https://sandbox.api.bpaygroup.com.au",
  };
  try {
    return getToken(options);
  } catch (error) {
    return "";
  }
}

//desctructing assigment here and explicit type annotation
export default function ({ access_token = "" }: { access_token: string }) {  
  group("/validatepayments", () =&gt; {
    let url = BASE_URL + `/validatepayments`;

    const payment: Payment = {
      billerCode: "565572",
      crn: "651234567890123",
      amount: 234.83,
      paymentMethod: PaymentPaymentMethodEnum.Debit,
      settlementDate: "2017-10-23",
    };
    const payment1: Payment = {
      billerCode: "1313",
      crn: "1230",
      amount: 1045.98,
      paymentMethod: PaymentPaymentMethodEnum.Debit,
      settlementDate: "2017-11-06",
    };
    const paymentItem: PaymentItem = { tid: "1", payment };
    const paymentItem1: PaymentItem = { tid: "2", payment: payment1 };
    const payments: Array&lt;PaymentItem&gt; = [];
    payments.push(paymentItem);
    payments.push(paymentItem1);
    const payload = { payments };
    let params = {
      headers: {
        "Content-Type": "application/json",
        Accept: "application/json",
        Authorization: `Bearer ${access_token}`,
      },
    };
    let request = http.post(url, JSON.stringify(payload), params);
    check(request, {
      "ok":
        (r) =&gt; r.status === 200,
    });
    sleep(SLEEP_DURATION);
  });
}
</code></pre>

<p>Let's look at the imports first. You can see we are importing bpay types that were auto-generated. These types are subsequently used when building up the payload inside the <br>
default function. I'm running against bpays sandbox env and it's a simple <a href="https://k6.io/docs/test-types/smoke-testing/">smoke test</a>. </p>

<p><em>I will shed some light on the different types of performance tests you can run later</em>.</p>

<p>Now in the setup function, you can see I've defined a helper fn to get the access token. This performs a request against the sandbox env with your client id and secret and returns the access token. I can then return this value which is then passed into the default fn to be used in the Authorisation header.</p>

<p><mark><strong>NB</strong> I am using env variables here for client_id and secret as they should not be committed into src code</mark>.</p>

<p>The default fn constructs the payload and submits the post request to the API with the bearer token.</p>

<p>I have not defined any thresholds in this example but just a simple check on the HTTP status response. <br>
You can see how easy it is to define your smoke test and utilize the tools at your disposal if you adopt the "api first" approach.</p>

<h3 id="browserbuilder">Browser builder</h3>

<p>Now, let's say I'm a bit lazy üòÖ and want to go the low-code approach! k6 does have a <a href="https://k6.io/docs/test-authoring/recording-a-session/">browser recorder extension</a> for chrome and firefox that can record your interactions and download a har (HTTP archive file). This then can be imported into <a href="https://k6.io/docs/test-authoring/test-builder/">k6 cloud</a> and provide a nice GUI to construct the script. Once you are happy with the script you can copy this locally and use it with the CLI.</p>

<p>Builder:</p>

<p><img src="https://raw.githubusercontent.com/shavo007/k6-demo/main/assets/builder.png" alt="Performance testing with k6"></p>

<p>Corresponding script:</p>

<p><img src="https://raw.githubusercontent.com/shavo007/k6-demo/main/assets/app-k6-io-Performance-testing-for-developers-like-unit-testing-for-performance.png" alt="Performance testing with k6"></p>

<h3 id="typesoftesting">Types of testing</h3>

<p>The examples above showcased smoke tests running against your environment(s) (or what is now called "shifting perf left"). But it is possible to perform many types of tests using k6, each type serving a different purpose.</p>

<h4 id="smoketest">Smoke test</h4>

<blockquote>
  <p>Smoke test is a regular load test, configured for a minimal load.</p>
</blockquote>

<p>You want to run a smoke test as a <code>sanity check</code> every time you write a new script or modify an existing script.</p>

<p><img src="https://raw.githubusercontent.com/shavo007/k6-demo/main/assets/smoke-test.png" alt="Performance testing with k6"></p>

<ul>
<li>can be run as part  of CI pipeline - "shifting perf left"</li>
<li>More ideal for microservices</li>
</ul>

<p>k6 options for smoke test:</p>

<p><img src="https://raw.githubusercontent.com/shavo007/k6-demo/main/assets/smoke.png" alt="Performance testing with k6"></p>

<h4 id="loadtest">Load test</h4>

<blockquote>
  <p>Load Testing is primarily concerned with assessing the current performance of your system in terms of concurrent users or requests per second.</p>
</blockquote>

<p><img src="https://raw.githubusercontent.com/shavo007/k6-demo/main/assets/load-test.png" alt="Performance testing with k6"></p>

<p>If you need some understanding about the amount of traffic your system is seeing on average and during peak <br>
hours. <strong>ie.</strong> How to configure performance thresholds.</p>

<p><mark>If your system crashes under a load test, it means that
your load test has morphed into a stress test!</mark></p>

<p>K6 options for load test:</p>

<p><img src="https://raw.githubusercontent.com/shavo007/k6-demo/main/assets/load.png" alt="Performance testing with k6"></p>

<h4 id="stresstest">Stress test</h4>

<blockquote>
  <p>Stress testing is to assess the availability and stability of the system under heavy load (think HA).</p>
</blockquote>

<p>To execute a proper stress test, you need a tool to push the system over its normal operations, to its limits, and beyond the <em>breaking point</em>.‚Äù</p>

<p><img src="https://raw.githubusercontent.com/shavo007/k6-demo/main/assets/stress-test.png" alt="Performance testing with k6"></p>

<p>You typically want to stress test an API or website to <br>
determine:</p>

<ul>
<li>How your system will behave under extreme
conditions.  </li>
<li>What the maximum capacity of your system is in
terms of users or throughput.  </li>
<li>The breaking point of your system and its failure
mode.  </li>
<li>If your system will recover without manual
intervention after the stress test is over.</li>
</ul>

<p><em>Companies use this form of testing for <strong>black Friday sales</strong> for example</em>.</p>

<p>K6 options for stress test:</p>

<p><img src="https://raw.githubusercontent.com/shavo007/k6-demo/main/assets/stress.png" alt="Performance testing with k6"></p>

<h4 id="soaktest">Soak test</h4>

<blockquote>
  <p>A soak test uncovers performance and reliability issues stemming from a system being under
  pressure for an extended period.</p>
</blockquote>

<p><img src="https://raw.githubusercontent.com/shavo007/k6-demo/main/assets/soak-test.png" alt="Performance testing with k6"></p>

<p>You typically run this test to:</p>

<ul>
<li>Verify that your system doesn't suffer from bugs or
memory leaks, which result in a crash or restart after <br>
several hours of operation.  </li>
<li>Verify that expected application restarts don't lose
requests.  </li>
<li>Find bugs related to race-conditions that appear
sporadically.  </li>
<li>Make sure your database doesn't exhaust the
allotted storage space and stops.  </li>
<li>Make sure your logs don't exhaust the allotted disk
storage.  </li>
<li>Make sure the external services you depend on don't
stop working after a certain amount of requests are <br>
executed.</li>
</ul>

<p>K6 options for soak test:</p>

<p><img src="https://raw.githubusercontent.com/shavo007/k6-demo/main/assets/soak.png" alt="Performance testing with k6"></p>

<h3 id="conclusion">Conclusion</h3>

<p>If you are comfortable developing in javascript or typescript, k6 is a breeze to use. There are so many options you could use, scenarios if you need advanced user behavior, saving the test result in a CSV or JSON file, having a dashboard for presentation using grafana, etc.</p>

<p>As I demoed earlier, if you adopt the "API first" approach there are tools you can use such as <code>open API generator</code> and <code>openapi-to-typescript</code> that can help improve the development experience.</p>

<p>Happy testing! </p>]]></content:encoded></item><item><title><![CDATA[Mocking a REST API the "API first" approach with Mockoon]]></title><description><![CDATA[How to mock an API with mockoon. Showcase using open API spec and mockoon to mock an API and run with docker.]]></description><link>https://blog.shanelee.name/2021/08/29/mocking-a-rest-api-the-api-first-way-with-mockoon/</link><guid isPermaLink="false">a64b3164-634b-4cd1-8761-2af6a3c1c2c1</guid><category><![CDATA[swagger]]></category><category><![CDATA[docker]]></category><category><![CDATA[openapi]]></category><category><![CDATA[oas]]></category><category><![CDATA[github]]></category><category><![CDATA[api]]></category><category><![CDATA[rest]]></category><category><![CDATA[mock]]></category><category><![CDATA[stub]]></category><category><![CDATA[mockoon]]></category><category><![CDATA[insomnia]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 29 Aug 2021 07:27:00 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2021/08/chris-ensminger-gWo-hfRotrI-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.shanelee.name/content/images/2021/08/chris-ensminger-gWo-hfRotrI-unsplash.jpg" alt="Mocking a REST API the "API first" approach with Mockoon"><p>Recently, I have spent considerable time researching and analyzing the tooling available for "API first". At the core of this eco-system is the OAS (Open API specification) or interface as we normally like to call it. </p>

<h2 id="oasopenapispecification">OAS (Open API Specification)</h2>

<p>OAS (or what was commonly known as swagger spec) is the industry standard for defining REST interfaces. I had previously worked with many others such as RAML and Blueprint, but it's good to see we have a winner! <a href="https://github.com/OAI/OpenAPI-Specification/releases/tag/3.1.0">Latest</a> release of OAS (V3.x) includes webhook support and the latest JSON schema draft. Open API is now under the governance of the Linux Foundation. The OpenAPI Specification was originally based on the Swagger Specification, donated by SmartBear Software.</p>

<h2 id="mocking">Mocking</h2>

<p>The term <strong>"mock"</strong> for a lot of developers will have unit-testing connotations. In unit-testing, a mock is a fake implementation of a class or function, which accepts the same arguments as the real thing. It might return something pretty similar to the expected output, and different test cases might even modify those returns to see how the code under test works.</p>

<p>This is almost exactly the concept here, just at a HTTP level instead. This is done using a "mock server", which will respond to the expected endpoints, error for non-existent endpoints, often even provide realistic validation errors if a client sends it an invalid request.</p>

<p>So today, I am going to talk about mocking REST APIs. Anyone that has worked in cross-functional teams before would be very used to mocking APIs for local development. If for example, your team consisted of front and back-end devs, normally the BE devs would aim to design the interface upfront (API first) and provide a mock API to FE devs to commence development in parallel. </p>

<p>Why? <br>
Both streams of work should occur in parallel, rather than sequentially. Less waterfall!</p>

<p>But there are many other benefits of mocking APIs such as:</p>

<ul>
<li>Showcasing to stakeholders the interactions as part of the API design process</li>
<li>Showcase to external consumers</li>
<li>Use as a sandbox on the dev portal</li>
<li>Integration testing on CI ("shift testing left")</li>
<li>Performance testing on CI ("shift perf left")</li>
</ul>

<h3 id="tools">Tools</h3>

<p>I have used many frameworks and tools in the past to mock APIs. When I developed with typescript or nodeJS, a framework I used heavily was expressJS. Other tools out there include mountebank or wiremock. But now there is a new breed of mock API tools that are <strong>OAS compliant</strong>. Two that I have found recently are prism from stoplight and Mockoon.</p>

<h2 id="mockoon">Mockoon</h2>

<p><img src="https://blog.shanelee.name/content/images/2021/08/Mockoon.svg" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<blockquote>
  <p>Mockoon lets you mock an API in seconds</p>
</blockquote>

<p>Some features include:</p>

<ul>
<li>Intuitive interface to create your mock API and run anywhere via CLI</li>
<li>Integrates with your workflow - Compatible with the OpenAPI specification, Mockoon integrates perfectly with your existing applications and API design workflow.</li>
<li>Advanced features and tackle the most complex situation with HTTP requests recording, proxying, integration testing, etc.</li>
<li>Complex rules system and dynamic body templating</li>
<li>Powerful forwarding and debugging</li>
</ul>

<p><insert img="" here=""></insert></p>

<h2 id="creatingourfirstapiwithmockoon">Creating our first API with Mockoon</h2>

<h3 id="step1installation">Step1. Installation</h3>

<p>Mockoon is available on the three major operating systems: Windows, macOS, and Linux. <br>
You can install the native app <a href="https://mockoon.com/download/">here</a>.</p>

<h3 id="step2createyourfirstmockapi">Step 2. Create your first mock API</h3>

<p>After launching the application for the first time, you will find a demo mock API, also called "environment" in Mockoon. You can keep it and build from here or create a new one. To create a new mock API, open the collapsible environments menu on the left and press the blue "plus" button:</p>

<p><img src="https://blog.shanelee.name/content/images/2021/08/Screen-Shot-2021-08-27-at-11-42-09-am.png" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<h3 id="step3createyourfirstapiroute">Step 3. Create your first API route</h3>

<p>The newly created mock API already includes a route on <code>/</code>. You can modify it by setting up the method and path of your choice.</p>

<p>You can also create a new endpoint by clicking on the blue "plus" button at the top of the endpoint list:</p>

<p><img src="https://blog.shanelee.name/content/images/2021/08/Screen-Shot-2021-08-27-at-11-55-50-am-5.png" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<h3 id="step4apiendpointconfiguration">Step 4. API endpoint configuration</h3>

<p>You can further customize your endpoint by adding a custom header and the following sample body (which makes use of Mockoon's templating system).</p>

<h3 id="step5runandcallmockapi">Step 5. Run and call Mock API</h3>

<p>The last step is to run your mock API. For this, click on the green "play" arrow in the header:</p>

<p>Your mock server is now available on <code>http://localhost:3001</code> (but also on <code>http://127.0.0.1</code> and all your local network adapters).</p>

<p>You can do a test call to the following URL <code>http://localhost:3001/tutorials</code> using your favorite tool (here using <a href="https://insomnia.rest/">Insomnia</a>) and see the returned response:</p>

<p><img src="https://blog.shanelee.name/content/images/2021/08/Screen-Shot-2021-08-27-at-11-59-39-am-3.png" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<pre><code class="language-bash">curl --request POST \  
  --url http://localhost:3001/tutorials
</code></pre>

<p>So now you have got the hang of it, let's look at its more advanced features and its "API first" support.</p>

<h2 id="apifirstapproach">API first approach</h2>

<p>So let's import a sample OAS file with Mockoon.</p>

<h3 id="step1importoas">Step 1 Import OAS</h3>

<p>Open the app and go to <code>Import/Export &gt; Swagger/Open API &gt; Import Swagger v2/Open API v3</code></p>

<p>I have defined a sample OAS file <a href="https://github.com/shavo007/mockoon-demo/blob/main/oas3.yaml">here</a> that you can use to follow along. Once you import, it now should look like this.</p>

<p><img src="https://blog.shanelee.name/content/images/2021/08/Screen-Shot-2021-08-27-at-12-22-57-pm.png" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<p>You can see that for each route, it has already been set up for you all the responses you have defined in the spec and the examples. You can easily toggle on random responses or sequential responses based on the route (<strong>NB: This will disable the rules tho</strong>)</p>

<p><img src="https://blog.shanelee.name/content/images/2021/08/Screen-Shot-2021-08-27-at-12-36-20-pm.png" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<p>It is even smart enough to convert certain fields <code>weatherType</code> and <code>status</code> to its templating language. How cool is that! And again, based on previous steps you can start the server and test it out via Insomnia.</p>

<p><img src="https://blog.shanelee.name/content/images/2021/08/Screen-Shot-2021-08-29-at-4-34-35-pm.png" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<p>Insomnia is such a powerful app also as it supports the "API first" approach. I can import in the exact same OAS file and test the endpoints. (You will start to see a recurring trend here of "API first" tools üòÖ) </p>

<h3 id="step2runmockanywhere">Step 2 Run mock anywhere</h3>

<p>Ok, so now we are happy with the mock responses, how can I run this anywhere? </p>

<h3 id="introducingmockooncli">Introducing Mockoon CLI üéâüéâüéäü•≥</h3>

<blockquote>
  <p>Mockoon's perfect complement for all your headless and automated environments.</p>
</blockquote>

<p>Mockoon CLI Supports all Mockoon's features, Lightweight and fast, and allows you to Run your mocks everywhere.</p>

<p>NB: Also available as a Docker image, run your mock APIs in Github Actions or on your favorite CI platform!</p>

<p>The CLI is a companion application to Mockoon's main interface designed to receive an exported Mockoon data file.</p>

<p>It has been written in JavaScript/TypeScript and uses some great libraries like <code>oclif</code> and <code>PM2</code>. One of the benefits of using PM2 is that you can easily manage your running mock APIs through the CLI or by using PM2 commands if you are used to them.</p>

<h3 id="step3installmockooncli">Step3. Install mockoon CLI</h3>

<p>I installed using nodeJS</p>

<pre><code class="language-bash">npm install -g @mockoon/cli
</code></pre>

<h3 id="step4exportyourmockapitoajsonfile">Step 4. Export your mock API to a JSON file</h3>

<p>To export your environment, open the <code>"Import/export"</code> application menu and choose <code>"Mockoon's format" -&gt; "Export all environments to a file (JSON)"</code> or <code>"Export current environment to a file (JSON)"</code>.</p>

<p>You can then select a location to save the export data file. Let's name the file <code>Greetings_Mockoon.json</code>.</p>

<h3 id="step5startyourmockapi">Step 5. Start your mock API</h3>

<p>After exporting your data file, you are ready to run your API mock with the CLI.</p>

<p>In your terminal, navigate to the folder where your export data file is and run the following command:</p>

<p><code>mockoon-cli start --data ./Greetings_Mockoon.json</code></p>

<p>If you want to use a remotely hosted file, you can also provide a URL to the --data flag like this:</p>

<p><code>mockoon-cli start --data https://domain.com/data-export.json</code></p>

<h3 id="step6manageyourapimock">Step 6. Manage your API mock</h3>

<p>After running one or more API server mock, you might want to check their health and statuses. To do so you can type <code>mockoon-cli list</code>:</p>

<pre><code class="language-bash">shanelee at shanes-MacBook-Air in ~/projects/mockoon-demo on main [?]  
$ mockoon-cli list
 Name                 Id   Status    Cpu    Memory    Hostname       Port
 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
 mockoon-greeting-api 0    online    0.5    71 MB     0.0.0.0        3002
</code></pre>

<p>You can also stop all running servers at once with <code>mockoon-cli stop all</code></p>

<h3 id="step7viewarunningmockslogs">Step 7. View a running mock's logs</h3>

<p>Mockoon CLI log all events like requests and errors in your user folder in the following files: <code>~/mockoon-cli/logs/{process_name}-out.log</code> and <code>~/mockoon-cli/logs/{process_name}-error.log</code>.</p>

<h3 id="step8deploymockooncliusingdocker">Step 8. Deploy Mockoon CLI using Docker</h3>

<p>Now to the fun part! The CLI can containerize your mock API for you. The docker base image is <code>node:14-alpine</code> which is very lightweight.</p>

<h3 id="usingthedockerizecommand">Using the dockerize command</h3>

<pre><code class="language-bash">mockoon-cli dockerize --data ./Greetings_Mockoon.json --port 3000 --index 0 --output ./Dockerfile
</code></pre>

<p>Now build the image:  </p>

<pre><code class="language-bash">    docker build -t mockoon-greeting-api .
</code></pre>

<p>And then run the container:</p>

<pre><code class="language-bash">    docker run -d -p 3000:3000 mockoon-greeting-api
</code></pre>

<p><a href="https://asciinema.org/a/432855"><img src="https://asciinema.org/a/432855.svg" alt="Mocking a REST API the "API first" approach with Mockoon" title=""></a></p>

<h3 id="step9usemockooncliinacienvironmentgithubactions">Step 9. Use Mockoon CLI in a CI environment: GitHub Actions</h3>

<p>Mockoon CLI being a Javascript application, it can run on any environment where Node.js is installed, including continuous integration systems like GitHub Actions, Buildkite, or CircleCI. It is useful when you want to run a mock server while running integration tests on another application. For example, you could mock the backend when running React front-end application tests. Or when you are running IT tests for a java based application that <code>integrates</code> with Greetings API.</p>

<p>Here is an example of a GitHub Action running a mock API (via docker) before running some tests:</p>

<pre><code class="language-yaml">name: Run mock API server

on: [push]

jobs:  
  run_integ_tests:
    name: Run integ tests
    runs-on: ubuntu-latest
    services:
      greetings:
        image: shanelee007/mockoon-greeting-api:latest
        ports:
          - 3000:3000

    steps:
    - uses: actions/checkout@v2
    - name: Set up JDK 11
      uses: actions/setup-java@v2
      with:
        java-version: '11'
        distribution: 'adopt'
    - name: Cache Maven packages
      uses: actions/cache@v2
      with:
        path: ~/.m2
        key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
        restore-keys: ${{ runner.os }}-m2
    - name: Build with Maven
      run: mvn --batch-mode -ff -V --update-snapshots verify
</code></pre>

<p>So just to summarise with the "API first" approach I can:</p>

<ul>
<li>import an OAS to mockoon</li>
<li>configure the routes (if needed)</li>
<li>set some rules</li>
<li>add dynamic templating to responses (more on this later)</li>
<li>export this file, containerise using the CLI and run it as part of my CI build. </li>
</ul>

<p>All within a matter of seconds!</p>

<h2 id="generatingdynamicdata">Generating dynamic data</h2>

<p>Mocking an API can save you time. By faking the backend responses early, you don't have to worry about whether an endpoint is ready or not. You are up and running in no time and can start implementing your application. However, your mock should still be realistic. And the examples provided in the OAS are often not enough to surface UI layout problems, container overflowed by text, etc.</p>

<p>When mocking using <code>Mockoon</code>, you can easily customize your endpoints to make them look like real ones and even behave realistically, thanks to the dynamic templating system.</p>

<h3 id="generaterandomfakedata">Generate random fake data</h3>

<p>Nowadays, most developers work with JSON. Generating a massive amount of fake JSON data with Mockoon is a breeze thanks to the powerful templating system based on Handlebars syntax.</p>

<p>Mockoon also offers multiple helpers and embarks the <code>Faker.js</code> library, which can generate localized random data as various as cities, addresses, first names, phone numbers, UUID, etc.</p>

<h4 id="completejsonexamplepostslist">Complete JSON example: posts list</h4>

<p>So let's revisit our Greetings API again. The API provides a <code>GET</code> request to return all greetings. So let's override the examples provided from OAS with much richer content.</p>

<p>By using a combination of <code>repeat</code>, <code>image.avatar</code>, <code>lorem.sentences</code>, etc. you can quickly get a massive amount of random data. Combined with the latency option, you can even simulate a slow server and check how your application behaves under stress.</p>

<p>To use the templating system, you only have to use the response body editor and start adding your content. Remember to use the double curly braces to delimit your helpers <code>{{ helperName }}</code> Let's have a look at what such a body could look like:</p>

<pre><code class="language-handlebars">[
  {{#repeat (queryParam 'total' '5')}}
  {
    "id": {{@index}},
    "message": "{{faker 'lorem.sentence' 3 5}}",
    "creationDate": "{{date '2020-11-20' '2020-11-25' "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"}}",
    "label": "key",
    "isFriendly": {{faker 'random.boolean'}},
    "weatherType": "{{oneOf (array '1' '2' '3')}}",
    "status": "{{oneOf (array 'SMILEY_FACE' 'SAD_FACE')}}"
  }
  {{/repeat}}
]
</code></pre>

<p>After a call to Mockoon, this would be the kind of body generated from this template:</p>

<pre><code class="language-json">[
  {
    "id": 0,
    "message": "Earum veritatis est.",
    "creationDate": "2020-11-24T17:32:33.293Z",
    "label": "key",
    "isFriendly": false,
    "weatherType": "1",
    "status": "SMILEY_FACE"
  },
  {
    "id": 1,
    "message": "Rerum ipsa autem.",
    "creationDate": "2020-11-23T10:30:00.526Z",
    "label": "key",
    "isFriendly": true,
    "weatherType": "3",
    "status": "SAD_FACE"
  },
  {
    "id": 2,
    "message": "Porro aut dolores.",
    "creationDate": "2020-11-22T21:03:58.452Z",
    "label": "key",
    "isFriendly": true,
    "weatherType": "1",
    "status": "SMILEY_FACE"
  },
  {
    "id": 3,
    "message": "Qui repudiandae quibusdam.",
    "creationDate": "2020-11-22T09:13:08.923Z",
    "label": "key",
    "isFriendly": true,
    "weatherType": "2",
    "status": "SAD_FACE"
  },
  {
    "id": 4,
    "message": "Qui et voluptatem.",
    "creationDate": "2020-11-22T08:36:19.770Z",
    "label": "key",
    "isFriendly": true,
    "weatherType": "1",
    "status": "SMILEY_FACE"
  }
]
</code></pre>

<p>This example makes extensive usage of what Mockoon and Faker.js have to offer. First, it generates as many "greetings" items as provided in the <code>total</code> query parameter (or default to 5) when calling GET <code>/your/endpoint?total=140</code>. It is especially useful when you want to request a specific number of items depending on the pagination or a "number per pages" user setting. Second, you can see that multiple properties are defined, and random mock data is generated like sentence, date-time, boolean etc.</p>

<p>There are a lot of possibilities and combinations you can try. You can also make your template react to a lot of parameters from the entering request by using Mockoon's helpers. We've already seen <code>queryParam</code> above, but you will find many more in the templating documentation. They allow you to query the request information like <code>body</code>, <code>urlParam</code>, <code>header</code>, <code>method</code>, etc.</p>

<blockquote>
  <p>Mockoon does not limit you to JSON. The templating language based on Handlebars is compatible with any content type. It means that you can generate CSV, HTML, XML, etc. You will find below some examples of what can you can achieve with the templating system.</p>
</blockquote>

<h4 id="generatedynamictemplatingdependingontherequest">Generate dynamic templating depending on the request</h4>

<p>We just saw some interesting use-cases but still quite simple. When working on your application, you may want to go a little bit further by making the template react to the request sent to Mockoon. This is possible by using various helpers that you will find in the templating documentation: <code>body</code>, <code>queryParam</code>, <code>urlParam</code>, <code>cookie</code>, <code>header</code>, <code>hostname</code>, <code>ip</code>, <code>method</code>, etc.</p>

<p>They allow you to access the entering request's information. Combined with other helpers like <code>repeat</code>, <code>switch</code>, or <code>if</code>, you will be able to dynamically generate more complex content.</p>

<p>You will find below some examples:</p>

<h5 id="newgreetingafterapostrequest">New greeting after a POST request</h5>

<p>We will reuse in the response the various parameters present in the request:</p>

<pre><code class="language-json">{
  "id": "{{faker 'random.uuid'}}",
  "message": "{{body 'message'}}",
  "creationDate": "{{date '2020-11-20' '2020-11-25' "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"}}",
  "label": "{{body 'label'}}",
  "isFriendly": {{body 'isFriendly'}},
  "weatherType": {{body 'weatherType'}},
  "status": "{{body 'status'}}"
}
</code></pre>

<p>After a call to this endpoint with the following body:</p>

<pre><code class="language-bash">POST /greetings  
Content-Type: application/json

{
  "message": "Hello Docker",
  "label": "key",
  "isFriendly": true,
  "weatherType": 0,
  "status": "SMILEY_FACE"
}
</code></pre>

<p>We would receive this kind of response content, containing the request information plus some new fields (id and creationDate):</p>

<pre><code class="language-json">{
  "id": "6df3c0c6-bce8-4094-ae29-5cb637fc15a3",
  "message": "Hello Docker",
  "creationDate": "2020-11-24T14:49:47.139Z",
  "label": "key",
  "isFriendly": true,
  "weatherType": 0,
  "status": "SMILEY_FACE"
}
</code></pre>

<p>For more complex cases or to test various error handling scenarios, you could also create multiple responses for the same route, with different bodies, and trigger them by defining some <code>rules</code>. To learn more about using multiple responses combined with rules, you can have a look at the related <a href="https://mockoon.com/docs/latest/route-responses/dynamic-rules/">documentation</a>.</p>

<p>Src files used in this post can be found on <a href="https://github.com/shavo007/mockoon-demo">github</a></p>]]></content:encoded></item><item><title><![CDATA[Jibbing with spring boot and google cloud run]]></title><description><![CDATA[<p>I said jibbing not jiving! üòÜ</p>

<h1 id="cloudnativearchitecture">Cloud native architecture</h1>

<p>When it comes to microservices and cloud native architecture you first think about containers. Now you can of course compose your own docker file. But with jvm based microservices there is a tool from google called jib that can simplify containerization.</p>

<p>We'll</p>]]></description><link>https://blog.shanelee.name/2020/08/29/jibbing-with-spring-boot-and-google-cloud-run/</link><guid isPermaLink="false">9dc3d48c-84cb-487a-b8ca-7d14229f1fe5</guid><category><![CDATA[gcp]]></category><category><![CDATA[cloudrun]]></category><category><![CDATA[spring-boot]]></category><category><![CDATA[spring]]></category><category><![CDATA[docker]]></category><category><![CDATA[microservice]]></category><category><![CDATA[cloud]]></category><category><![CDATA[google]]></category><category><![CDATA[jib]]></category><category><![CDATA[redoc]]></category><category><![CDATA[swagger]]></category><category><![CDATA[openapi]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 29 Aug 2020 08:08:34 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2020/08/ardian-lumi-6Woj_wozqmA-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.shanelee.name/content/images/2020/08/ardian-lumi-6Woj_wozqmA-unsplash.jpg" alt="Jibbing with spring boot and google cloud run"><p>I said jibbing not jiving! üòÜ</p>

<h1 id="cloudnativearchitecture">Cloud native architecture</h1>

<p>When it comes to microservices and cloud native architecture you first think about containers. Now you can of course compose your own docker file. But with jvm based microservices there is a tool from google called jib that can simplify containerization.</p>

<p>We'll take a simple Spring Boot application and build its Docker image using <a href="https://github.com/GoogleContainerTools/jib">Jib</a>. And then we'll also publish to GCR and deploy on google cloud run.</p>

<h2 id="jib">Jib</h2>

<blockquote>
  <p>Jib builds optimized Docker and OCI images for your Java applications without a Docker daemon - and without deep mastery of Docker best-practices. It is available as plugins for Maven and Gradle and as a Java library.</p>
</blockquote>

<p>You don't even need docker installed. Just use maven or gradle plugin and away you go! It uses <a href="https://github.com/GoogleCloudPlatform/distroless">distroless</a> base image under the hood, but as you expect all of this is configurable.</p>

<p>Jib supports multiple container registries,  can change the base image, jvm flags, tags, volumes and much more.</p>

<h2 id="greetingapp">Greeting App</h2>

<p>I use <strong>VSCode</strong> heavily for development so I wanted to see what support they have for java and spring boot. In fact they have many extensions. The extensions I installed are:</p>

<ul>
<li><a href="https://marketplace.visualstudio.com/items?itemName=richardwillis.vscode-gradle-extension-pack">Gradle Extension Pack</a></li>
<li><a href="https://aka.ms/vscode-java-installer-mac">VS Code for java</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=Pivotal.vscode-spring-boot">Spring boot tools</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-spring-initializr">Spring Initializr Java Support</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-spring-boot-dashboard">Spring Boot Dashboard</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=GabrielBB.vscode-lombok">Lombok Annotations Support for VS Code</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=42Crunch.vscode-openapi">OpenAPI (Swagger) </a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=Shan.code-settings-sync">Settings Sync </a> very important to sync if I change laptop in the future...</li>
</ul>

<p>I used Spring Initializr to create a simple project in vs code. <br>
It'll expose a simple <strong>GET</strong> endpoint:</p>

<pre><code class="language-bash">http://localhost:8080/greeting  
</code></pre>

<h3 id="deployment">Deployment</h3>

<p>Now I wanted to deploy this on cloud run and not worry about defining a docker file. So I used the gradle jib plugin and configured the credentials helper to deploy to <a href="https://github.com/GoogleContainerTools/jib/tree/master/jib-gradle-plugin#configuration">GCR</a> (Google cloud registry)</p>

<h4 id="cloudrun">Cloud run</h4>

<p>You can think of cloud run as CAAS (container-as-a-service) or serverless containers. It allows you to  run your stateless HTTP containers without worrying about provisioning machines, clusters or autoscaling. The main difference with app engine flexible is that it can scale to zero - only pay per request.</p>

<p>If you are interested to learn more about cloud run check out the unofficial <a href="https://github.com/ahmetb/cloud-run-faq">faqs</a></p>

<p>There is even a <a href="https://github.com/GoogleCloudPlatform/cloud-run-button">cloud run button</a> if you want to deploy your API publicly.</p>

<p>There is a fantastic VSCode extension called <a href="https://cloud.google.com/code">cloud code</a> that can help you deploy to cloud run or GKE on google cloud. It can even pick up if you are using <a href="https://blog.shanelee.name/2019/02/20/skaffold-for-local-kubernetes-development/">skaffold</a> or jib under the hood.</p>

<p><img src="https://blog.shanelee.name/content/images/2020/08/cloudcode1.png" alt="Jibbing with spring boot and google cloud run">
<img src="https://blog.shanelee.name/content/images/2020/08/cloudcode2.png" alt="Jibbing with spring boot and google cloud run"></p>

<p>This command will build, push your image to GCR and deploy to cloud run. And that's it!</p>

<p>Now you can access the public url and test out the endpoint.</p>

<p><img src="https://blog.shanelee.name/content/images/2020/08/Cloudcodeexp.png" alt="Jibbing with spring boot and google cloud run"></p>

<p>The source code for the example is over on <a href="https://github.com/shavo007/spring-boot-jib">github</a></p>

<h2 id="apidesign">API design</h2>

<p>In my previous <a href="https://blog.shanelee.name/2019/05/17/graphql-api-google-cloud-run/">post</a> I discussed graphQL. Graphql provides a schema for introspection and type safety. </p>

<p>Now for REST, swagger or <a href="https://swagger.io/resources/open-api/">open API spec</a> is the standard for designing and documenting your API. Even <a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-specification">kubernetes API </a> supports open api spec. Every time you call <code>kubectl describe &lt;resource&gt;</code> it calls this endpoint for info.</p>

<p>Tools such as <a href="https://support.insomnia.rest/article/94-introduction">insomnia designer</a> and api gateways now support open api spec.</p>

<p>I decided to design and document my greetings API using <a href="https://github.com/Redocly/redoc">redoc</a></p>

<h3 id="redoc">Redoc</h3>

<blockquote>
  <p>OpenAPI/Swagger-generated API Reference Documentation</p>
</blockquote>

<p>Redoc supports open api spec v3 and provides responsive documentation with code samples. There is many ways to deploy but again Ill go the docker way and deploy on cloud run.</p>

<h4 id="redocgenerator">Redoc generator</h4>

<p>Redoc has a <a href="https://github.com/Redocly/create-openapi-repo">generator</a> which provides a docs-like-code approach to OpenAPI definitions. It allows you to validate your spec before bundling it. I documented my greetings API above, bundled it and served it as static content from the API <a href="https://github.com/shavo007/spring-boot-jib/blob/master/src/main/resources/static/swagger.yaml">itself</a></p>

<p><mark>One thing to note is you need to enable CORS in your API to serve the static file. I also had an issue with @EnableMVC annotation so I removed that.</mark></p>

<p>To deploy, I built redoc on GCR and deployed to cloud run. You can run this in cloud shell if you like</p>

<pre><code class="language-bash">export PROJECT_ID=$(gcloud config list --format 'value(core.project)')  
#change CLOUD_RUN_SPRING_BOOT_BASE_URI to the location of your API deployed on cloud run

gcloud run deploy redoc-greetings-api --project $PROJECT_ID --image gcr.io/$PROJECT_ID/redoc --platform managed --region us-central1 --port 80 --cpu 1 --memory 256Mi --concurrency 80 --timeout 300 --update-env-vars SPEC_URL=https://&lt;CLOUD_RUN_SPRING_BOOT_BASE_URI&gt;/swagger.yaml  
</code></pre>

<p><img src="https://blog.shanelee.name/content/images/2020/08/Redoc.png" alt="Jibbing with spring boot and google cloud run"></p>

<p>Again, the src code can be found on <a href="https://github.com/shavo007/greetings-doc">github</a></p>

<p>And thats it. Happy jibbing! üï∫üèª</p>]]></content:encoded></item><item><title><![CDATA[How I passed the professional google cloud architect exam]]></title><description><![CDATA[<h1 id="sowhygooglecloud">So why Google cloud??</h1>

<p>Well, the Google Cloud Platform (GCP) is behind the 8 ball for sure in comparison to AWS. But with the huge spike in companies looking to reduce their Capex and move to the cloud, its offerings in the data analytics space for example is very enticing.</p>]]></description><link>https://blog.shanelee.name/2020/08/04/how-i-passed-the-professional-google-cloud-architect-exam/</link><guid isPermaLink="false">f54e6eec-b076-47af-959c-23f838c318f6</guid><category><![CDATA[cloud]]></category><category><![CDATA[google]]></category><category><![CDATA[kubernetes]]></category><category><![CDATA[exam]]></category><category><![CDATA[gcp]]></category><category><![CDATA[google cloud]]></category><category><![CDATA[architect]]></category><category><![CDATA[bigquery]]></category><category><![CDATA[linux academy]]></category><category><![CDATA[cloud native]]></category><category><![CDATA[hybrid]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 04 Aug 2020 06:47:49 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2020/08/morning-brew-T0qYg2nPUWM-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<h1 id="sowhygooglecloud">So why Google cloud??</h1>

<img src="https://blog.shanelee.name/content/images/2020/08/morning-brew-T0qYg2nPUWM-unsplash.jpg" alt="How I passed the professional google cloud architect exam"><p>Well, the Google Cloud Platform (GCP) is behind the 8 ball for sure in comparison to AWS. But with the huge spike in companies looking to reduce their Capex and move to the cloud, its offerings in the data analytics space for example is very enticing. Also in my view, they have the best managed kubernetes service GKE. Which makes sense, when you understand that kubernetes originated from googles own internal cluster orcherstration service called <a href="https://research.google/pubs/pub43438/">borg</a>. </p>

<p>I have worked with AWS for many years professionally. Gaining exposure to other cloud providers is important when you see companies adopting a  multi-cloud approach but fundamentally many of the underlying patterns and managed services commonly exist across the top 3 (AWS, GCP, Azure).</p>

<p>Up until recently I did not fully understand where <a href="https://www.forbes.com/sites/janakirammsv/2020/07/19/why-bigquery-omni-is-a-big-deal-for-google-cloud-customers-and-partners/#3644f89b1843">Anthos</a> fit into the picture. But now I do. By bringing compute closer to the data, existing cloud users can now use the likes of GKE and BigQuery. Nicely played Google! <strong>If the mountain won't come to muhammed...</strong> ‚ò∫Ô∏è </p>

<h2 id="certification">Certification</h2>

<p>This <a href="https://cloud.google.com/certification/cloud-architect">certification</a> allowed me to analyse and understand the majority of GCP services. But also provide a solid understanding of architectural best practices when it comes to design considerations, security, reliability and cost optimisation.</p>

<h2 id="exampreparation">Exam Preparation</h2>

<p>There are several training providers out there that provide courses specific to this exam: Coursera, Udemy, Linux academy to name but a few.</p>

<h3 id="courseras">Coursera‚Äôs:</h3>

<ul>
<li><p><a href="https://www.coursera.org/specializations/gcp-architecture">Architecting with Google Compute Engine
</a></p></li>
<li><p><a href="https://www.coursera.org/learn/preparing-cloud-professional-cloud-architect-exam/">Preparing for the Professional Cloud Architect Examination</a></p></li>
</ul>

<h3 id="linuxacademys">Linux Academy‚Äôs:</h3>

<ul>
<li><a href="https://linuxacademy.com/cp/modules/view/id/321">Google Cloud Certified Professional Cloud Architect
</a></li>
</ul>

<p>I took the updated course from Linux Academy‚Äôs Google Cloud Certified Professional Cloud Architect.</p>

<p>Linux Academy‚Äôs updated course was a good choice as it covers the majority of exam related GCP based topics and its practice sessions are an added advantage associated after each lesson. Their <a href="https://interactive.linuxacademy.com/diagrams/MasterBuildersGuide.html">master builders guide</a> was a very important document that I referred back to many times.</p>

<p>I also like to be hands on, so I signed up to <a href="https://www.qwiklabs.com/">qwiklabs</a> and completed many of the quests they provide in relation to GCP. It's extremely helpful especially if you do not have your own google cloud account. </p>

<p>Google also provides a <a href="https://lp.cloudplatformonline.com/rs/808-GJW-314/images/Professional%20Cloud%20Architect%20Journey.pdf">learning path</a> as an alternative option.</p>

<h2 id="examregistration">Exam registration</h2>

<p><a href="https://webassessor.com/wa.do?page=publicHome&amp;branding=GOOGLECLOUD">Register</a> for the exam when you are about 1 week into your study. This allows you to book in a slot not too far in the future and keeps you motivated.</p>

<h2 id="examlayout">Exam layout</h2>

<blockquote>
  <p>The exam is two hours long. You can take it remotely or at a test centre ( I took it at home). There are 50 questions - multiple choice and multiple select. The GCP case studies take up about 12 questions with the rest, GCP in general.</p>
</blockquote>

<h3 id="examscore">Exam score</h3>

<p>The passing marks are not shared by Google in any of GCP exam except it tells about pass or fail. However it has been assumed from various blogs and training providers that it‚Äôs roughly around 80% for GCP exams though no official confirmation about it.</p>

<h3 id="examreadiness">Exam readiness</h3>

<p>Check your readiness by taking the <a href="https://forms.gle/SHcLhSXckievBNBn6">google practice exam</a> and Linux academys (if you have signed up for the course). If you are not scoring 90% or above, then keep studying!</p>

<h3 id="tips">Tips</h3>

<ul>
<li><p>You will have plenty of time to go back and review marked questions. So make sure if you are stuck, mark question and move on. </p></li>
<li><p>Eliminate answers that you know are incorrect</p></li>
<li><p>Look out for main keywords in the question - speeds, low latency, failover, serverless etc etc </p></li>
</ul>

<h2 id="finalsuggestions">Final suggestions</h2>

<ul>
<li><p>Remember to deeply learn Kubernetes, Bigdata services(Big Query, BigTable), NoSQL options, App engine, Storage, IAM, BigQuery Roles, Case studies..</p></li>
<li><p>Googles new <a href="https://cloud.google.com/architecture/framework">architecture framework</a> is a very useful resource too.</p></li>
</ul>

<p><img src="https://blog.shanelee.name/content/images/2020/08/googlecert.jpg" alt="How I passed the professional google cloud architect exam"></p>

<p>Still waiting on my bag!! Best of luck.</p>]]></content:encoded></item><item><title><![CDATA[Road to clean energy and sustainability]]></title><description><![CDATA[<blockquote>
  <p>This is an overview of my journey to reducing my carbon footprint and making my home as energy efficient as possible.</p>
</blockquote>

<h2 id="energyefficiency">Energy efficiency</h2>

<p>WFH myself since March, I did not realise how cold and under insulated australian households can be! I'm european and I feel colder here than winter time</p>]]></description><link>https://blog.shanelee.name/2019/11/28/road-to-clean-energy-and-sustainability/</link><guid isPermaLink="false">e710a094-36a0-417a-980d-ad93850aa892</guid><category><![CDATA[solar]]></category><category><![CDATA[clean energy]]></category><category><![CDATA[melbourne]]></category><category><![CDATA[sustainability]]></category><category><![CDATA[cycle]]></category><category><![CDATA[bicycle]]></category><category><![CDATA[climatechange]]></category><category><![CDATA[renewables]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Thu, 28 Nov 2019 03:20:01 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2019/11/matt-duncan-IUY_3DvM__w-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
  <img src="https://blog.shanelee.name/content/images/2019/11/matt-duncan-IUY_3DvM__w-unsplash.jpg" alt="Road to clean energy and sustainability"><p>This is an overview of my journey to reducing my carbon footprint and making my home as energy efficient as possible.</p>
</blockquote>

<h2 id="energyefficiency">Energy efficiency</h2>

<p>WFH myself since March, I did not realise how cold and under insulated australian households can be! I'm european and I feel colder here than winter time in Vienna at minus 10 degrees! ü•∂</p>

<blockquote>
  <p>We‚Äôre lagging between 10-15 years behind in Europe, parts of US and Canada,‚Äù says Trivess Moore, a senior lecturer in construction at RMIT and member of the Sustainable Building Innovation Laboratory.</p>
</blockquote>

<p>You can read more on this topic where a housing estate project looked at building homes with at least an energy rating of <strong>7.5</strong> and the outcomes that came from this in relation to cost savings  but also wellbeing <br>
<a href="https://onestepoffthegrid.com.au/even-with-rooftop-solar-boom-consumers-are-paying-dearly-for-what-lies-underneath/">https://onestepoffthegrid.com.au/even-with-rooftop-solar-boom-consumers-are-paying-dearly-for-what-lies-underneath/</a></p>

<p>So what can we do about it?? Well, one recommendation I would like to see is for state and local councils to work with property developers and provide incentives to build energy efficient homes. But in the meantime, what can a homeowner do to rectify the situation... ü§î</p>

<h2 id="renewables">Renewables</h2>

<p>Last year, I installed solar panels on my house. This is something I had always planned to do when I purchased my own home. With Victorias rebate program, there is a huge uptake in residents installing solar. And why wouldn't you! Reduce your energy costs over time and become less reliant on the grid and our coal-burning plants. üåç</p>

<h3 id="solar">Solar</h3>

<p><img src="https://blog.shanelee.name/content/images/2019/11/antonio-garcia-ndz_u1_tFZo-unsplash.jpg" alt="Road to clean energy and sustainability"></p>

<p>Before I chose a solar installer to go with, I researched online to understand the best solution for my home.</p>

<p>Clean energy council (CEC) has a great guide on solar <a href="https://www.cleanenergycouncil.org.au/consumers">here</a>. I choose not to get a battery now as they are still quite expensive. But with the rapid innovation in this space, it won't be long in my view before it becomes attainable. </p>

<p><em>Victoria energy council has expanded the <a href="https://www.solar.vic.gov.au/solar-battery-rebate">battery rebate</a> to certain postcodes if applicable.</em></p>

<p>Finn at <a href="https://www.solarquotes.com.au/solar101.html">solar quotes</a> has invaluable information on choosing the right panels, inverters and provides quotes for local reviewed installers.</p>

<p>I contacted around five or six installers (all CEC accredited) and was surprised by the results. One actually refused to do a house inspection and focused solely on nearmap satellite imagery. Another stated that it was not worth installing based on their panel placement! </p>

<h4 id="systemchoice">System choice</h4>

<p>So the system I decided on was <strong>6.27KW</strong> Solar System with:</p>

<ul>
<li>19x Q-Cells Dual Cell Q.Peak Duo G5-330W</li>
<li>19x Micro-Inverters Enphase IQ7+</li>
<li>1x Envoy S-Metered + DRM</li>
<li>1x Sub-board installation to fit the Envoy relays and allow room for future use</li>
</ul>

<p>All panels were installed west facing.</p>

<p><img src="https://blog.shanelee.name/content/images/2019/11/enphase.gif" alt="Road to clean energy and sustainability"></p>

<p><strong>Enphase Micro-Inverters</strong> make each panel run independently, which is a big advantage compared to a string inverter system where when one panel starts to act faulty, it shuts down the entire array of panels.</p>

<p>It also comes with a monitoring system, the Enphase enlighten view, which allows you to monitor not only the PV production of your solar system but also your home electricity consumption.</p>

<p>More information on micro inverters can be found <a href="https://www.cleanenergyreviews.info/blog/microinverters">here</a></p>

<p>There are some myths about installing on the south for example but it really depends on where you live as Finn <a href="https://www.solarquotes.com.au/panels/direction/">explained</a>, but it will inevitably still produce energy! It all comes back to <strong>self consumption</strong></p>

<p>I have the monitoring setup using enlighten and have a cheeky look every day to see my usage and production.</p>

<p>Below is the energy produced for a typical summers day in Melbourne.</p>

<p><img src="https://blog.shanelee.name/content/images/2020/08/Screenshot_20200819-112109-1.png" alt="Road to clean energy and sustainability"></p>

<p>There is a great sense of satisfaction knowing I am using clean energy to power my house during the day.</p>

<h4 id="selfconsumption">Self consumption</h4>

<p>I mentioned earlier about self-consumption. As my wife and I both work during the day (pre-covid), we try to utilize solar power when we can.</p>

<p>For example, I turn on the dishwasher before I leave for work in the morning and set the washing machine on a timer to turn on at peak time around 12-1 pm during the day.</p>

<p>Here is a great <a href="https://www.solar.vic.gov.au/making-most-solar">case study</a> of how a young family self consume.</p>

<p>It is important to understand that the feed-in tariff (FIT) most likely will fluctuate in the future (in VIC Jul 2020 reduced from 12c to 10.2c) so try and use up as much clean energy as possible.</p>

<p>Lastly, here is a fantastic write up of a resident using <a href="https://onestepoffthegrid.com.au/analysis-of-my-home-battery-solar-systems-first-year-performance/">solar</a> for a year</p>

<h2 id="anenergyefficienthome">An energy efficient home</h2>

<p>After installing solar, I looked into other areas I could make my home more energy efficient.</p>

<p>I stumbled across <a href="https://www.victorianenergysaver.vic.gov.au/save-energy-and-money/discount-energy-saving-products/save-with-these-energy-efficient-products">vic energy saver site </a> that provides savings for energy efficient products.</p>

<p>I had existing halogen downlights that I wanted to replace with LEDs and this initiative allowed me to replace all for <strong>FREE</strong>! </p>

<p>I used an accredited <a href="https://www.easybeinggreen.com.au/">provider</a> and it was pretty painless. They replaced all 40 of my downlights (apart from my dimmers). <strong>#winning</strong></p>

<p>The program even supports showerheads, weather sealing, and hotwater systems. Buying energy efficient appliances such as your oven, heat pump dryer, washing machine will make a difference too.</p>

<h3 id="getaninspection">Get an inspection</h3>

<p>Finally, if you want you can look into an <a href="https://www.victorianenergysaver.vic.gov.au/save-energy-and-money/get-a-home-energy-assessment">energy assessment</a> of your house.</p>

<h2 id="waste">Waste</h2>

<p>I was happy to see the Vic government bring in the plastic bag ban. Definitely well overdue. I believe in Ireland it came into play about ten years ago! </p>

<p>Since China stopped accepting alot of our waste the government has been scrambling to fix the waste management issue. I am happy to see Vic government outlining their <a href="https://www.vic.gov.au/sites/default/files/2020-02/Recycling%20Victoria%20A%20new%20economy.pdf">plan.</a> Highlights are:</p>

<ul>
<li>Four bin system üôåüèª</li>
<li>Container deposit scheme</li>
<li>Circular economy</li>
</ul>

<p>I actually saw new <a href="https://www.yarracity.vic.gov.au/services/roads-and-traffic/wellington-street-bike-lanes--stage-2">popup bicycle lanes</a> in fitzroy north recently using recycled plastic üëç</p>

<p>More info on using recycled materials can be found <a href="https://www.sustainability.vic.gov.au/en/About-us/Latest-news/2020/08/14/00/50/New-funding-to-use-recycled-materials">here</a></p>

<h3 id="foodwaste">Food waste</h3>

<blockquote>
  <p>Each year in Victoria households throw out 250,000 tonnes worth of food ‚Äì enough wasted food to fill Melbourne's Eureka Tower. üò±</p>
</blockquote>

<p>My wife and I have made a joint effort when shopping to buy fruit and veg loosely if possible. And also buy organically too. We have trialed growing some herbs and vegetables at home. Trial and error! But a great learning experience.</p>

<p>I listened to a podcast recently with Dr. Sandro Demaio (CEO of VicHealth) on <a href="https://open.spotify.com/episode/6w4JIeLLrnqUIDsX4jMC3E?si=nhJmzw81RtWAyH57FoNDGA">food</a> and its impact on society, our ecosystem and health. Less red meat people!!  <strong>#votewithyournote</strong></p>

<p>We also bring our <a href="https://www.redcycle.net.au/">scrunchy plastic</a> to the supermarket. Both coles and woolworths support the initiative.</p>

<h2 id="transport">Transport</h2>

<p>After energy, transport is the biggest carbon emitter. </p>

<blockquote>
  <p>Transport in Australia contributes around 100 million tonnes of greenhouse gasses into our atmosphere every year.</p>
</blockquote>

<p>The main reasons for transport emissions trending upwards are an over-dependence on cars with high average fuel use and an over-reliance on energy-intensive road freight.</p>

<p>So it comes to my favourite topic of all active transport and cycling!</p>

<h3 id="activetransport">Active transport</h3>

<p>Anyone who knows me, knows how I love cycling. I learned to cycle from a very young age in the irish countryside before you had to worry about cars on the road. Wherever I have worked and lived, whether in europe or australia my preferred mode of transport is the bike. For me, I love the independence and time alone on the bike. Plus it's a great way to wake up in the morning (especially if you need to cycle down chapel st ü§£)</p>

<p>Anytime my wife and I go abroad, I always go check out a city's shared bicycle scheme if available and explore the city by bike. This to me is the best way to see a city. On a warm summer evening cycling down along the banks of the river seine in Paris listening to some jazz and watching the world go by is pure bliss. üá´üá∑</p>

<p><img src="https://blog.shanelee.name/content/images/2020/08/IMG-20180522-WA0000-1.jpg" alt="Road to clean energy and sustainability"></p>

<p>Since covid there has been a huge uptake in cycling. I notice it myself at the weekend and on the trails. Governments like <a href="https://ecf.com/news-and-events/news/ireland-will-invest-10-total-transport-capital-budget-cycling?s=09">ireland</a> and the <a href="https://www.forbes.com/sites/carltonreid/2020/07/27/well-build-thousands-of-miles-of-protected-cycleways-pledges-boris-johnson/">UK</a> have invested heavily in protected cycleways and pedestrian infrastructure. </p>

<p>Change is coming, there is no doubt about it. The new paris mayor Anne Hidalgo has called it the <a href="https://www.theguardian.com/world/2020/feb/07/paris-mayor-unveils-15-minute-city-plan-in-re-election-campaign">15 minute city</a> idea. That is to have all services within a 15 minute walk/ride from your home. The days of commuter towns and work in the city after covid will not be the same. Working from home and even coworking spaces in rural areas will become more mainstream I predict.</p>

<blockquote>
  <p>‚ÄúWe need to reinvent the idea of urban proximity,‚Äù Moreno says. ‚ÄúWe know it is better for people to work near to where they live, and if they can go shopping nearby and have the leisure and services they need around them too, it allows them to have a more tranquil existence.</p>
</blockquote>

<h3 id="electricvehicles">Electric vehicles</h3>

<p>Just a quick mention on EVs. I am still waiting for the national EV strategy to be released in Australia. It has been delayed many times. Nudge, nudge Angus Taylor...</p>

<p>Now take <a href="https://thedriven.io/2020/07/10/the-electric-recipe-of-norways-zero-emissions-transport-boom/">Norway</a> for example - The Norwegian Parliament has decided on a goal that all new cars sold by <strong>2025</strong> should be zero (battery electric or hydrogen) emission vehicles. </p>

<p>Unfortunately for alot of aussies, EVs are just too expensive right now and there is no incentive to switch.</p>

<h2 id="investingforthefuture">Investing for the future</h2>

<p>I just finished Ross Garnuat book recently <a href="https://books.google.com.au/books/about/Superpower.html?id=KPiPDwAAQBAJ&amp;printsec=frontcover&amp;source=kp_read_button&amp;redir_esc=y#v=onepage&amp;q&amp;f=false">Superpower: Australia's low carbon opportunity</a> and our renewable investment needs to increase dramatically. The IMF says Australia should be spending more on infrastructure, but this should be on rail, airports and seaports, <strong>rather</strong> than roads.</p>

<p>With low interest rates for the forseeable future, Deloitte, Grattan institute, and many others have stated now is the time to invest and invest smartly. The IMF has called this "great reset" an opportunity for fiscal stimulus that is <strong>smarter</strong>, <strong>greener</strong> and <strong>fairer</strong>. So lets push ahead and be bold for a change.</p>

<h2 id="finally">Finally</h2>

<p>I know reducing our carbon footprint sounds challenging and some feel overwhelmed by it all. But I hope this article helps educate others and know there is support out there. Albert Einstein once said: </p>

<blockquote>
  <p>‚ÄúIn the middle of difficulty lies opportunity.‚Äù </p>
</blockquote>

<p>We <strong>all</strong> have an opportunity to make a difference and create a more sustainable future. </p>]]></content:encoded></item><item><title><![CDATA[GraphQL API on the new Google Cloud Run]]></title><description><![CDATA[ Cloud Run, a serverless environment based on containers and Kubernetes.]]></description><link>https://blog.shanelee.name/2019/05/17/graphql-api-google-cloud-run/</link><guid isPermaLink="false">1dfdf148-8167-4016-8259-29cd1cd39f87</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[google]]></category><category><![CDATA[graphql]]></category><category><![CDATA[k8s]]></category><category><![CDATA[docker]]></category><category><![CDATA[gke]]></category><category><![CDATA[cloud]]></category><category><![CDATA[cloudrun]]></category><category><![CDATA[nodejs]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Fri, 17 May 2019 08:07:00 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2019/05/fabio-comparelli-696510-unsplash-1.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
  <img src="https://blog.shanelee.name/content/images/2019/05/fabio-comparelli-696510-unsplash-1.jpg" alt="GraphQL API on the new Google Cloud Run"><p>Cloud Run is a layer that Google built on top of Knative to simplify deploying serverless applications on the Google Cloud Platform.</p>
</blockquote>

<p>So whats <strong>Knative</strong>?? </p>

<p><a href="https://cloud.google.com/knative/">Knative</a> provides an open API and runtime environment that enables you to run your serverless workloads anywhere you choose: fully managed on Google Cloud, on Google Kubernetes Engine (GKE), or on your own Kubernetes cluster.</p>

<p>Knative can be deployed on any Kubernetes cluster. It acts as the middleware bridging the gap between core infrastructure services and developer experience.</p>

<p>Cloud Run is Googles own implementation of Knative. <br>
 It enables you to run stateless containers that are invocable via web requests or Cloud Pub/Sub events.</p>

<p>Features include:</p>

<ul>
<li>Fast autoscaling</li>
<li>Managed</li>
<li>Redundancy</li>
<li>Integrated logging and monitoring</li>
<li>Custom domains</li>
<li>Built on knative</li>
</ul>

<p>There are <strong>two</strong> options when it comes to using Google Cloud Run. </p>

<h2 id="flavorsofcloudrun">Flavors of Cloud Run</h2>

<p>Currently in beta, Google Cloud Run is available as a standalone environment and within the Google Kubernetes Engine (GKE).</p>

<p>Developers can deploy apps to Cloud Run through the console or CLI. If there is a GKE cluster with Istio installation, apps targeting Cloud Run can be easy deployed to an existing Kubernetes cluster.</p>

<p>Each deployment to service creates a revision. A revision consists of a specific container image, along with environment settings such as environment variables, memory limits, or concurrency value.</p>

<p>Requests are automatically routed as soon as possible to the latest healthy service revision.</p>

<p>For more check out the video below on the differences.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/RVdhyprptTQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<h2 id="tutorial">Tutorial</h2>

<p>In this tutorial, we will deploy a graphql API based on Node.js and Postgres to the Cloud Run platform.</p>

<p>There are two steps involved in this workflow: provisioning a cloud sql postgres database instance, and deploying code to Cloud Run. This tutorial assumes you have an active account on Google Cloud Platform with the CLI and SDK installed on your development machine. </p>

<p><del>You also need Docker Desktop to build images locally.</del></p>

<p>Actually, that's not true! You don't even need docker locally!! You can use google cloud build to run it remotely for you ü§ì</p>

<h3 id="letsbegin">Lets begin</h3>

<p>To follow along, you can find the github project <a href="https://github.com/shavo007/graphql-playground/tree/master/api">here</a></p>

<p>In a previous <a href="https://blog.shanelee.name/2019/02/20/skaffold-for-local-kubernetes-development/">post</a>, I talked about running this example on kubernetes using skaffold</p>

<h4 id="googlecloudsetup">Google cloud setup</h4>

<p>First on your google cloud account you need to enable billing and <a href="http://console.cloud.google.com/apis/library/run.googleapis.com">cloud run API</a></p>

<p>Create your new google cloud project first and then run the following commands</p>

<pre><code class="language-bash">gcloud components install beta #install beta components  
gcloud components update #update components  
gcloud config set run/region us-central1 #set cloud run region  
gcloud services enable container.googleapis.com containerregistry.googleapis.com cloudbuild.googleapis.com  
gcloud config set project [PROJECT_ID] #set project id  
gcloud beta auth login  
</code></pre>

<h4 id="step1createcloudsqlforpostgres">Step 1: create cloud sql for postgres</h4>

<p>GraphQL APIs datasource is a postgres database. Cloud run supports <a href="https://cloud.google.com/run/docs/configuring/connect-cloudsql">cloud sql</a> service.</p>

<pre><code class="language-bash">gcloud sql instances create [INSTANCE_NAME]  --database-version=POSTGRES_9_6 \  
       --tier db-f1-micro --region us-central1 
#save on costs by using a shared-core instance 
gcloud sql users set-password postgres no-host --instance=[INSTANCE_NAME] \  
       --password=[PASSWORD]
</code></pre>

<p>For testing purposes, I am running a micro instance. </p>

<p><a href="https://cloud.google.com/sql/docs/postgres/create-instance">Refer to doc
</a> for more info</p>

<p><img src="https://blog.shanelee.name/content/images/2019/05/CloudSQL.png" alt="GraphQL API on the new Google Cloud Run"></p>

<h4 id="step2buildinganddeployingacloudrunservice">Step 2: Building and Deploying a Cloud Run Service</h4>

<p><mark>You can find the docker file and src in the github repo</mark></p>

<p>We will build the Docker image remotely and push it to Google Container Registry (GCR)</p>

<pre><code class="language-bash">gcloud builds submit --tag gcr.io/[PROJECT-ID]/graphql #build container image  
</code></pre>

<p>Verify the image exists in GCR</p>

<pre><code class="language-bash">gcloud container images list  
</code></pre>

<p>Deploy the container using cloud run and overwrite env vars to connect to the DB</p>

<pre><code class="language-bash">#overwrite the host to connect over a unix domain socket and db password
gcloud beta run deploy --image gcr.io/[PROJECT-ID]/graphql --add-cloudsql-instances [INSTANCE-NAME] --update-env-vars DB_HOST=/cloudsql/[CONNECTION NAME],name=graphql,DATABASE_PASSWORD=[PASSWORD] #respond y to allow unauthenticated invocations.  
</code></pre>

<p>The switch, <mark>‚Äìallow-unauthenticated</mark>, will let the service accept the traffic from the public internet. Notice that we are passing the Postgres connection string generated by cloudSQL as an environment variable. The code expects the connection string from the <mark>DBHOST</mark> environment variable.</p>

<p>See details of the running service by running the command below</p>

<pre><code class="language-bash">gcloud beta run services list  
</code></pre>

<p>Or you can view the UI console</p>

<p><img src="https://blog.shanelee.name/content/images/2019/05/CloudRunServiceGraphql.png" alt="GraphQL API on the new Google Cloud Run"></p>

<p>You can verify it works locally by using <a href="https://insomnia.rest/">Insomnia</a></p>

<p><img src="https://blog.shanelee.name/content/images/2019/05/insomnia.png" alt="GraphQL API on the new Google Cloud Run"></p>

<p>Some sample queries can be seen below to sign in a user and get back their details</p>

<pre><code class="language-json"> mutation
 {
   signUp(username: "shane", email: "slee@x.com",password: "xxx") {
     token
   }
 }

 query {
   users {
     username
     id
   }
 }
</code></pre>

<h4 id="finally">Finally</h4>

<p>Finally, delete all resources after.</p>

<p>Easiest way is to delete the test <a href="https://console.cloud.google.com/iam-admin/projects">project</a></p>

<p>Any questions feel free to comment below. </p>]]></content:encoded></item><item><title><![CDATA[Skaffold for local kubernetes development]]></title><description><![CDATA[skaffold for local kubernetes development]]></description><link>https://blog.shanelee.name/2019/02/20/skaffold-for-local-kubernetes-development/</link><guid isPermaLink="false">939f9b10-3288-4a62-83c4-8de5aa50b6f6</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[docker]]></category><category><![CDATA[node]]></category><category><![CDATA[skaffold]]></category><category><![CDATA[graphql]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Wed, 20 Feb 2019 06:37:09 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2019/02/jacek-dylag-680347-unsplash-1.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
  <img src="https://blog.shanelee.name/content/images/2019/02/jacek-dylag-680347-unsplash-1.jpg" alt="Skaffold for local kubernetes development"><p>Easy and Repeatable Kubernetes Development</p>
</blockquote>

<p><strong>TLDR</strong> Below, I will showcase how to install and use skaffold for local development with kubernetes.</p>

<p>Currently <strong>(20/02/2019)</strong>, skaffold has nearly <mark>6000</mark> ‚≠ê on github.</p>

<p>I have been using Skaffold for all my new projects that involve cloud native microservices, and it works like a charm on top of <code>Docker Desktop for Mac</code>/Minikube.</p>

<p><strong>Skaffold</strong> is fantastic for local development with kubernetes. I can test locally my changes without having to deploy remotely. This helps speed up my local development and gives me confidence in my changes.</p>

<h2 id="overview">Overview</h2>

<p>Skaffold is a tool to develop containerized applications locally or remotely while deploying them on Kubernetes. It automatically builds and deploys your apps as you change your source code.</p>

<p>Skaffold primarily simplifies the <mark>build ‚Üí deploy ‚Üí refactor ‚Üí repeat</mark> cycle.</p>

<h3 id="skaffoldmodes">Skaffold modes</h3>

<p><img src="https://blog.shanelee.name/content/images/2019/02/skaffold-cmds.jpg" alt="Skaffold for local kubernetes development"></p>

<p>In a single command, Skaffold can:</p>

<ul>
<li>Collects and watches your source code for changes</li>
<li>Syncs files directly to pods if user marks them as syncable</li>
<li>Builds artifacts from the source code</li>
<li>Tests the built artifacts using container-structure-tests</li>
<li>Tags the artifacts</li>
<li>Pushes the artifacts</li>
<li>Deploys the artifacts</li>
<li>Monitors the deployed artifacts</li>
<li>Cleans up deployed artifacts on exit (Ctrl+C)</li>
</ul>

<h2 id="skaffoldfeatures">Skaffold features</h2>

<ul>
<li><p><strong>Remote development:</strong> Skaffold doesn‚Äôt require you to run a local Kubernetes cluster (minikube or docker-for-desktop). It can build/push images locally with docker, and run them on the remote clusters (such as GKE). This is a laptop battery saver!</p></li>
<li><p><strong>More remote development:</strong> You actually don‚Äôt need to run a local docker either. Skaffold can do remote builds using services like Google Container Builder. Although it‚Äôll be slow.</p></li>
<li><p><strong>Tag management:</strong> In your Kubernetes manifests, you leave the image tags out in the ‚Äúimage:‚Äù field, and Skaffold automatically changes the manifests with the new tags as it rebuilds the images.</p></li>
<li><p><strong>Rebuild only what‚Äôs changed:</strong> If your microservices are on separate directories, changing source code for one will not cause rebuild for all images. Skaffold understands which images have been impacted by the change.</p></li>
<li><p><strong>Cleanup on exit:</strong> Terminating ‚Äúskaffold dev‚Äù runs a routine that cleans up the deployed k8s resources. If this fails, you can run ‚Äúskaffold delete‚Äù to clean up deployed artifacts.</p></li>
</ul>

<h2 id="letsgetstarted">Lets get started!</h2>

<p>On mac, you can install skaffold using brew</p>

<pre><code class="language-bash">brew install skaffold  
</code></pre>

<h3 id="localdevelopment">Local development</h3>

<p>Run <code>skaffold init</code> to bootstrap Skaffold config.</p>

<p>Once that is complete, define in the yaml file the location of where your kubernetes manifests are defined.</p>

<p>Sample skaffold yaml file</p>

<pre><code class="language-yaml">apiVersion: skaffold/v1beta5  
kind: Config  
build:  
  artifacts:
  - image: shanelee007/graphql
deploy:  
  kubectl:
    manifests:
    - kubernetes/config.yaml
    - kubernetes/deployment.yaml
    - kubernetes/secret.yaml
profiles:  
- name: dev
  build:
    artifacts:
    - image: shanelee007/graphql
      sync:
        '**/*.js': .
      docker:
        dockerfile: Dockerfile.dev
</code></pre>

<p>Here you can see where I defined my manifest files. Also for local development I have used a <code>profile</code> to define a development dockerfile and utilised the sync feature. </p>

<blockquote>
  <p>profiles feature grants you the freedom to switch tools as you see fit depending on the context.</p>
</blockquote>

<h3 id="localdevelopmentworkflow">Local development workflow</h3>

<p><img src="https://blog.shanelee.name/content/images/2019/02/skaffold_workflow_local.png" alt="Skaffold for local kubernetes development"></p>

<h5 id="syncfilestoyourpodswithskaffold">Sync files to your pods with Skaffold</h5>

<p>With even one change to a file, Skaffold rebuilds the images that depend on that file, pushes them to a registry, and then redeploys the relevant parts of your Kubernetes application. </p>

<p>The Skaffold file sync feature solves this problem. For each image, you can specify which files can be synced directly into a running container. Then, when you modify these files, Skaffold copies them directly into the running container rather than kicking off a full rebuild and redeploy. With Skaffold‚Äôs file sync feature, you can enjoy even faster development!</p>

<p><a href="https://skaffold.dev/docs/how-tos/filesync/">Sync</a> is quite a new feature. Think of it as similar to <code>nodemon</code></p>

<p>I have created my own demo <strong>github</strong> project <a href="https://github.com/shavo007/graphql-playground/tree/master/api#skaffold">here</a> if you want to follow along.</p>

<p>There was an issue with publishing docker image for local development every time I ran skaffold. To prevent this from happening there is global config to disable this.</p>

<pre><code class="language-bash">skaffold config set --global local-cluster true #do not push images after building  
</code></pre>

<h4 id="noteondockerfile">Note on Dockerfile</h4>

<p>In my github project, you can see I use multi-stage approach with my docker <a href="https://github.com/shavo007/graphql-playground/blob/master/api/Dockerfile">files</a></p>

<p>Think of it as a build pipeline as code.</p>

<p>It allows you to selectively copy artifacts from one stage to another, leaving behind everything you don‚Äôt want in the final image. </p>

<p>To analyse your final production image I found a useful tool called <a href="https://github.com/wagoodman/dive">dive</a></p>

<p>It allows you to explore and optimise your docker image size.</p>

<p><img src="https://blog.shanelee.name/content/images/2019/02/Screen-Shot-2019-02-20-at-7-23-39-pm.png" alt="Skaffold for local kubernetes development"></p>

<p>Now we can run skaffold!</p>

<pre><code class="language-bash">skaffold dev -p dev -v=info #run locally/watching changes dev mode  
</code></pre>

<p>If you want to try out the new experimental gui run instead </p>

<pre><code class="language-bash">skaffold dev -p dev -v=info --experimental-gui  
</code></pre>

<p>Console output will look like so:</p>

<p><a href="https://asciinema.org/a/220028" target="_blank"><img src="https://asciinema.org/a/220028.svg" alt="Skaffold for local kubernetes development"></a></p>

<p>Every time you make a src code change, skaffold will watch for these changes and update the pod on the fly. Pretty neat!!</p>

<p><img src="https://media.giphy.com/media/vEgtLzJo8n7qg/giphy.gif" alt="Skaffold for local kubernetes development"></p>

<h4 id="upgrade">Upgrade</h4>

<p>As of <strong>20/02/2019</strong> the latest version is <code>v1beta5</code>. To upgrade just run </p>

<pre><code class="language-bash">brew upgrade skaffold  
skaffold fix --overwrite  
</code></pre>

<p>Thats all for now. In my next post, I will discuss deploying remotely using skaffold.üòÉ</p>

<h2 id="learnmore">Learn more</h2>

<ul>
<li><p>Check out skaffold <a href="https://skaffold.dev/docs/">doc</a> to understand more</p></li>
<li><p>There is numerous <a href="https://github.com/GoogleContainerTools/skaffold/tree/master/examples">examples</a> at the github repo</p></li>
<li><p>My sample github <a href="https://github.com/shavo007/graphql-playground/tree/master/api">repo</a> showcasing graphql api with kubernetes</p></li>
</ul>]]></content:encoded></item><item><title><![CDATA[How I aced the Certified Kubernetes Administrator (CKA) Exam]]></title><description><![CDATA[<p>I took the CKA exam recently and would like to share my own preparation and tips.</p>

<p>The exam is intense as it requires 3 hours of concentrated effort during which you need to solve 24 problems. So timekeeping is very important.  If you are interested to take this exam and</p>]]></description><link>https://blog.shanelee.name/2018/10/17/how-i-aced-the-certified-kubernetes-administrator-cka-exam/</link><guid isPermaLink="false">ab6a2a07-ad36-4be5-ae74-d629d25a25bb</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[cka]]></category><category><![CDATA[exam]]></category><category><![CDATA[kubectl]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Wed, 17 Oct 2018 07:29:30 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2018/10/clement-h-544786-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.shanelee.name/content/images/2018/10/clement-h-544786-unsplash.jpg" alt="How I aced the Certified Kubernetes Administrator (CKA) Exam"><p>I took the CKA exam recently and would like to share my own preparation and tips.</p>

<p>The exam is intense as it requires 3 hours of concentrated effort during which you need to solve 24 problems. So timekeeping is very important.  If you are interested to take this exam and have second thought about it, my sincere advice is, just do it!</p>

<h2 id="gettingstarted">Getting started</h2>

<p>As part of black friday deal last year I bought from linux foundation the package deal. It consisted of the CKA exam and linux foundation kubernetes fundamentals <a href="https://training.linuxfoundation.org/training/kubernetes-fundamentals/">course</a>.</p>

<p>In a previous contract role I had worked with kubernetes also on a daily basis. So I was familiar with the basic concepts.</p>

<h2 id="preparation">Preparation</h2>

<ul>
<li><p>Understanding the kubernetes documentation is vital here as that is the only resource you are now allowed as part of the exam. Search ‚Äú<strong>What resources am I allowed to access during my exam?</strong>‚Äù at <a href="https://www.cncf.io/certification/cka/faq/">faq section</a></p></li>
<li><p>Familiarising yourself with the official <a href="https://kubernetes.io/docs/concepts/">documentation</a> is a <strong>must</strong>.</p></li>
<li><p>Practice each topic. Since this exam is all about solving problems, you need to practice a lot. You can use katacoda <a href="https://www.katacoda.com/courses/kubernetes/playground">playground</a> or this <a href="https://labs.play-with-k8s.com/">labs</a> site by Docker. These two tools helped me a lot during preparation. Katacoda also has several scenario based topics and it is important to practice <a href="https://www.katacoda.com/courses/kubernetes">these</a>.</p></li>
<li><p>For each topic I found this <a href="https://cka-exam.blog/">blog</a>  extremely useful to go through.
<em>Practice, practice, practice</em>. Get used to deploying on a cluster and using all the <strong>kubectl</strong> commands. Locally on my mac, <em>docker for mac</em> now incorporates kubernetes so no need for minikube anymore.</p></li>
<li><p>Kubernetes the hard way - Kelsey hightowers tutorial on <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">github</a> is vital to pass this exam. Managed services like AKS, EKS or kubeadm are not going to help here. You need to get right under the hood and understand how the control plane works.</p></li>
<li><p>Familiarise yourself with tools such as: openssl, cfssl, systemctl, etcdctl (for managing etcd)</p></li>
</ul>

<h2 id="examtime">Exam Time</h2>

<p>There are 24 problems and the exam duration is 3 hours. This means you can spend seven and half minutes per question. However, the difficulty range varies; nearly dozen problems are straight and super simple. So, have a strategy for the exam. My strategy was to complete 10 easy questions in the first hour, 8 medium in the second hour and leave final hour for remaining 6 (tough) questions. Once you are done with the question, just double check whether you are saving the output as described in the question and move on (don‚Äôt try to come back and check the answers again). <strong>You will not have time to go back and check!</strong></p>

<h3 id="progresstracking">Progress Tracking</h3>

<p>The CKA exam allows you to write notes to a notebook, which they provide in exam‚Äôs UI. Use it wisely. Once I had my exam available, I used their Notebook to keep track of my progress. It helps you to see your progress and how much left to be finished. </p>

<p>The format I used:</p>

<pre><code class="language-vim"># - scoring - total
1 4 4  
2 5 9  
3 3 12  
4  
5  
...
24  
</code></pre>

<p>The first column is just the number of question (24 in total). The second column‚Ää‚Äî‚Ääscoring, is how much the problem worth and you will get that from the problem description. The third column is ‚Äútotal‚Äù, where you put the total scoring as you go and it will let you know what problems are already completed and how many points you already have. This little bureaucracy will help you to understand how well you are doing and come back to some problems if you decided to skip them before. </p>

<h3 id="kubectlninja">Kubectl Ninja</h3>

<p>Use kubectl to create resources (such as deployment, service, cronjobs etc) instead of creating them from manifest files. It saves lot of time. <br>
I used the <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">cheatsheet</a> heavily during the exam. </p>

<p>First thing I did was copy in the below to notepad. It allowed me to create resources quickly from <code>stdin</code>  </p>

<pre><code class="language-bash">cat &lt;&lt;EOF | kubectl create -f -

EOF  
</code></pre>

<blockquote>
  <p>Take the exam in the morning. </p>
</blockquote>

<p>I took it at 11AM. I went for a quick walk beforehand to get some fresh air and then I was ready to go!</p>

<p>To reiterate, familiarise yourself with the official documentation. It has multiple sections (tasks, concepts and references) for the same topic. So, you should know what/where to look for quickly in the documentation during exam.</p>

<p>Best of luck! Remember you have one free retake if needed.üòÉüöÄ‚öì</p>

<p><img src="https://blog.shanelee.name/content/images/2018/10/CKAcertificate.png" alt="How I aced the Certified Kubernetes Administrator (CKA) Exam"></p>]]></content:encoded></item><item><title><![CDATA[Istio on Azure AKS]]></title><description><![CDATA[How to deploy istio service mesh on azure kubernetes service (AKS) and run bookinfo example application]]></description><link>https://blog.shanelee.name/2018/08/12/istio-on-azure-aks/</link><guid isPermaLink="false">1300cd8a-88a9-43ef-a409-66a2b74c5ad0</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[azure]]></category><category><![CDATA[aks]]></category><category><![CDATA[istio]]></category><category><![CDATA[google]]></category><category><![CDATA[service-mesh]]></category><category><![CDATA[k8s]]></category><category><![CDATA[microservice]]></category><category><![CDATA[grafana]]></category><category><![CDATA[jaeger]]></category><category><![CDATA[tracing]]></category><category><![CDATA[metrics]]></category><category><![CDATA[prometheus]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 12 Aug 2018 04:20:09 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2018/08/peter-gonzalez-621059-unsplash-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.shanelee.name/content/images/2018/08/peter-gonzalez-621059-unsplash-1.jpg" alt="Istio on Azure AKS"><p><img src="https://blog.shanelee.name/content/images/2018/08/istio.png" alt="Istio on Azure AKS"></p>

<p>Istio recently <a href="https://istio.io/about/notes/1.0">announced</a> that they are production ready. Service meshes are becoming an important level of abstraction for a developer using kubernetes. And <a href="https://www.envoyproxy.io/">Envoy</a> is the heartbeat of this service mesh and continues its impressive growth. </p>

<blockquote>
  <p>Istio reduces complexity of managing microservice deployments by providing a uniform way to secure, connect, and monitor microservices.</p>
</blockquote>

<p>Google have also released a managed istio <a href="https://cloudplatform.googleblog.com/2018/07/cloud-services-platform-bringing-the-best-of-the-cloud-to-you.html">service</a></p>

<p>I have previously designed and built cloud native architectures (especially on AWS). But I found that AWS have <em>dropped the ball</em> in relation to kubernetes. Azure and Google managed kuberentes services are more mature and Azure kubernetes offering is even free! </p>

<p><strong>TLDR:</strong> I particulary like Azure AKS and below I will showcase how easy it is to create a cluster and run istio.</p>

<p>Brendan Burns (co-founder of kubernetes and leading the azure container team) and Microsoft have invested wisely I feel in the cloud and are kicking goals. Read the latest <a href="https://azure.microsoft.com/en-us/blog/azure-kubernetes-service-aks-ga-new-regions-new-features-new-productivity/">blog</a> from Brendan on the most recent releases and updates. </p>

<h2 id="launchkubernetesclusteronazureaks">Launch kubernetes cluster on azure (AKS)</h2>

<p>Below these commands assume you have azure cli installed. If not check out <a href="https://docs.microsoft.com/en-us/cli/azure/get-started-with-azure-cli?view=azure-cli-latest">azure cli setup</a></p>

<p>Like previous posts, I also use bash aliases for kubectl. Github project exists <a href="https://github.com/ahmetb/kubectl-aliases">here</a></p>

<p>Find the location you want to create the cluster and what version of kubernetes to run. Then create the cluster with three worker nodes.</p>

<pre><code class="language-bash">    az provider list --query "[?namespace=='Microsoft.ContainerService'].resourceTypes[] | [?resourceType=='managedClusters'].locations[]" -o tsv

    az aks get-versions --location "Australia East" --query "orchestrators[].orchestratorVersion" 

    az group create --name myResourceGroup1 --location "Australia East"

    az aks create --resource-group myResourceGroup1 --name myAKSCluster --node-count 3 --kubernetes-version 1.11.1 --generate-ssh-keys
</code></pre>

<p>It takes a few minutes to spin up the cluster. Once its up, download the kubeconfig to use kubectl locally </p>

<pre><code class="language-bash">  az aks get-credentials --resource-group myResourceGroup1 --name myAKSCluster
</code></pre>

<p>Verify the nodes are running <code>kgnoowide</code></p>

<p><img src="https://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-2-28-58-pm.png" alt="Istio on Azure AKS"></p>

<h2 id="deployistio">Deploy istio</h2>

<p>Istio is installed in two parts. The first part involves the CLI tooling that will be used to deploy and manage Istio backed services. The second part configures the Kubernetes cluster to support Istio.</p>

<pre><code>curl -L https://git.io/getLatestIstio | sh -  
cd istio-1.0.0  
export PATH=$PWD/bin:$PATH
</code></pre>

<h3 id="configureistiocrds">Configure Istio CRDs</h3>

<p><code>kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml
</code></p>

<h3 id="installistiowithdefaultmutualtlsauthentication">Install Istio with default mutual TLS authentication</h3>

<p><code>kubectl apply -f install/kubernetes/istio-demo-auth.yaml
</code></p>

<p>This will deploy Pilot, Mixer, Ingress-Controller, Egress-Controller and the Istio CA (Certificate Authority).</p>

<p>Check the status of the pods <br>
<code>kgpoowide -n istio-system</code></p>

<p><img src="https://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-2-44-04-pm.png" alt="Istio on Azure AKS"></p>

<h3 id="istioarchitecture">Istio architecture</h3>

<p><img src="https://blog.shanelee.name/content/images/2018/08/istio-architecture.png" alt="Istio on Azure AKS"></p>

<p>The previous step deployed the Istio Pilot, Mixer, Ingress-Controller, Egress-Controller and the Istio CA (Certificate Authority).</p>

<ul>
<li><p><strong>Pilot</strong> - Responsible for configuring the Envoy and Mixer at runtime.</p></li>
<li><p><strong>Envoy</strong> - Sidecar proxies per microservice to handle ingress/egress traffic between services in the cluster and from a service to external services. The proxies form a secure microservice mesh providing a rich set of functions like discovery, rich layer-7 routing, circuit breakers, policy enforcement and telemetry recording/reporting functions.</p></li>
<li><p><strong>Mixer</strong> - Create a portability layer on top of infrastructure backends. Enforce policies such as ACLs, rate limits, quotas, authentication, request tracing and telemetry collection at an infrastructure level.</p></li>
<li><p><strong>Ingress/Egress</strong> - Configure path based routing.</p></li>
<li><p><strong>Istio CA</strong> - Secures service to service communication over TLS. Providing a key management system to automate key and certificate generation, distribution, rotation, and revocation</p></li>
</ul>

<h3 id="deploybookinfoexampleapplication">Deploy BookInfo example application</h3>

<p>This sample deploys a simple application composed of four separate microservices which will be used to demonstrate various features of the Istio service mesh.</p>

<p>Enable default side-car injection <br>
<code>kubectl label namespace default istio-injection=enabled
</code></p>

<p>Deploy the services <br>
<code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
</code></p>

<p>Verify the pods and services are running <br>
<code>kubectl get svc,pod</code></p>

<p>Deploy the ingress <a href="https://istio.io/docs/concepts/traffic-management/#gateways">gateway</a></p>

<p><code>kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml
</code></p>

<p>Now determine the ingress ip and port</p>

<p><code>kubectl get svc istio-ingressgateway -n istio-system
</code>
Set the ingress ip and port</p>

<pre><code class="language-bash">export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')  
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')  
export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')  
</code></pre>

<p>Set the <code>gateway url</code></p>

<p><code>export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
</code></p>

<p>Verify the app is up and running <br>
<code>curl -o /dev/null -s -w "%{http_code}\n" http://${GATEWAY_URL}/productpage
</code></p>

<h5 id="applydefaultdestinationrules">Apply default destination rules</h5>

<p>Before you can use Istio to control the Bookinfo version routing, you need to define the available versions, called subsets, in destination rules.</p>

<p><code>kubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml
</code></p>

<blockquote>
  <p>Deploying a microservice-based application in an Istio service mesh allows one to externally control service monitoring and tracing, request (version) routing, resiliency testing, security and policy enforcement, etc., in a consistent way across the services, for the application as a whole.</p>
</blockquote>

<p>Werner Vogels (CTO of AWS) quoted at AWS Re:Invent  </p>

<blockquote>
  <p>"In the future, all the code you ever write will be business logic." </p>
</blockquote>

<p>Service mesh goes along way in helping you succeed that statement.</p>

<h5 id="controlrouting">Control Routing</h5>

<p>One of the main features of Istio is its traffic management. As a Microservice architectures scale, there is a requirement for more advanced service-to-service communication control.</p>

<h6 id="userbasedtestingrequestrouting">User Based Testing / Request Routing</h6>

<p>One aspect of traffic management is controlling traffic routing based on the HTTP request, such as user agent strings, IP address or cookies.</p>

<p>The example below will send all traffic for the user "jason" to the reviews:v2, meaning they'll only see the black stars.</p>

<p><code>kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml</code></p>

<p>Visit the product page and signin as a user jason (password jason)</p>

<h6 id="trafficshapingforcanaryreleases">Traffic Shaping for Canary Releases</h6>

<p>The ability to split traffic for testing and rolling out changes is important. This allows for A/B variation testing or deploying canary releases.</p>

<p>The rule below ensures that 50% of the traffic goes to reviews:v1 (no stars), or reviews:v3 (red stars).</p>

<p><code>kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml</code></p>

<p><img src="https://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-10-00-pm.png" alt="Istio on Azure AKS"></p>

<h4 id="newreleases">New Releases</h4>

<p>Given the above approach, if the canary release were successful then we'd want to move 100% of the traffic to reviews:v3.</p>

<p><code>kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-v3.yaml</code></p>

<h5 id="listallroutes">List all routes</h5>

<pre><code class="language-bash">$ istioctl get virtualservices
VIRTUAL-SERVICE NAME   GATEWAYS           HOSTS     #HTTP     #TCP      NAMESPACE   AGE  
bookinfo               bookinfo-gateway   *             1        0      default     20m  
reviews                                   reviews       1        0      default     5m  
</code></pre>

<h5 id="accessmetrics">Access Metrics</h5>

<p><img alt="Istio on Azure AKS" src="https://blog.shanelee.name/content/images/2018/08/prometheus-icon-color-1.png" style="width: 250px; height:250px"></p>

<p>With Istio's insight into how applications communicate, it can generate profound insights into how applications are working and performance metrics.</p>

<p>Send traffic to the application</p>

<pre><code class="language-bash">while true; do  
  curl -s http://$GATEWAY_URL/productpage &gt; /dev/null
  echo -n .;
  sleep 0.2
done  
</code></pre>

<p>Setup port forwarding  </p>

<pre><code class="language-bash">kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath='{.items[0].metadata.name}') 9090:9090 &amp;  
</code></pre>

<p>View metrics in <a href="http://localhost:9090/graph#%5B%7B%22range_input%22%3A%221h%22%2C%22expr%22%3A%22istio_double_request_count%22%2C%22tab%22%3A1%7D%5D">Prometheus UI</a></p>

<p>The provided link opens the Prometheus UI and executes a query for values of the <code>istio_double_request_count</code> metric.</p>

<p><img src="https://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-26-44-pm.png" alt="Istio on Azure AKS"></p>

<p>Prometheus was recently <a href="https://www.cncf.io/announcement/2018/08/09/prometheus-graduates/">promoted</a> from CNCF as a graduate project, following kubernetes.</p>

<h5 id="grafana">Grafana</h5>

<p>Verify the services are up  </p>

<pre><code>kubectl -n istio-system get svc grafana prometheus  
</code></pre>

<p>Open the Istio Dashboard via the Grafana UI.</p>

<p>In Kubernetes environments, execute the following command:</p>

<pre><code>$ kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000 &amp;
</code></pre>

<p>Visit <code>http://localhost:3000/dashboard/db/istio-mesh-dashboard</code> in your web browser.</p>

<p><img src="https://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-35-45-pm.png" alt="Istio on Azure AKS"></p>

<p>This gives the global view of the Mesh along with services and workloads in the mesh. </p>

<p>For more info checkout <a href="https://istio.io/docs/tasks/telemetry/using-istio-dashboard/">https://istio.io/docs/tasks/telemetry/using-istio-dashboard/</a></p>

<h5 id="distributedtracing">Distributed Tracing</h5>

<p><img alt="Istio on Azure AKS" src="https://blog.shanelee.name/content/images/2018/08/jaeger-icon-color.png" style="width: 250px; height:250px"></p>

<p>This task shows you how Istio-enabled applications can be configured to collect trace spans. After completing this task, you should understand all of the assumptions about your application and how to have it participate in tracing, regardless of what language/framework/platform you use to build your application.</p>

<h6 id="accessingthedashboard">Accessing the dashboard</h6>

<p>Setup access to the Jaeger dashboard by using port-forwarding:</p>

<pre><code>$ kubectl port-forward -n istio-system $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath='{.items[0].metadata.name}') 16686:16686 &amp;
</code></pre>

<p>Access the Jaeger dashboard by opening your browser to <code>http://localhost:16686</code>.</p>

<p>From the left-hand pane of the Jaeger dashboard, select <code>productpage</code> from the Service drop-down list and click Find Traces. You should see something similar to the following:</p>

<p><img src="https://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-43-04-pm.png" alt="Istio on Azure AKS"></p>

<p>If you click on the top (most recent) trace, you should see the details corresponding to your latest refresh of the /productpage. The page should look something like this:</p>

<p><img src="https://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-43-16-pm.png" alt="Istio on Azure AKS"></p>

<p>And there you have it. For more information on istio check out <a href="https://istio.io/">https://istio.io/</a></p>

<p>Dont forget to delete your cluster after your finished!</p>

<p>Stay tuned for more posts on <strong>kubernetes</strong>.</p>]]></content:encoded></item><item><title><![CDATA[Test drive heptio sonobuoy diagnostic kubernetes tool]]></title><description><![CDATA[<blockquote>
  <p>Heptio Sonobuoy is a diagnostic tool that makes it easier to understand the state of a Kubernetes cluster by running a set of Kubernetes conformance tests in an accessible and non-destructive manner. </p>
</blockquote>

<p>Heptio have come out with some useful kubernetes tools; sonobuoy being one. </p>

<p>In this post, I will spin</p>]]></description><link>https://blog.shanelee.name/2018/02/03/test-drive-heptio-sonobuoy-diagnostic-kubernetes-tool/</link><guid isPermaLink="false">395f2c06-dbbf-46ae-a335-0c12203079ef</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[docker]]></category><category><![CDATA[aws]]></category><category><![CDATA[heptio]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 03 Feb 2018 09:33:23 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2018/08/joseph-barrientos-93565.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
  <img src="https://blog.shanelee.name/content/images/2018/08/joseph-barrientos-93565.jpg" alt="Test drive heptio sonobuoy diagnostic kubernetes tool"><p>Heptio Sonobuoy is a diagnostic tool that makes it easier to understand the state of a Kubernetes cluster by running a set of Kubernetes conformance tests in an accessible and non-destructive manner. </p>
</blockquote>

<p>Heptio have come out with some useful kubernetes tools; sonobuoy being one. </p>

<p>In this post, I will spin up a kubernetes cluster on AWS using latest version of kops <strong>(1.8)</strong> and test drive sonobuoy scanner tool.</p>

<p>Rather than install kops and kubectl locally, I have published a docker image that you can use as a utility container.</p>

<p><a href="https://hub.docker.com/r/shanelee007/alpine-kops/">Alpine kops</a> docker image includes kops, kubectl, terraform, aws cli and helm. The swiss army knife for kubernetes!! </p>

<h2 id="kubernetesinstallation">Kubernetes installation</h2>

<p>Run the container  </p>

<pre><code class="language-bash">docker run --rm -it \  
  -v "$HOME"/.ssh:/root/.ssh:ro \
  -v "$HOME"/.aws:/root/.aws:ro \
  -v "$HOME"/.kube:/root/.kube:rw \
  -v "$HOME"/.helm:/root/.helm:rw \
  -v "$(pwd)":/workdir \
  -w /workdir \
  shanelee007/alpine-kops
</code></pre>

<p>Then create the cluster on AWS. Here I am creating one master instance and two worker nodes</p>

<pre><code class="language-bash">  kops create cluster --v=0 \
    --cloud=aws \
    --node-count 2 \
    --master-size=m3.medium \
    --master-zones=ap-southeast-2a \
    --zones ap-southeast-2a,ap-southeast-2c \
    --name= ${NAME} \
    --node-size=m3.medium \
    --node-volume-size=20
</code></pre>

<p>Before scanning the cluster, I will deploy a few applications.</p>

<p>Bitnami have come out with kubeapps, to easily deploy apps on your cluster.</p>

<h2 id="bitnamikubeapps">Bitnami KubeApps</h2>

<blockquote>
  <p>Kubeapps is a Kubernetes dashboard that supercharges your Kubernetes cluster with simple browse and click deployment of apps in any format. </p>
</blockquote>

<h3 id="installation">Installation</h3>

<pre><code class="language-bash">sudo curl -L https://github.com/kubeapps/installer/releases/download/v0.2.0/kubeapps-linux-amd64 -o /usr/local/bin/kubeapps &amp;&amp; sudo chmod +x /usr/local/bin/kubeapps  
</code></pre>

<p>To see what it installs, run dry run first</p>

<pre><code class="language-bash">  kubeapps up --dry-run -o yaml
</code></pre>

<p>Once your happy, lets kick it off</p>

<pre><code class="language-bash"> kubeapps up
</code></pre>

<h3 id="dashboard">Dashboard</h3>

<p>Once Kubeapps is installed, securely access the Kubeapps Dashboard from your system by running:</p>

<pre><code class="language-bash">kubeapps dashboard  
</code></pre>

<p>This will start an HTTP proxy for secure access to the Kubeapps Dashboard and launch your default browser to access it. </p>

<h3 id="deploywordpress">Deploy wordpress</h3>

<p>Using the "Charts" menu from the Dashboard welcome page I will select wordpress application from the list of charts in the official Kubernetes chart repository.</p>

<h2 id="sonobuoy">Sonobuoy</h2>

<p>Now lets install sonobuoy on the cluster and run the diagnostics.</p>

<p>You will find the steps <a href="https://scanner.heptio.com">here</a></p>

<p>Once you run the command, you will see in the browser to see the conformance results.</p>

<p><img src="https://blog.shanelee.name/content/images/2018/02/Screen-Shot-2018-02-03-at-9-35-17-pm.png" alt="Test drive heptio sonobuoy diagnostic kubernetes tool"></p>

<p>You will see two pods running in the namespace heptio-sonobuoy</p>

<pre><code class="language-bash">NAME                                READY     STATUS    RESTARTS   AGE  
sonobuoy                            3/3       Running   0          5m  
sonobuoy-e2e-job-bf586487f2f64f0b   2/2       Running   0          4m  
</code></pre>

<p>It may take up to 60 mins to run the tests. So sit back and relax... üòâ</p>

<p>To see what is happening you can use kubetail to tail the logs</p>

<pre><code class="language-bash">kubetail sonobuoy -n heptio-sonobuoy  
kubetail sonobuoy-e2e-job-bf586487f2f64f0b -n heptio-sonobuoy
</code></pre>

<h3 id="sonobuoyresults">Sonobuoy results</h3>

<p>Once it finishes, you can download the results and keep the report by exporting as a pdf.</p>

<p><img src="https://blog.shanelee.name/content/images/2018/02/Screen-Shot-2018-02-03-at-10-19-17-pm.png" alt="Test drive heptio sonobuoy diagnostic kubernetes tool"></p>

<p>All tests passed. There you have it!</p>

<p>Stay tuned for more kubernetes goodness.. ‚öìÔ∏è</p>]]></content:encoded></item><item><title><![CDATA[Logging on kubernetes with fluentd and elasticsearch 6]]></title><description><![CDATA[How to configure EFK (Elastic-Fluentd-Kibana) stack on kubernetes for logging]]></description><link>https://blog.shanelee.name/2017/12/16/logging-on-kubernetes-with-fluentd-and-elasticsearch-6/</link><guid isPermaLink="false">cac30a75-368d-462f-aee8-9c510bfd9de1</guid><category><![CDATA[elasticsearch]]></category><category><![CDATA[kubernetes]]></category><category><![CDATA[docker]]></category><category><![CDATA[ingress]]></category><category><![CDATA[nginx]]></category><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[curator]]></category><category><![CDATA[fluentd]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 16 Dec 2017 13:06:59 GMT</pubDate><media:content url="https://blog.shanelee.name/content/images/2018/01/robert-larsson-64547-1.jpg" medium="image"/><content:encoded><![CDATA[<h1 id="tldr">TLDR</h1>

<img src="https://blog.shanelee.name/content/images/2018/01/robert-larsson-64547-1.jpg" alt="Logging on kubernetes with fluentd and elasticsearch 6"><p>The solution I have used in the past for logging in kubernetes clusters is EFK (Elastic-Fluentd-Kibana). Alot of you have probably heard of ELK stack but I find that logstash is more heavyweight and does not provide the same output plugins as fluentd.</p>

<p><img alt="Logging on kubernetes with fluentd and elasticsearch 6" src="https://blog.shanelee.name/content/images/2017/12/icon-elasticsearch-bb-1.svg" style="width: 300px; height:300px"></p>

<h2 id="elasticsearch">Elasticsearch</h2>

<p>Elasticsearch 6 has just been <a href="https://www.elastic.co/blog/elasticsearch-6-0-0-released">announced</a> with some major performance improvements.</p>

<p>You have a few options to deploy elasticsearch - elastic cloud or spin up your own ES cluster in kubernetes. But I have decided to go with the AWS managed service. It now has <a href="https://aws.amazon.com/blogs/aws/amazon-elasticsearch-service-now-supports-vpc/">VPC support</a> and the new release 6.0. </p>

<p>The reason being that administrating Elasticsearch can be a lot of work, as many people experienced with the system will tell you it can be tricky to keep running smoothly and that it‚Äôs a task better outsourced to an external service.</p>

<p>I recently read dubsmash story <a href="https://stackshare.io/dubsmash/dubsmash-scaling-to-200-million-users-with-3-engineers">https://stackshare.io/dubsmash/dubsmash-scaling-to-200-million-users-with-3-engineers</a> and how they focus on the product deliverables and not infrastructure. If you have a small team or not in the business of managing services such as ES, i find its best to outsource..</p>

<p>Following the 12 factor principles for logging <a href="https://12factor.net/logs">https://12factor.net/logs</a> all applications that are containerised (using docker) should log to STDOUT.</p>

<blockquote>
  <p>A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout.</p>
</blockquote>

<p>Whether your using winston for nodeJS or console appender for spring boot, this is the recommended way. </p>

<p>Every pod in a <strong>K8S</strong> cluster has its standard output and standard error captured and stored in the /var/log/containers/ node directory. </p>

<p>Now you need a logging agent ( or logging shipper) to ingest these logs and output to a target. </p>

<h2 id="fluentd">FluentD</h2>

<p><img src="https://blog.shanelee.name/content/images/2017/12/container_logging-1024x536.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<p>Fluentd is an open-source framework for data collection that unifies the collection and consumption of data in a pluggable manner. There are several producer and consumer loggers for various kinds of applications. It has a huge suite of <a href="https://www.fluentd.org/plugins">plugins</a> to choose from. </p>

<p>Fluentd will be deployed as a <strong>daemonset</strong> on the kubernetes cluster.</p>

<p>Kubernetes logs the content of the stdout and stderr streams of a pod to a file. It creates one file for each container in a pod. The default location for these files is /var/log/containers . The filename contains the pod name, the namespace of the pod, the container name, and the container id. The file contains one JSON object per line of the two streams stdout and stderr. </p>

<p>A DaemonSet ensures that a certain pod is scheduled to each kubelet exactly once. The fluentd pod mounts the <strong>/var/lib/containers/</strong> host volume to access the logs of all pods scheduled to that kubelet.</p>

<h2 id="whatsmissinggeo">Whats missing - Geo</h2>

<p><img src="https://blog.shanelee.name/content/images/2017/12/joao-silas-72562.jpg" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<p>I found with the examples online for fluentd daesmonset there was none that supported <strong>geoip</strong>. Coreos offering and fluentd official kubernetes daemonset do not provide this feature. </p>

<p>Geoip is a very useful tool when inspecting access logs through kibana. I use nginx  ingress controller in kubernetes and I wanted to see where incoming requests arose geographically. </p>

<p>Knowing from where in the world people are accessing your website is important not only for troubleshooting and operational intelligence but also for other use cases such as business intelligence</p>

<h3 id="ingressandnginx">Ingress and Nginx</h3>

<p>I use nginx as a reverse proxy behind AWS ELB to manage my routing. By default, nginx does not output to json. And instead of figuring out the fluentd nginx parser I decided to configure nginx to enable json logging. </p>

<p>Sample conf can be seen below:</p>

<pre><code class="language-yaml">kind: ConfigMap  
apiVersion: v1  
metadata:  
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app: ingress-nginx
data:  
  use-proxy-protocol: "false"
  log-format-escape-json: "true"
  log-format-upstream: '{"proxy_protocol_addr": "$proxy_protocol_addr","remote_addr": "$remote_addr", "proxy_add_x_forwarded_for": "$proxy_add_x_forwarded_for",
   "request_id": "$request_id","remote_user": "$remote_user", "time_local": "$time_local", "request" : "$request", "status": "$status", "vhost": "$host","body_bytes_sent": "$body_bytes_sent",
   "http_referer":  "$http_referer", "http_user_agent": "$http_user_agent", "request_length" : "$request_length", "request_time" : "$request_time",
    "proxy_upstream_name": "$proxy_upstream_name", "upstream_addr": "$upstream_addr",  "upstream_response_length": "$upstream_response_length",
    "upstream_response_time": "$upstream_response_time", "upstream_status": "$upstream_status"}'
</code></pre>

<h3 id="fluentddockerimage">Fluentd docker image</h3>

<p>I then extended the fluentd debian elasticsearch docker image to install the geo-ip plugin and also update the max mind database.</p>

<p>Docker Image can be found <a href="https://hub.docker.com/r/shanelee007/fluentd-kubernetes/">here</a> on docker hub. Tag is <strong>v0.12-debian-elasticsearch-geo</strong></p>

<pre><code class="language-bash">docker pull shanelee007/fluentd-kubernetes:v0.12-debian-elasticsearch-geo  
</code></pre>

<p>Now I needed to amend the fluentd config to filter my nginx access logs and translate ip address to geo co-ordinates. </p>

<p>Sample config in the yaml file for daemonset with updated database is</p>

<pre><code class="language-yaml">  geoip-filter.conf: |
    &lt;filter kube.ingress-nginx.nginx-ingress-controller&gt;
        type geoip

        # Specify one or more geoip lookup field which has ip address (default: host)
        # in the case of accessing nested value, delimit keys by dot like 'host.ip'.
        geoip_lookup_key  remote_addr

        # Specify optional geoip database (using bundled GeoLiteCity databse by default)
        geoip_database    "/home/fluent/GeoLiteCity.dat"

        # Set adding field with placeholder (more than one settings are required.)
        &lt;record&gt;
          city            ${city["remote_addr"]}
          lat             ${latitude["remote_addr"]}
          lon             ${longitude["remote_addr"]}
          country_code3   ${country_code3["remote_addr"]}
          country         ${country_code["remote_addr"]}
          country_name    ${country_name["remote_addr"]}
          dma             ${dma_code["remote_addr"]}
          area            ${area_code["remote_addr"]}
          region          ${region["remote_addr"]}
          geoip           '{"location":[${longitude["remote_addr"]},${latitude["remote_addr"]}]}'
        &lt;/record&gt;

        # To avoid get stacktrace error with `[null, null]` array for elasticsearch.
        skip_adding_null_record  true

        # Set log_level for fluentd-v0.10.43 or earlier (default: warn)
        log_level         info

        # Set buffering time (default: 0s)
        flush_interval    1s
    &lt;/filter&gt;
</code></pre>

<p>I also updated the elasticsearch template to version 6 as there was issues with version 5.</p>

<p>Now to the fun part... testing it out!</p>

<h2 id="tryityourself">Try it yourself</h2>

<p>This tutorial can be executed in less than 15 minutes, as log as you already have:</p>

<ul>
<li><p>Kubernetes cluster up</p></li>
<li><p>Nginx ingress installed</p></li>
</ul>

<h3 id="installing">Installing</h3>

<p>Github project for creating all resources in kubernetes can be found at <a href="https://github.com/shavo007/k8s-ingress-letsencrypt/tree/master">https://github.com/shavo007/k8s-ingress-letsencrypt/tree/master</a></p>

<p>To create the namespace and manifests for logging, the only change you need is to update the elasticsearch endpoint in the configmap.</p>

<p>File is located at <a href="https://github.com/shavo007/k8s-ingress-letsencrypt/blob/master/resources/logging/fluentd-configmap.yaml#L291">https://github.com/shavo007/k8s-ingress-letsencrypt/blob/master/resources/logging/fluentd-configmap.yaml#L291</a></p>

<p>Then you can create the resources. </p>

<h4 id="kubectlaliases">Kubectl aliases</h4>

<p>For the commands below I am using bash aliases. Aliases save me alot of time when I am querying a cluster. You can find the github project <a href="https://github.com/ahmetb/kubectl-aliases">here</a></p>

<pre><code class="language-bash">ka resources/logging --record  
</code></pre>

<p>Verify the pods are created</p>

<pre><code class="language-bash">kgpon logging  
</code></pre>

<p>If you have the dashboard installed you can inspect the logs there or you can tail the logs from the command line using <a href="https://github.com/johanhaleby/kubetail">kubetail</a></p>

<pre><code class="language-bash">kubetail fluentd -n logging  
</code></pre>

<h2 id="kibana">Kibana</h2>

<p><img alt="Logging on kubernetes with fluentd and elasticsearch 6" src="https://blog.shanelee.name/content/images/2017/12/icon-kibana-bb.svg" style="width: 300px; height:300px"></p>

<p>Now access kibana - GUI for elasticsearch</p>

<p>You should now see your logs ingested in elasticsearch. </p>

<p><img src="https://blog.shanelee.name/content/images/2017/12/Screen-Shot-2017-12-26-at-3-10-18-pm.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<h3 id="tilemapvisualization">Tile map visualization</h3>

<p>There is a limitation with managed service and tile map view. I customized the settings to use WMS compliant map server. See below:</p>

<p><img src="https://blog.shanelee.name/content/images/2017/12/Screen-Shot-2017-12-26-at-3-14-43-pm.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<p>Once you pick the pattern, and assuming your geopoints are correctly mapped, Kibana will automatically populate the visualisation settings such as which field to aggregate on, and display the map almost instantly.</p>

<p>I have defined a visualisation for heatmap. This helps in visualizing geospatial data. You can go to management and import the json file from here  <a href="https://github.com/shavo007/k8s-ingress-letsencrypt/blob/master/resources/assets/export.json">https://github.com/shavo007/k8s-ingress-letsencrypt/blob/master/resources/assets/export.json</a></p>

<p>This includes multiple visualizations and a dashboard.</p>

<p><img src="https://blog.shanelee.name/content/images/2017/12/Screen-Shot-2017-12-17-at-11-47-26-am.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<h4 id="inaccurategeolocation">Inaccurate Geolocation</h4>

<p>You may find the IP is matched to an inaccurate location. Be aware that the free Maxmind database that is used is ‚Äúcomparable to, but less accurate than, MaxMind‚Äôs GeoIP2 databases‚Äù, and, ‚ÄúIP geolocation is inherently imprecise. Locations are often near the center of the population.‚Äù See the MaxMind site for further details.</p>

<h4 id="widgets">Widgets</h4>

<p>You can build up more widgets, such as url count or count by country.</p>

<p><img src="https://blog.shanelee.name/content/images/2017/12/Screen-Shot-2017-12-26-at-3-43-45-pm.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<h2 id="cleanupoflogindices">Cleanup of log indices</h2>

<p>For cleanup of old log indices there is a tool called <a href="https://github.com/elastic/curator">curator</a>. I found a serverless option <a href="https://github.com/cloudreach/aws-lambda-es-cleanup">https://github.com/cloudreach/aws-lambda-es-cleanup</a> and created the lambda function via <strong>terraform</strong>.</p>

<p>You can easily schedule the lambda function to cleanup log indices greater than x no. of days.</p>

<p>And thats it! Stay tuned for more posts on <strong>kubernetes</strong>. </p>]]></content:encoded></item><item><title><![CDATA[Kubernetes ingress and sticky sessions]]></title><description><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Recently I had to look at horizontally scaling a traditional web-app on kubernetes. Here i will explain how I achieved it and what ingress controller is and why to use it.</p>

<p>I  assume you know what pods are so I will quickly breakdown service and ingress resources.</p>

<h2 id="service">Service</h2>]]></description><link>https://blog.shanelee.name/2017/10/15/kubernetes-ingress-and-sticky-sessions/</link><guid isPermaLink="false">52c2d7b6-231c-4d3b-8fcd-7f4fa090d576</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[docker]]></category><category><![CDATA[ingress]]></category><category><![CDATA[sticky]]></category><category><![CDATA[elb]]></category><category><![CDATA[nginx]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 15 Oct 2017 14:21:48 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Recently I had to look at horizontally scaling a traditional web-app on kubernetes. Here i will explain how I achieved it and what ingress controller is and why to use it.</p>

<p>I  assume you know what pods are so I will quickly breakdown service and ingress resources.</p>

<h2 id="service">Service</h2>

<blockquote>
  <p>Service is a logical abstraction communication layer to pods. During normal operations pods get‚Äôs created, destroyed, scaled out, etc.</p>
</blockquote>

<p>A Service make‚Äôs it easy to always connect to the pods by connecting to their service which stays stable during the pod life cycle. A important thing about services are what their type is, it determines how the service expose itself to the cluster or the internet. Some of the service types are :</p>

<ul>
<li><p>ClusterIP Your service is only expose internally to the cluster on the internal cluster IP. A example would be to deploy Hasicorp‚Äôs vault and expose it only internally.</p></li>
<li><p>NodePort Expose the service on the EC2 Instance on the specified port. This will be exposed to the internet. Off course it this all depends on your AWS Security group / VPN rules.</p></li>
<li><p>LoadBalancer Supported on Amazon and Google cloud, this creates the cloud providers your using load balancer. So on Amazon it creates a ELB that points to your service on your cluster.</p></li>
<li><p>ExternalName Create a CNAME dns record to a external domain.</p></li>
</ul>

<p>For more information about Services look at <a href="https://kubernetes.io/docs/concepts/services-networking/service/">https://kubernetes.io/docs/concepts/services-networking/service/</a></p>

<h2 id="ingress">Ingress</h2>

<blockquote>
  <p>An Ingress is a collection of rules that allow inbound connections to reach the cluster services</p>
</blockquote>

<p>You define a number of <strong>rules</strong> to access a <strong>service</strong></p>

<h2 id="scenario">Scenario</h2>

<p>Imagine this scenario, you have a cluster running, on Amazon, you have multiple applications deployed to it, some are jvm microservices (spring boot) running inside embedded tomcat, and to add to the mix, you have a couple of SPA sitting in a Apache web server that serves static content. </p>

<p>All applications needs to have TLS, some of the api‚Äôs endpoints have changed, but you still have to serve the old endpoint path, so you need to do some sort of path rewrite. How do you expose everything to the internet? The obvious answer is create a type <strong>LoadBalancer</strong> service for each, but, then multiple ELB‚Äôs will be created, you have to deal with TLS termination at each ELB, you have to CNAME your applications/api‚Äôs domain names to the right ELB‚Äôs, and in general just have very little control over the ELB.</p>

<p>Enter Ingress Controllers. üëç</p>

<h3 id="whatisaningresscontroller">What is an ingress controller?</h3>

<blockquote>
  <p>An Ingress Controller is a daemon, deployed as a Kubernetes Pod, that watches the apiserver's /ingresses endpoint for updates to the Ingress resource. Its job is to satisfy requests for Ingresses.</p>
</blockquote>

<p>You deploy a ingress controller, create a type LoadBalancer service for it, and it sits and monitors Kubernetes api server‚Äôs <strong>/ingresses</strong> endpoint and acts as a reverse proxy for the ingress rules it found there. </p>

<p>You then deploy your application and expose it‚Äôs service as a type NodePort, and create ingress rules for it. The ingress controller then picks up the new deployed service and proxy traffic to it from outside.</p>

<p>Following this setup, you only have one ELB then on Amazon, and a central place at the ingress controller to manage the traffic coming into your cluster to your applications.</p>

<p>To visualise how this works, check out this little guy! Traefik is one implementation you can use as an ingress.</p>

<p><img src="https://blog.shanelee.name/content/images/2017/10/architecture.png" alt=""></p>

<p>But I have chosen nginx ingress controller instead as it supports sticky sessions and as a reverse proxy is extremely popular solution.</p>

<p>So lets get to the interesting part; <strong>coding</strong>!!!</p>

<h2 id="demo">Demo</h2>

<p>I am going to setup a kubernetes gossip cluster on AWS using kops. Then create nginx ingress controller and reverse proxy to a sample app called echoheader. </p>

<p>To setup a k8s cluster on AWS, follow the guide at <a href="https://github.com/shavo007/k8s-ingress">https://github.com/shavo007/k8s-ingress</a></p>

<p>If you do not want to install kops and the other tools needed, I have built a simple docker image that you can use instead. </p>

<p><a href="https://store.docker.com/community/images/shanelee007/alpine-kops">https://store.docker.com/community/images/shanelee007/alpine-kops</a></p>

<p>This includes:</p>

<ul>
<li>Kops</li>
<li>Kubectl</li>
<li>AWS CLI</li>
<li>Terraform</li>
</ul>

<p>Once you have the cluster what we need to do is setup a default backend service for nginx.</p>

<p>The default backend is the default service that nginx falls backs to if if cannot route a request successfully. The default backend needs to satisfy the following two requirements :</p>

<p>serves a 404 page at / <br>
serves 200 on a /healthz</p>

<p>See more at <a href="https://github.com/kubernetes/ingress-nginx/tree/master/deploy">https://github.com/kubernetes/ingress-nginx/tree/master/deploy</a></p>

<p>Run the mandatory commands and install without RBAC roles.</p>

<p>Then install layer 7 service on AWS <br>
<a href="https://github.com/kubernetes/ingress-nginx/tree/master/deploy#aws">https://github.com/kubernetes/ingress-nginx/tree/master/deploy#aws</a> or install the service defined in my repo</p>

<pre><code class="language-bash">kubectl apply -f ingress-nginx-svc.yaml  
</code></pre>

<p>When you run these commands, it created a deployment with one replica of the nginx-ingress-controller and a service for it of type LoadBalancer which created a ELB for us on AWS. Let‚Äôs confirm that. Get the service :</p>

<pre><code class="language-bash">kubectl get services -n ingress-nginx  -o wide | grep nginx  
</code></pre>

<p>We can now test the default back-end</p>

<pre><code class="language-bash">ELB=$(kubectl get svc ingress-nginx -n ingress-nginx  -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

curl $ELB  
</code></pre>

<p>You should see the following:  </p>

<pre><code class="language-bash">default backend - 404  
</code></pre>

<p>All good so far.. </p>

<p>This means everything is working correctly and the ELB forwarded traffic to our nginx-ingress-controller and the nginx-ingress-controller passed it along to the default-backend-service that we deployed.</p>

<h4 id="deployourapplication">Deploy our application</h4>

<p>Now run</p>

<pre><code class="language-bash">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/images/echoheaders/echo-app.yaml

kubectl apply -f ingress.yaml  
</code></pre>

<p>This will create deployment and service for echo-header app. This app simply returns information about the http request as output.</p>

<p>If you look at the ingress resource, you will see annotations defined. </p>

<p><code>ingress.kubernetes.io/ssl-redirect: "true"</code>  will redirect http to https.</p>

<p>To view all annotations check out <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md">https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md</a></p>

<p>One ingress rule is to route all requests for virtual host foo.bar.com to service  <strong>echoheaders</strong> on path /backend. So lets test it out!</p>

<pre><code class="language-bash">curl $ELB/backend -H 'Host: foo.bar.com'
</code></pre>

<p>You should get 200 response back with request headers and other info.</p>

<h3 id="stickysessions">Sticky sessions</h3>

<p>Now to one of the main features that nginx provides.  nginx-ingress-controller can handle sticky sessions as it bypass the service level and route directly the pods. More info can be found here <br>
<a href="https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/affinity/cookie">https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/affinity/cookie</a></p>

<p><mark>Update (17/10/2017) examples have been removed from repo! To find out more on the annotations related to sticky session go to <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md#miscellaneous</mark>">https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md#miscellaneous==</a></mark></p>

<p>To test it out we need to first scale our app echo-headers: Lets scale echo-headers deployment to three pods</p>

<pre><code class="language-bash"> kubectl scale --replicas=3 deployment/echoheaders
</code></pre>

<p>Now lets create the sticky ingress</p>

<pre><code class="language-bash">apiVersion: extensions/v1beta1  
kind: Ingress  
metadata:  
  name: nginx-test-sticky
  annotations:
    kubernetes.io/ingress.class: "nginx"
    ingress.kubernetes.io/affinity: "cookie"
    ingress.kubernetes.io/session-cookie-name: "route"
    ingress.kubernetes.io/session-cookie-hash: "sha1"

spec:  
  rules:
  - host: stickyingress.example.com
    http:
      paths:
      - backend:
          serviceName: echoheaders
          servicePort: 80
        path: /foo
</code></pre>

<p>What this setting does it, instruct nginx to use the nginx-sticky-module-ng module (<a href="https://bitbucket.org/nginx-goodies/nginx-sticky-module-ng">https://bitbucket.org/nginx-goodies/nginx-sticky-module-ng</a>) that‚Äôs bundled with the controller to handle all sticky sessions for us.</p>

<pre><code class="language-bash">kubectl apply -f sticky-ingress.yaml  
</code></pre>

<p>There is a very useful tool called kubetail that you can use to tail the logs of a pod and verify the  sticky session behaviour. To install kubetail check out <a href="https://github.com/johanhaleby/kubetail">https://github.com/johanhaleby/kubetail</a></p>

<p>Now in one terminal window, we can tail the logs </p>

<pre><code class="language-bash">kubetail -l app=echoheaders  
</code></pre>

<p>and in another send in multiple requests to the virtual host stickyingress.example.com</p>

<pre><code class="language-bash">curl -D cookies.txt $ELB/foo -H 'Host: stickyingress.example.com'


while true; do sleep 1;curl -b cookies.txt $ELB/foo -H 'Host: stickyingress.example.com';done
</code></pre>

<p>When the backend server is removed, the requests are then re-routed to another upstream server and NGINX creates a new cookie, as the previous hash became invalid.</p>

<p>As, you can see, requests are sent to the same pod for every subsequent request.</p>

<p><img src="https://blog.shanelee.name/content/images/2017/10/stickySession.gif" alt=""></p>

<h3 id="proxyprotocol">Proxy protocol</h3>

<p>Lots of times you need to pass a user‚Äôs IP address / hostname through to your application. A example would be, to have the hostname of the user in your application logs.</p>

<p>To enable passing along the hostname, enable the below annotation</p>

<pre><code class="language-bash"> service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'
</code></pre>

<p><strong>Update 1(7/10/2017) It looks like this is not needed anymore</strong></p>

<p>For more see <a href="https://github.com/kubernetes/ingress-nginx#source-ip-address">https://github.com/kubernetes/ingress-nginx#source-ip-address</a></p>

<p>To conclude, i have showcased above a subset of features for ingress. Others include path-rewrite, TLS termination, path routing, scaling, rbac, auth and prometheus metrics. For more info check out resources below.</p>

<h2 id="resources">Resources</h2>

<p>For more information visit:</p>

<p>Github project : <a href="https://github.com/shavo007/k8s-ingress">https://github.com/shavo007/k8s-ingress</a></p>

<p>Kubernetes nginx ingress: <a href="https://github.com/kubernetes/ingress-nginx">https://github.com/kubernetes/ingress-nginx</a></p>

<p>External DNS: <a href="https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/nginx-ingress.md">https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/nginx-ingress.md</a></p>

<p>Kubernetes faqs: <br>
<a href="https://github.com/hubt/kubernetes-faq/blob/master/README.md#kubernetes-faq">https://github.com/hubt/kubernetes-faq/blob/master/README.md#kubernetes-faq</a></p>

<p>Alpine-kops: <a href="https://store.docker.com/community/images/shanelee007/alpine-kops">https://store.docker.com/community/images/shanelee007/alpine-kops</a></p>]]></content:encoded></item></channel></rss>