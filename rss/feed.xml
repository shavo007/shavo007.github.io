<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Tech Blog]]></title><description><![CDATA[Thoughts, stories and ideas.]]></description><link>http://blog.shanelee.name/</link><generator>Ghost 0.8</generator><lastBuildDate>Tue, 23 Apr 2019 08:38:09 GMT</lastBuildDate><atom:link href="http://blog.shanelee.name/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Skaffold for local kubernetes development]]></title><description><![CDATA[skaffold for local kubernetes development]]></description><link>http://blog.shanelee.name/2019/02/20/skaffold-for-local-kubernetes-development/</link><guid isPermaLink="false">939f9b10-3288-4a62-83c4-8de5aa50b6f6</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[docker]]></category><category><![CDATA[node]]></category><category><![CDATA[skaffold]]></category><category><![CDATA[graphql]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Wed, 20 Feb 2019 06:37:09 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2019/02/jacek-dylag-680347-unsplash-1.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
  <img src="http://blog.shanelee.name/content/images/2019/02/jacek-dylag-680347-unsplash-1.jpg" alt="Skaffold for local kubernetes development"><p>Easy and Repeatable Kubernetes Development</p>
</blockquote>

<p><strong>TLDR</strong> Below, I will showcase how to install and use skaffold for local development with kubernetes.</p>

<p>Currently <strong>(20/02/2019)</strong>, skaffold has nearly <mark>6000</mark> ‚≠ê on github.</p>

<p>I have been using Skaffold for all my new projects that involve cloud native microservices, and it works like a charm on top of <code>Docker Desktop for Mac</code>/Minikube.</p>

<p><strong>Skaffold</strong> is fantastic for local development with kubernetes. I can test locally my changes without having to deploy remotely. This helps speed up my local development and gives me confidence in my changes.</p>

<h2 id="overview">Overview</h2>

<p>Skaffold is a tool to develop containerized applications locally or remotely while deploying them on Kubernetes. It automatically builds and deploys your apps as you change your source code.</p>

<p>Skaffold primarily simplifies the <mark>build ‚Üí deploy ‚Üí refactor ‚Üí repeat</mark> cycle.</p>

<h3 id="skaffoldmodes">Skaffold modes</h3>

<p><img src="http://blog.shanelee.name/content/images/2019/02/skaffold-cmds.jpg" alt="Skaffold for local kubernetes development"></p>

<p>In a single command, Skaffold can:</p>

<ul>
<li>Collects and watches your source code for changes</li>
<li>Syncs files directly to pods if user marks them as syncable</li>
<li>Builds artifacts from the source code</li>
<li>Tests the built artifacts using container-structure-tests</li>
<li>Tags the artifacts</li>
<li>Pushes the artifacts</li>
<li>Deploys the artifacts</li>
<li>Monitors the deployed artifacts</li>
<li>Cleans up deployed artifacts on exit (Ctrl+C)</li>
</ul>

<h2 id="skaffoldfeatures">Skaffold features</h2>

<ul>
<li><p><strong>Remote development:</strong> Skaffold doesn‚Äôt require you to run a local Kubernetes cluster (minikube or docker-for-desktop). It can build/push images locally with docker, and run them on the remote clusters (such as GKE). This is a laptop battery saver!</p></li>
<li><p><strong>More remote development:</strong> You actually don‚Äôt need to run a local docker either. Skaffold can do remote builds using services like Google Container Builder. Although it‚Äôll be slow.</p></li>
<li><p><strong>Tag management:</strong> In your Kubernetes manifests, you leave the image tags out in the ‚Äúimage:‚Äù field, and Skaffold automatically changes the manifests with the new tags as it rebuilds the images.</p></li>
<li><p><strong>Rebuild only what‚Äôs changed:</strong> If your microservices are on separate directories, changing source code for one will not cause rebuild for all images. Skaffold understands which images have been impacted by the change.</p></li>
<li><p><strong>Cleanup on exit:</strong> Terminating ‚Äúskaffold dev‚Äù runs a routine that cleans up the deployed k8s resources. If this fails, you can run ‚Äúskaffold delete‚Äù to clean up deployed artifacts.</p></li>
</ul>

<h2 id="letsgetstarted">Lets get started!</h2>

<p>On mac, you can install skaffold using brew</p>

<pre><code class="language-bash">brew install skaffold  
</code></pre>

<h3 id="localdevelopment">Local development</h3>

<p>Run <code>skaffold init</code> to bootstrap Skaffold config.</p>

<p>Once that is complete, define in the yaml file the location of where your kubernetes manifests are defined.</p>

<p>Sample skaffold yaml file</p>

<pre><code class="language-yaml">apiVersion: skaffold/v1beta5  
kind: Config  
build:  
  artifacts:
  - image: shanelee007/graphql
deploy:  
  kubectl:
    manifests:
    - kubernetes/config.yaml
    - kubernetes/deployment.yaml
    - kubernetes/secret.yaml
profiles:  
- name: dev
  build:
    artifacts:
    - image: shanelee007/graphql
      sync:
        '**/*.js': .
      docker:
        dockerfile: Dockerfile.dev
</code></pre>

<p>Here you can see where I defined my manifest files. Also for local development I have used a <code>profile</code> to define a development dockerfile and utilised the sync feature. </p>

<blockquote>
  <p>profiles feature grants you the freedom to switch tools as you see fit depending on the context.</p>
</blockquote>

<h3 id="localdevelopmentworkflow">Local development workflow</h3>

<p><img src="http://blog.shanelee.name/content/images/2019/02/skaffold_workflow_local.png" alt="Skaffold for local kubernetes development"></p>

<h5 id="syncfilestoyourpodswithskaffold">Sync files to your pods with Skaffold</h5>

<p>With even one change to a file, Skaffold rebuilds the images that depend on that file, pushes them to a registry, and then redeploys the relevant parts of your Kubernetes application. </p>

<p>The Skaffold file sync feature solves this problem. For each image, you can specify which files can be synced directly into a running container. Then, when you modify these files, Skaffold copies them directly into the running container rather than kicking off a full rebuild and redeploy. With Skaffold‚Äôs file sync feature, you can enjoy even faster development!</p>

<p><a href="https://skaffold.dev/docs/how-tos/filesync/">Sync</a> is quite a new feature. Think of it as similar to <code>nodemon</code></p>

<p>I have created my own demo <strong>github</strong> project <a href="https://github.com/shavo007/graphql-playground/tree/master/api#skaffold">here</a> if you want to follow along.</p>

<p>There was an issue with publishing docker image for local development every time I ran skaffold. To prevent this from happening there is global config to disable this.</p>

<pre><code class="language-bash">skaffold config set --global local-cluster true #do not push images after building  
</code></pre>

<h4 id="noteondockerfile">Note on Dockerfile</h4>

<p>In my github project, you can see I use multi-stage approach with my docker <a href="https://github.com/shavo007/graphql-playground/blob/master/api/Dockerfile">files</a></p>

<p>Think of it as a build pipeline as code.</p>

<p>It allows you to selectively copy artifacts from one stage to another, leaving behind everything you don‚Äôt want in the final image. </p>

<p>To analyse your final production image I found a useful tool called <a href="https://github.com/wagoodman/dive">dive</a></p>

<p>It allows you to explore and optimise your docker image size.</p>

<p><img src="http://blog.shanelee.name/content/images/2019/02/Screen-Shot-2019-02-20-at-7-23-39-pm.png" alt="Skaffold for local kubernetes development"></p>

<p>Now we can run skaffold!</p>

<pre><code class="language-bash">skaffold dev -p dev -v=info #run locally/watching changes dev mode  
</code></pre>

<p>If you want to try out the new experimental gui run instead </p>

<pre><code class="language-bash">skaffold dev -p dev -v=info --experimental-gui  
</code></pre>

<p>Console output will look like so:</p>

<p><a href="https://asciinema.org/a/220028" target="_blank"><img src="https://asciinema.org/a/220028.svg" alt="Skaffold for local kubernetes development"></a></p>

<p>Every time you make a src code change, skaffold will watch for these changes and update the pod on the fly. Pretty neat!!</p>

<p><img src="https://media.giphy.com/media/vEgtLzJo8n7qg/giphy.gif" alt="Skaffold for local kubernetes development"></p>

<h4 id="upgrade">Upgrade</h4>

<p>As of <strong>20/02/2019</strong> the latest version is <code>v1beta5</code>. To upgrade just run </p>

<pre><code class="language-bash">brew upgrade skaffold  
skaffold fix --overwrite  
</code></pre>

<p>Thats all for now. In my next post, I will discuss deploying remotely using skaffold.üòÉ</p>

<h2 id="learnmore">Learn more</h2>

<ul>
<li><p>Check out skaffold <a href="https://skaffold.dev/docs/">doc</a> to understand more</p></li>
<li><p>There is numerous <a href="https://github.com/GoogleContainerTools/skaffold/tree/master/examples">examples</a> at the github repo</p></li>
<li><p>My sample github <a href="https://github.com/shavo007/graphql-playground/tree/master/api">repo</a> showcasing graphql api with kubernetes</p></li>
</ul>]]></content:encoded></item><item><title><![CDATA[How I aced the Certified Kubernetes Administrator (CKA) Exam]]></title><description><![CDATA[<p>I took the CKA exam recently and would like to share my own preparation and tips.</p>

<p>The exam is intense as it requires 3 hours of concentrated effort during which you need to solve 24 problems. So timekeeping is very important.  If you are interested to take this exam and</p>]]></description><link>http://blog.shanelee.name/2018/10/17/how-i-aced-the-certified-kubernetes-administrator-cka-exam/</link><guid isPermaLink="false">ab6a2a07-ad36-4be5-ae74-d629d25a25bb</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[cka]]></category><category><![CDATA[exam]]></category><category><![CDATA[kubectl]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Wed, 17 Oct 2018 07:29:30 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2018/10/clement-h-544786-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://blog.shanelee.name/content/images/2018/10/clement-h-544786-unsplash.jpg" alt="How I aced the Certified Kubernetes Administrator (CKA) Exam"><p>I took the CKA exam recently and would like to share my own preparation and tips.</p>

<p>The exam is intense as it requires 3 hours of concentrated effort during which you need to solve 24 problems. So timekeeping is very important.  If you are interested to take this exam and have second thought about it, my sincere advice is, just do it!</p>

<h2 id="gettingstarted">Getting started</h2>

<p>As part of black friday deal last year I bought from linux foundation the package deal. It consisted of the CKA exam and linux foundation kubernetes fundamentals <a href="https://training.linuxfoundation.org/training/kubernetes-fundamentals/">course</a>.</p>

<p>In a previous contract role I had worked with kubernetes also on a daily basis. So I was familiar with the basic concepts.</p>

<h2 id="preparation">Preparation</h2>

<ul>
<li><p>Understanding the kubernetes documentation is vital here as that is the only resource you are now allowed as part of the exam. Search ‚Äú<strong>What resources am I allowed to access during my exam?</strong>‚Äù at <a href="https://www.cncf.io/certification/cka/faq/">faq section</a></p></li>
<li><p>Familiarising yourself with the official <a href="https://kubernetes.io/docs/concepts/">documentation</a> is a <strong>must</strong>.</p></li>
<li><p>Practice each topic. Since this exam is all about solving problems, you need to practice a lot. You can use katacoda <a href="https://www.katacoda.com/courses/kubernetes/playground">playground</a> or this <a href="https://labs.play-with-k8s.com/">labs</a> site by Docker. These two tools helped me a lot during preparation. Katacoda also has several scenario based topics and it is important to practice <a href="https://www.katacoda.com/courses/kubernetes">these</a>.</p></li>
<li><p>For each topic I found this <a href="https://cka-exam.blog/">blog</a>  extremely useful to go through.
<em>Practice, practice, practice</em>. Get used to deploying on a cluster and using all the <strong>kubectl</strong> commands. Locally on my mac, <em>docker for mac</em> now incorporates kubernetes so no need for minikube anymore.</p></li>
<li><p>Kubernetes the hard way - Kelsey hightowers tutorial on <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">github</a> is vital to pass this exam. Managed services like AKS, EKS or kubeadm are not going to help here. You need to get right under the hood and understand how the control plane works.</p></li>
<li><p>Familiarise yourself with tools such as: openssl, cfssl, systemctl, etcdctl (for managing etcd)</p></li>
</ul>

<h2 id="examtime">Exam Time</h2>

<p>There are 24 problems and the exam duration is 3 hours. This means you can spend seven and half minutes per question. However, the difficulty range varies; nearly dozen problems are straight and super simple. So, have a strategy for the exam. My strategy was to complete 10 easy questions in the first hour, 8 medium in the second hour and leave final hour for remaining 6 (tough) questions. Once you are done with the question, just double check whether you are saving the output as described in the question and move on (don‚Äôt try to come back and check the answers again). <strong>You will not have time to go back and check!</strong></p>

<h3 id="progresstracking">Progress Tracking</h3>

<p>The CKA exam allows you to write notes to a notebook, which they provide in exam‚Äôs UI. Use it wisely. Once I had my exam available, I used their Notebook to keep track of my progress. It helps you to see your progress and how much left to be finished. </p>

<p>The format I used:</p>

<pre><code class="language-vim"># - scoring - total
1 4 4  
2 5 9  
3 3 12  
4  
5  
...
24  
</code></pre>

<p>The first column is just the number of question (24 in total). The second column‚Ää‚Äî‚Ääscoring, is how much the problem worth and you will get that from the problem description. The third column is ‚Äútotal‚Äù, where you put the total scoring as you go and it will let you know what problems are already completed and how many points you already have. This little bureaucracy will help you to understand how well you are doing and come back to some problems if you decided to skip them before. </p>

<h3 id="kubectlninja">Kubectl Ninja</h3>

<p>Use kubectl to create resources (such as deployment, service, cronjobs etc) instead of creating them from manifest files. It saves lot of time. <br>
I used the <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">cheatsheet</a> heavily during the exam. </p>

<p>First thing I did was copy in the below to notepad. It allowed me to create resources quickly from <code>stdin</code>  </p>

<pre><code class="language-bash">cat &lt;&lt;EOF | kubectl create -f -

EOF  
</code></pre>

<blockquote>
  <p>Take the exam in the morning. </p>
</blockquote>

<p>I took it at 11AM. I went for a quick walk beforehand to get some fresh air and then I was ready to go!</p>

<p>To reiterate, familiarise yourself with the official documentation. It has multiple sections (tasks, concepts and references) for the same topic. So, you should know what/where to look for quickly in the documentation during exam.</p>

<p>Best of luck! Remember you have one free retake if needed.üòÉüöÄ‚öì</p>

<p><img src="http://blog.shanelee.name/content/images/2018/10/CKAcertificate.png" alt="How I aced the Certified Kubernetes Administrator (CKA) Exam"></p>]]></content:encoded></item><item><title><![CDATA[Istio on Azure AKS]]></title><description><![CDATA[How to deploy istio service mesh on azure kubernetes service (AKS) and run bookinfo example application]]></description><link>http://blog.shanelee.name/2018/08/12/istio-on-azure-aks/</link><guid isPermaLink="false">1300cd8a-88a9-43ef-a409-66a2b74c5ad0</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[azure]]></category><category><![CDATA[aks]]></category><category><![CDATA[istio]]></category><category><![CDATA[google]]></category><category><![CDATA[service-mesh]]></category><category><![CDATA[k8s]]></category><category><![CDATA[microservice]]></category><category><![CDATA[grafana]]></category><category><![CDATA[jaeger]]></category><category><![CDATA[tracing]]></category><category><![CDATA[metrics]]></category><category><![CDATA[prometheus]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 12 Aug 2018 04:20:09 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2018/08/peter-gonzalez-621059-unsplash-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://blog.shanelee.name/content/images/2018/08/peter-gonzalez-621059-unsplash-1.jpg" alt="Istio on Azure AKS"><p><img src="http://blog.shanelee.name/content/images/2018/08/istio.png" alt="Istio on Azure AKS"></p>

<p>Istio recently <a href="https://istio.io/about/notes/1.0">announced</a> that they are production ready. Service meshes are becoming an important level of abstraction for a developer using kubernetes. And <a href="https://www.envoyproxy.io/">Envoy</a> is the heartbeat of this service mesh and continues its impressive growth. </p>

<blockquote>
  <p>Istio reduces complexity of managing microservice deployments by providing a uniform way to secure, connect, and monitor microservices.</p>
</blockquote>

<p>Google have also released a managed istio <a href="https://cloudplatform.googleblog.com/2018/07/cloud-services-platform-bringing-the-best-of-the-cloud-to-you.html">service</a></p>

<p>I have previously designed and built cloud native architectures (especially on AWS). But I found that AWS have <em>dropped the ball</em> in relation to kubernetes. Azure and Google managed kuberentes services are more mature and Azure kubernetes offering is even free! </p>

<p><strong>TLDR:</strong> I particulary like Azure AKS and below I will showcase how easy it is to create a cluster and run istio.</p>

<p>Brendan Burns (co-founder of kubernetes and leading the azure container team) and Microsoft have invested wisely I feel in the cloud and are kicking goals. Read the latest <a href="https://azure.microsoft.com/en-us/blog/azure-kubernetes-service-aks-ga-new-regions-new-features-new-productivity/">blog</a> from Brendan on the most recent releases and updates. </p>

<h2 id="launchkubernetesclusteronazureaks">Launch kubernetes cluster on azure (AKS)</h2>

<p>Below these commands assume you have azure cli installed. If not check out <a href="https://docs.microsoft.com/en-us/cli/azure/get-started-with-azure-cli?view=azure-cli-latest">azure cli setup</a></p>

<p>Like previous posts, I also use bash aliases for kubectl. Github project exists <a href="https://github.com/ahmetb/kubectl-aliases">here</a></p>

<p>Find the location you want to create the cluster and what version of kubernetes to run. Then create the cluster with three worker nodes.</p>

<pre><code class="language-bash">    az provider list --query "[?namespace=='Microsoft.ContainerService'].resourceTypes[] | [?resourceType=='managedClusters'].locations[]" -o tsv

    az aks get-versions --location "Australia East" --query "orchestrators[].orchestratorVersion" 

    az group create --name myResourceGroup1 --location "Australia East"

    az aks create --resource-group myResourceGroup1 --name myAKSCluster --node-count 3 --kubernetes-version 1.11.1 --generate-ssh-keys
</code></pre>

<p>It takes a few minutes to spin up the cluster. Once its up, download the kubeconfig to use kubectl locally </p>

<pre><code class="language-bash">  az aks get-credentials --resource-group myResourceGroup1 --name myAKSCluster
</code></pre>

<p>Verify the nodes are running <code>kgnoowide</code></p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-2-28-58-pm.png" alt="Istio on Azure AKS"></p>

<h2 id="deployistio">Deploy istio</h2>

<p>Istio is installed in two parts. The first part involves the CLI tooling that will be used to deploy and manage Istio backed services. The second part configures the Kubernetes cluster to support Istio.</p>

<pre><code>curl -L https://git.io/getLatestIstio | sh -  
cd istio-1.0.0  
export PATH=$PWD/bin:$PATH
</code></pre>

<h3 id="configureistiocrds">Configure Istio CRDs</h3>

<p><code>kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml
</code></p>

<h3 id="installistiowithdefaultmutualtlsauthentication">Install Istio with default mutual TLS authentication</h3>

<p><code>kubectl apply -f install/kubernetes/istio-demo-auth.yaml
</code></p>

<p>This will deploy Pilot, Mixer, Ingress-Controller, Egress-Controller and the Istio CA (Certificate Authority).</p>

<p>Check the status of the pods <br>
<code>kgpoowide -n istio-system</code></p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-2-44-04-pm.png" alt="Istio on Azure AKS"></p>

<h3 id="istioarchitecture">Istio architecture</h3>

<p><img src="http://blog.shanelee.name/content/images/2018/08/istio-architecture.png" alt="Istio on Azure AKS"></p>

<p>The previous step deployed the Istio Pilot, Mixer, Ingress-Controller, Egress-Controller and the Istio CA (Certificate Authority).</p>

<ul>
<li><p><strong>Pilot</strong> - Responsible for configuring the Envoy and Mixer at runtime.</p></li>
<li><p><strong>Envoy</strong> - Sidecar proxies per microservice to handle ingress/egress traffic between services in the cluster and from a service to external services. The proxies form a secure microservice mesh providing a rich set of functions like discovery, rich layer-7 routing, circuit breakers, policy enforcement and telemetry recording/reporting functions.</p></li>
<li><p><strong>Mixer</strong> - Create a portability layer on top of infrastructure backends. Enforce policies such as ACLs, rate limits, quotas, authentication, request tracing and telemetry collection at an infrastructure level.</p></li>
<li><p><strong>Ingress/Egress</strong> - Configure path based routing.</p></li>
<li><p><strong>Istio CA</strong> - Secures service to service communication over TLS. Providing a key management system to automate key and certificate generation, distribution, rotation, and revocation</p></li>
</ul>

<h3 id="deploybookinfoexampleapplication">Deploy BookInfo example application</h3>

<p>This sample deploys a simple application composed of four separate microservices which will be used to demonstrate various features of the Istio service mesh.</p>

<p>Enable default side-car injection <br>
<code>kubectl label namespace default istio-injection=enabled
</code></p>

<p>Deploy the services <br>
<code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
</code></p>

<p>Verify the pods and services are running <br>
<code>kubectl get svc,pod</code></p>

<p>Deploy the ingress <a href="https://istio.io/docs/concepts/traffic-management/#gateways">gateway</a></p>

<p><code>kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml
</code></p>

<p>Now determine the ingress ip and port</p>

<p><code>kubectl get svc istio-ingressgateway -n istio-system
</code>
Set the ingress ip and port</p>

<pre><code class="language-bash">export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')  
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')  
export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')  
</code></pre>

<p>Set the <code>gateway url</code></p>

<p><code>export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
</code></p>

<p>Verify the app is up and running <br>
<code>curl -o /dev/null -s -w "%{http_code}\n" http://${GATEWAY_URL}/productpage
</code></p>

<h5 id="applydefaultdestinationrules">Apply default destination rules</h5>

<p>Before you can use Istio to control the Bookinfo version routing, you need to define the available versions, called subsets, in destination rules.</p>

<p><code>kubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml
</code></p>

<blockquote>
  <p>Deploying a microservice-based application in an Istio service mesh allows one to externally control service monitoring and tracing, request (version) routing, resiliency testing, security and policy enforcement, etc., in a consistent way across the services, for the application as a whole.</p>
</blockquote>

<p>Werner Vogels (CTO of AWS) quoted at AWS Re:Invent  </p>

<blockquote>
  <p>"In the future, all the code you ever write will be business logic." </p>
</blockquote>

<p>Service mesh goes along way in helping you succeed that statement.</p>

<h5 id="controlrouting">Control Routing</h5>

<p>One of the main features of Istio is its traffic management. As a Microservice architectures scale, there is a requirement for more advanced service-to-service communication control.</p>

<h6 id="userbasedtestingrequestrouting">User Based Testing / Request Routing</h6>

<p>One aspect of traffic management is controlling traffic routing based on the HTTP request, such as user agent strings, IP address or cookies.</p>

<p>The example below will send all traffic for the user "jason" to the reviews:v2, meaning they'll only see the black stars.</p>

<p><code>kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml</code></p>

<p>Visit the product page and signin as a user jason (password jason)</p>

<h6 id="trafficshapingforcanaryreleases">Traffic Shaping for Canary Releases</h6>

<p>The ability to split traffic for testing and rolling out changes is important. This allows for A/B variation testing or deploying canary releases.</p>

<p>The rule below ensures that 50% of the traffic goes to reviews:v1 (no stars), or reviews:v3 (red stars).</p>

<p><code>kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml</code></p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-10-00-pm.png" alt="Istio on Azure AKS"></p>

<h4 id="newreleases">New Releases</h4>

<p>Given the above approach, if the canary release were successful then we'd want to move 100% of the traffic to reviews:v3.</p>

<p><code>kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-v3.yaml</code></p>

<h5 id="listallroutes">List all routes</h5>

<pre><code class="language-bash">$ istioctl get virtualservices
VIRTUAL-SERVICE NAME   GATEWAYS           HOSTS     #HTTP     #TCP      NAMESPACE   AGE  
bookinfo               bookinfo-gateway   *             1        0      default     20m  
reviews                                   reviews       1        0      default     5m  
</code></pre>

<h5 id="accessmetrics">Access Metrics</h5>

<p><img alt="Istio on Azure AKS" src="http://blog.shanelee.name/content/images/2018/08/prometheus-icon-color-1.png" style="width: 250px; height:250px"></p>

<p>With Istio's insight into how applications communicate, it can generate profound insights into how applications are working and performance metrics.</p>

<p>Send traffic to the application</p>

<pre><code class="language-bash">while true; do  
  curl -s http://$GATEWAY_URL/productpage &gt; /dev/null
  echo -n .;
  sleep 0.2
done  
</code></pre>

<p>Setup port forwarding  </p>

<pre><code class="language-bash">kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath='{.items[0].metadata.name}') 9090:9090 &amp;  
</code></pre>

<p>View metrics in <a href="http://localhost:9090/graph#%5B%7B%22range_input%22%3A%221h%22%2C%22expr%22%3A%22istio_double_request_count%22%2C%22tab%22%3A1%7D%5D">Prometheus UI</a></p>

<p>The provided link opens the Prometheus UI and executes a query for values of the <code>istio_double_request_count</code> metric.</p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-26-44-pm.png" alt="Istio on Azure AKS"></p>

<p>Prometheus was recently <a href="https://www.cncf.io/announcement/2018/08/09/prometheus-graduates/">promoted</a> from CNCF as a graduate project, following kubernetes.</p>

<h5 id="grafana">Grafana</h5>

<p>Verify the services are up  </p>

<pre><code>kubectl -n istio-system get svc grafana prometheus  
</code></pre>

<p>Open the Istio Dashboard via the Grafana UI.</p>

<p>In Kubernetes environments, execute the following command:</p>

<pre><code>$ kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000 &amp;
</code></pre>

<p>Visit <code>http://localhost:3000/dashboard/db/istio-mesh-dashboard</code> in your web browser.</p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-35-45-pm.png" alt="Istio on Azure AKS"></p>

<p>This gives the global view of the Mesh along with services and workloads in the mesh. </p>

<p>For more info checkout <a href="https://istio.io/docs/tasks/telemetry/using-istio-dashboard/">https://istio.io/docs/tasks/telemetry/using-istio-dashboard/</a></p>

<h5 id="distributedtracing">Distributed Tracing</h5>

<p><img alt="Istio on Azure AKS" src="http://blog.shanelee.name/content/images/2018/08/jaeger-icon-color.png" style="width: 250px; height:250px"></p>

<p>This task shows you how Istio-enabled applications can be configured to collect trace spans. After completing this task, you should understand all of the assumptions about your application and how to have it participate in tracing, regardless of what language/framework/platform you use to build your application.</p>

<h6 id="accessingthedashboard">Accessing the dashboard</h6>

<p>Setup access to the Jaeger dashboard by using port-forwarding:</p>

<pre><code>$ kubectl port-forward -n istio-system $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath='{.items[0].metadata.name}') 16686:16686 &amp;
</code></pre>

<p>Access the Jaeger dashboard by opening your browser to <code>http://localhost:16686</code>.</p>

<p>From the left-hand pane of the Jaeger dashboard, select <code>productpage</code> from the Service drop-down list and click Find Traces. You should see something similar to the following:</p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-43-04-pm.png" alt="Istio on Azure AKS"></p>

<p>If you click on the top (most recent) trace, you should see the details corresponding to your latest refresh of the /productpage. The page should look something like this:</p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-43-16-pm.png" alt="Istio on Azure AKS"></p>

<p>And there you have it. For more information on istio check out <a href="https://istio.io/">https://istio.io/</a></p>

<p>Dont forget to delete your cluster after your finished!</p>

<p>Stay tuned for more posts on <strong>kubernetes</strong>.</p>]]></content:encoded></item><item><title><![CDATA[Test drive heptio sonobuoy diagnostic kubernetes tool]]></title><description><![CDATA[<blockquote>
  <p>Heptio Sonobuoy is a diagnostic tool that makes it easier to understand the state of a Kubernetes cluster by running a set of Kubernetes conformance tests in an accessible and non-destructive manner. </p>
</blockquote>

<p>Heptio have come out with some useful kubernetes tools; sonobuoy being one. </p>

<p>In this post, I will spin</p>]]></description><link>http://blog.shanelee.name/2018/02/03/test-drive-heptio-sonobuoy-diagnostic-kubernetes-tool/</link><guid isPermaLink="false">395f2c06-dbbf-46ae-a335-0c12203079ef</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[docker]]></category><category><![CDATA[aws]]></category><category><![CDATA[heptio]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 03 Feb 2018 09:33:23 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2018/08/joseph-barrientos-93565.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
  <img src="http://blog.shanelee.name/content/images/2018/08/joseph-barrientos-93565.jpg" alt="Test drive heptio sonobuoy diagnostic kubernetes tool"><p>Heptio Sonobuoy is a diagnostic tool that makes it easier to understand the state of a Kubernetes cluster by running a set of Kubernetes conformance tests in an accessible and non-destructive manner. </p>
</blockquote>

<p>Heptio have come out with some useful kubernetes tools; sonobuoy being one. </p>

<p>In this post, I will spin up a kubernetes cluster on AWS using latest version of kops <strong>(1.8)</strong> and test drive sonobuoy scanner tool.</p>

<p>Rather than install kops and kubectl locally, I have published a docker image that you can use as a utility container.</p>

<p><a href="https://hub.docker.com/r/shanelee007/alpine-kops/">Alpine kops</a> docker image includes kops, kubectl, terraform, aws cli and helm. The swiss army knife for kubernetes!! </p>

<h2 id="kubernetesinstallation">Kubernetes installation</h2>

<p>Run the container  </p>

<pre><code class="language-bash">docker run --rm -it \  
  -v "$HOME"/.ssh:/root/.ssh:ro \
  -v "$HOME"/.aws:/root/.aws:ro \
  -v "$HOME"/.kube:/root/.kube:rw \
  -v "$HOME"/.helm:/root/.helm:rw \
  -v "$(pwd)":/workdir \
  -w /workdir \
  shanelee007/alpine-kops
</code></pre>

<p>Then create the cluster on AWS. Here I am creating one master instance and two worker nodes</p>

<pre><code class="language-bash">  kops create cluster --v=0 \
    --cloud=aws \
    --node-count 2 \
    --master-size=m3.medium \
    --master-zones=ap-southeast-2a \
    --zones ap-southeast-2a,ap-southeast-2c \
    --name= ${NAME} \
    --node-size=m3.medium \
    --node-volume-size=20
</code></pre>

<p>Before scanning the cluster, I will deploy a few applications.</p>

<p>Bitnami have come out with kubeapps, to easily deploy apps on your cluster.</p>

<h2 id="bitnamikubeapps">Bitnami KubeApps</h2>

<blockquote>
  <p>Kubeapps is a Kubernetes dashboard that supercharges your Kubernetes cluster with simple browse and click deployment of apps in any format. </p>
</blockquote>

<h3 id="installation">Installation</h3>

<pre><code class="language-bash">sudo curl -L https://github.com/kubeapps/installer/releases/download/v0.2.0/kubeapps-linux-amd64 -o /usr/local/bin/kubeapps &amp;&amp; sudo chmod +x /usr/local/bin/kubeapps  
</code></pre>

<p>To see what it installs, run dry run first</p>

<pre><code class="language-bash">  kubeapps up --dry-run -o yaml
</code></pre>

<p>Once your happy, lets kick it off</p>

<pre><code class="language-bash"> kubeapps up
</code></pre>

<h3 id="dashboard">Dashboard</h3>

<p>Once Kubeapps is installed, securely access the Kubeapps Dashboard from your system by running:</p>

<pre><code class="language-bash">kubeapps dashboard  
</code></pre>

<p>This will start an HTTP proxy for secure access to the Kubeapps Dashboard and launch your default browser to access it. </p>

<h3 id="deploywordpress">Deploy wordpress</h3>

<p>Using the "Charts" menu from the Dashboard welcome page I will select wordpress application from the list of charts in the official Kubernetes chart repository.</p>

<h2 id="sonobuoy">Sonobuoy</h2>

<p>Now lets install sonobuoy on the cluster and run the diagnostics.</p>

<p>You will find the steps <a href="https://scanner.heptio.com">here</a></p>

<p>Once you run the command, you will see in the browser to see the conformance results.</p>

<p><img src="http://blog.shanelee.name/content/images/2018/02/Screen-Shot-2018-02-03-at-9-35-17-pm.png" alt="Test drive heptio sonobuoy diagnostic kubernetes tool"></p>

<p>You will see two pods running in the namespace heptio-sonobuoy</p>

<pre><code class="language-bash">NAME                                READY     STATUS    RESTARTS   AGE  
sonobuoy                            3/3       Running   0          5m  
sonobuoy-e2e-job-bf586487f2f64f0b   2/2       Running   0          4m  
</code></pre>

<p>It may take up to 60 mins to run the tests. So sit back and relax... üòâ</p>

<p>To see what is happening you can use kubetail to tail the logs</p>

<pre><code class="language-bash">kubetail sonobuoy -n heptio-sonobuoy  
kubetail sonobuoy-e2e-job-bf586487f2f64f0b -n heptio-sonobuoy
</code></pre>

<h3 id="sonobuoyresults">Sonobuoy results</h3>

<p>Once it finishes, you can download the results and keep the report by exporting as a pdf.</p>

<p><img src="http://blog.shanelee.name/content/images/2018/02/Screen-Shot-2018-02-03-at-10-19-17-pm.png" alt="Test drive heptio sonobuoy diagnostic kubernetes tool"></p>

<p>All tests passed. There you have it!</p>

<p>Stay tuned for more kubernetes goodness.. ‚öìÔ∏è</p>]]></content:encoded></item><item><title><![CDATA[Logging on kubernetes with fluentd and elasticsearch 6]]></title><description><![CDATA[How to configure EFK (Elastic-Fluentd-Kibana) stack on kubernetes for logging]]></description><link>http://blog.shanelee.name/2017/12/17/logging-on-kubernetes-with-fluentd-and-elasticsearch-6/</link><guid isPermaLink="false">cac30a75-368d-462f-aee8-9c510bfd9de1</guid><category><![CDATA[elasticsearch]]></category><category><![CDATA[kubernetes]]></category><category><![CDATA[docker]]></category><category><![CDATA[ingress]]></category><category><![CDATA[nginx]]></category><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[curator]]></category><category><![CDATA[fluentd]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 16 Dec 2017 13:06:59 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2018/01/robert-larsson-64547-1.jpg" medium="image"/><content:encoded><![CDATA[<h1 id="tldr">TLDR</h1>

<img src="http://blog.shanelee.name/content/images/2018/01/robert-larsson-64547-1.jpg" alt="Logging on kubernetes with fluentd and elasticsearch 6"><p>The solution I have used in the past for logging in kubernetes clusters is EFK (Elastic-Fluentd-Kibana). Alot of you have probably heard of ELK stack but I find that logstash is more heavyweight and does not provide the same output plugins as fluentd.</p>

<p><img alt="Logging on kubernetes with fluentd and elasticsearch 6" src="http://blog.shanelee.name/content/images/2017/12/icon-elasticsearch-bb-1.svg" style="width: 300px; height:300px"></p>

<h2 id="elasticsearch">Elasticsearch</h2>

<p>Elasticsearch 6 has just been <a href="https://www.elastic.co/blog/elasticsearch-6-0-0-released">announced</a> with some major performance improvements.</p>

<p>You have a few options to deploy elasticsearch - elastic cloud or spin up your own ES cluster in kubernetes. But I have decided to go with the AWS managed service. It now has <a href="https://aws.amazon.com/blogs/aws/amazon-elasticsearch-service-now-supports-vpc/">VPC support</a> and the new release 6.0. </p>

<p>The reason being that administrating Elasticsearch can be a lot of work, as many people experienced with the system will tell you it can be tricky to keep running smoothly and that it‚Äôs a task better outsourced to an external service.</p>

<p>I recently read dubsmash story <a href="https://stackshare.io/dubsmash/dubsmash-scaling-to-200-million-users-with-3-engineers">https://stackshare.io/dubsmash/dubsmash-scaling-to-200-million-users-with-3-engineers</a> and how they focus on the product deliverables and not infrastructure. If you have a small team or not in the business of managing services such as ES, i find its best to outsource..</p>

<p>Following the 12 factor principles for logging <a href="https://12factor.net/logs">https://12factor.net/logs</a> all applications that are containerised (using docker) should log to STDOUT.</p>

<blockquote>
  <p>A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout.</p>
</blockquote>

<p>Whether your using winston for nodeJS or console appender for spring boot, this is the recommended way. </p>

<p>Every pod in a <strong>K8S</strong> cluster has its standard output and standard error captured and stored in the /var/log/containers/ node directory. </p>

<p>Now you need a logging agent ( or logging shipper) to ingest these logs and output to a target. </p>

<h2 id="fluentd">FluentD</h2>

<p><img src="http://blog.shanelee.name/content/images/2017/12/container_logging-1024x536.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<p>Fluentd is an open-source framework for data collection that unifies the collection and consumption of data in a pluggable manner. There are several producer and consumer loggers for various kinds of applications. It has a huge suite of <a href="https://www.fluentd.org/plugins">plugins</a> to choose from. </p>

<p>Fluentd will be deployed as a <strong>daemonset</strong> on the kubernetes cluster.</p>

<p>Kubernetes logs the content of the stdout and stderr streams of a pod to a file. It creates one file for each container in a pod. The default location for these files is /var/log/containers . The filename contains the pod name, the namespace of the pod, the container name, and the container id. The file contains one JSON object per line of the two streams stdout and stderr. </p>

<p>A DaemonSet ensures that a certain pod is scheduled to each kubelet exactly once. The fluentd pod mounts the <strong>/var/lib/containers/</strong> host volume to access the logs of all pods scheduled to that kubelet.</p>

<h2 id="whatsmissinggeo">Whats missing - Geo</h2>

<p><img src="http://blog.shanelee.name/content/images/2017/12/joao-silas-72562.jpg" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<p>I found with the examples online for fluentd daesmonset there was none that supported <strong>geoip</strong>. Coreos offering and fluentd official kubernetes daemonset do not provide this feature. </p>

<p>Geoip is a very useful tool when inspecting access logs through kibana. I use nginx  ingress controller in kubernetes and I wanted to see where incoming requests arose geographically. </p>

<p>Knowing from where in the world people are accessing your website is important not only for troubleshooting and operational intelligence but also for other use cases such as business intelligence</p>

<h3 id="ingressandnginx">Ingress and Nginx</h3>

<p>I use nginx as a reverse proxy behind AWS ELB to manage my routing. By default, nginx does not output to json. And instead of figuring out the fluentd nginx parser I decided to configure nginx to enable json logging. </p>

<p>Sample conf can be seen below:</p>

<pre><code class="language-yaml">kind: ConfigMap  
apiVersion: v1  
metadata:  
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app: ingress-nginx
data:  
  use-proxy-protocol: "false"
  log-format-escape-json: "true"
  log-format-upstream: '{"proxy_protocol_addr": "$proxy_protocol_addr","remote_addr": "$remote_addr", "proxy_add_x_forwarded_for": "$proxy_add_x_forwarded_for",
   "request_id": "$request_id","remote_user": "$remote_user", "time_local": "$time_local", "request" : "$request", "status": "$status", "vhost": "$host","body_bytes_sent": "$body_bytes_sent",
   "http_referer":  "$http_referer", "http_user_agent": "$http_user_agent", "request_length" : "$request_length", "request_time" : "$request_time",
    "proxy_upstream_name": "$proxy_upstream_name", "upstream_addr": "$upstream_addr",  "upstream_response_length": "$upstream_response_length",
    "upstream_response_time": "$upstream_response_time", "upstream_status": "$upstream_status"}'
</code></pre>

<h3 id="fluentddockerimage">Fluentd docker image</h3>

<p>I then extended the fluentd debian elasticsearch docker image to install the geo-ip plugin and also update the max mind database.</p>

<p>Docker Image can be found <a href="https://hub.docker.com/r/shanelee007/fluentd-kubernetes/">here</a> on docker hub. Tag is <strong>v0.12-debian-elasticsearch-geo</strong></p>

<pre><code class="language-bash">docker pull shanelee007/fluentd-kubernetes:v0.12-debian-elasticsearch-geo  
</code></pre>

<p>Now I needed to amend the fluentd config to filter my nginx access logs and translate ip address to geo co-ordinates. </p>

<p>Sample config in the yaml file for daemonset with updated database is</p>

<pre><code class="language-yaml">  geoip-filter.conf: |
    &lt;filter kube.ingress-nginx.nginx-ingress-controller&gt;
        type geoip

        # Specify one or more geoip lookup field which has ip address (default: host)
        # in the case of accessing nested value, delimit keys by dot like 'host.ip'.
        geoip_lookup_key  remote_addr

        # Specify optional geoip database (using bundled GeoLiteCity databse by default)
        geoip_database    "/home/fluent/GeoLiteCity.dat"

        # Set adding field with placeholder (more than one settings are required.)
        &lt;record&gt;
          city            ${city["remote_addr"]}
          lat             ${latitude["remote_addr"]}
          lon             ${longitude["remote_addr"]}
          country_code3   ${country_code3["remote_addr"]}
          country         ${country_code["remote_addr"]}
          country_name    ${country_name["remote_addr"]}
          dma             ${dma_code["remote_addr"]}
          area            ${area_code["remote_addr"]}
          region          ${region["remote_addr"]}
          geoip           '{"location":[${longitude["remote_addr"]},${latitude["remote_addr"]}]}'
        &lt;/record&gt;

        # To avoid get stacktrace error with `[null, null]` array for elasticsearch.
        skip_adding_null_record  true

        # Set log_level for fluentd-v0.10.43 or earlier (default: warn)
        log_level         info

        # Set buffering time (default: 0s)
        flush_interval    1s
    &lt;/filter&gt;
</code></pre>

<p>I also updated the elasticsearch template to version 6 as there was issues with version 5.</p>

<p>Now to the fun part... testing it out!</p>

<h2 id="tryityourself">Try it yourself</h2>

<p>This tutorial can be executed in less than 15 minutes, as log as you already have:</p>

<ul>
<li><p>Kubernetes cluster up</p></li>
<li><p>Nginx ingress installed</p></li>
</ul>

<h3 id="installing">Installing</h3>

<p>Github project for creating all resources in kubernetes can be found at <a href="https://github.com/shavo007/k8s-ingress-letsencrypt/tree/master">https://github.com/shavo007/k8s-ingress-letsencrypt/tree/master</a></p>

<p>To create the namespace and manifests for logging, the only change you need is to update the elasticsearch endpoint in the configmap.</p>

<p>File is located at <a href="https://github.com/shavo007/k8s-ingress-letsencrypt/blob/master/resources/logging/fluentd-configmap.yaml#L291">https://github.com/shavo007/k8s-ingress-letsencrypt/blob/master/resources/logging/fluentd-configmap.yaml#L291</a></p>

<p>Then you can create the resources. </p>

<h4 id="kubectlaliases">Kubectl aliases</h4>

<p>For the commands below I am using bash aliases. Aliases save me alot of time when I am querying a cluster. You can find the github project <a href="https://github.com/ahmetb/kubectl-aliases">here</a></p>

<pre><code class="language-bash">ka resources/logging --record  
</code></pre>

<p>Verify the pods are created</p>

<pre><code class="language-bash">kgpon logging  
</code></pre>

<p>If you have the dashboard installed you can inspect the logs there or you can tail the logs from the command line using <a href="https://github.com/johanhaleby/kubetail">kubetail</a></p>

<pre><code class="language-bash">kubetail fluentd -n logging  
</code></pre>

<h2 id="kibana">Kibana</h2>

<p><img alt="Logging on kubernetes with fluentd and elasticsearch 6" src="http://blog.shanelee.name/content/images/2017/12/icon-kibana-bb.svg" style="width: 300px; height:300px"></p>

<p>Now access kibana - GUI for elasticsearch</p>

<p>You should now see your logs ingested in elasticsearch. </p>

<p><img src="http://blog.shanelee.name/content/images/2017/12/Screen-Shot-2017-12-26-at-3-10-18-pm.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<h3 id="tilemapvisualization">Tile map visualization</h3>

<p>There is a limitation with managed service and tile map view. I customized the settings to use WMS compliant map server. See below:</p>

<p><img src="http://blog.shanelee.name/content/images/2017/12/Screen-Shot-2017-12-26-at-3-14-43-pm.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<p>Once you pick the pattern, and assuming your geopoints are correctly mapped, Kibana will automatically populate the visualisation settings such as which field to aggregate on, and display the map almost instantly.</p>

<p>I have defined a visualisation for heatmap. This helps in visualizing geospatial data. You can go to management and import the json file from here  <a href="https://github.com/shavo007/k8s-ingress-letsencrypt/blob/master/resources/assets/export.json">https://github.com/shavo007/k8s-ingress-letsencrypt/blob/master/resources/assets/export.json</a></p>

<p>This includes multiple visualizations and a dashboard.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/12/Screen-Shot-2017-12-17-at-11-47-26-am.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<h4 id="inaccurategeolocation">Inaccurate Geolocation</h4>

<p>You may find the IP is matched to an inaccurate location. Be aware that the free Maxmind database that is used is ‚Äúcomparable to, but less accurate than, MaxMind‚Äôs GeoIP2 databases‚Äù, and, ‚ÄúIP geolocation is inherently imprecise. Locations are often near the center of the population.‚Äù See the MaxMind site for further details.</p>

<h4 id="widgets">Widgets</h4>

<p>You can build up more widgets, such as url count or count by country.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/12/Screen-Shot-2017-12-26-at-3-43-45-pm.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<h2 id="cleanupoflogindices">Cleanup of log indices</h2>

<p>For cleanup of old log indices there is a tool called <a href="https://github.com/elastic/curator">curator</a>. I found a serverless option <a href="https://github.com/cloudreach/aws-lambda-es-cleanup">https://github.com/cloudreach/aws-lambda-es-cleanup</a> and created the lambda function via <strong>terraform</strong>.</p>

<p>You can easily schedule the lambda function to cleanup log indices greater than x no. of days.</p>

<p>And thats it! Stay tuned for more posts on <strong>kubernetes</strong>. </p>]]></content:encoded></item><item><title><![CDATA[Kubernetes ingress and sticky sessions]]></title><description><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Recently I had to look at horizontally scaling a traditional web-app on kubernetes. Here i will explain how I achieved it and what ingress controller is and why to use it.</p>

<p>I  assume you know what pods are so I will quickly breakdown service and ingress resources.</p>

<h2 id="service">Service</h2>]]></description><link>http://blog.shanelee.name/2017/10/16/kubernetes-ingress-and-sticky-sessions/</link><guid isPermaLink="false">52c2d7b6-231c-4d3b-8fcd-7f4fa090d576</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[docker]]></category><category><![CDATA[ingress]]></category><category><![CDATA[sticky]]></category><category><![CDATA[elb]]></category><category><![CDATA[nginx]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 15 Oct 2017 14:21:48 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Recently I had to look at horizontally scaling a traditional web-app on kubernetes. Here i will explain how I achieved it and what ingress controller is and why to use it.</p>

<p>I  assume you know what pods are so I will quickly breakdown service and ingress resources.</p>

<h2 id="service">Service</h2>

<blockquote>
  <p>Service is a logical abstraction communication layer to pods. During normal operations pods get‚Äôs created, destroyed, scaled out, etc.</p>
</blockquote>

<p>A Service make‚Äôs it easy to always connect to the pods by connecting to their service which stays stable during the pod life cycle. A important thing about services are what their type is, it determines how the service expose itself to the cluster or the internet. Some of the service types are :</p>

<ul>
<li><p>ClusterIP Your service is only expose internally to the cluster on the internal cluster IP. A example would be to deploy Hasicorp‚Äôs vault and expose it only internally.</p></li>
<li><p>NodePort Expose the service on the EC2 Instance on the specified port. This will be exposed to the internet. Off course it this all depends on your AWS Security group / VPN rules.</p></li>
<li><p>LoadBalancer Supported on Amazon and Google cloud, this creates the cloud providers your using load balancer. So on Amazon it creates a ELB that points to your service on your cluster.</p></li>
<li><p>ExternalName Create a CNAME dns record to a external domain.</p></li>
</ul>

<p>For more information about Services look at <a href="https://kubernetes.io/docs/concepts/services-networking/service/">https://kubernetes.io/docs/concepts/services-networking/service/</a></p>

<h2 id="ingress">Ingress</h2>

<blockquote>
  <p>An Ingress is a collection of rules that allow inbound connections to reach the cluster services</p>
</blockquote>

<p>You define a number of <strong>rules</strong> to access a <strong>service</strong></p>

<h2 id="scenario">Scenario</h2>

<p>Imagine this scenario, you have a cluster running, on Amazon, you have multiple applications deployed to it, some are jvm microservices (spring boot) running inside embedded tomcat, and to add to the mix, you have a couple of SPA sitting in a Apache web server that serves static content. </p>

<p>All applications needs to have TLS, some of the api‚Äôs endpoints have changed, but you still have to serve the old endpoint path, so you need to do some sort of path rewrite. How do you expose everything to the internet? The obvious answer is create a type <strong>LoadBalancer</strong> service for each, but, then multiple ELB‚Äôs will be created, you have to deal with TLS termination at each ELB, you have to CNAME your applications/api‚Äôs domain names to the right ELB‚Äôs, and in general just have very little control over the ELB.</p>

<p>Enter Ingress Controllers. üëç</p>

<h3 id="whatisaningresscontroller">What is an ingress controller?</h3>

<blockquote>
  <p>An Ingress Controller is a daemon, deployed as a Kubernetes Pod, that watches the apiserver's /ingresses endpoint for updates to the Ingress resource. Its job is to satisfy requests for Ingresses.</p>
</blockquote>

<p>You deploy a ingress controller, create a type LoadBalancer service for it, and it sits and monitors Kubernetes api server‚Äôs <strong>/ingresses</strong> endpoint and acts as a reverse proxy for the ingress rules it found there. </p>

<p>You then deploy your application and expose it‚Äôs service as a type NodePort, and create ingress rules for it. The ingress controller then picks up the new deployed service and proxy traffic to it from outside.</p>

<p>Following this setup, you only have one ELB then on Amazon, and a central place at the ingress controller to manage the traffic coming into your cluster to your applications.</p>

<p>To visualise how this works, check out this little guy! Traefik is one implementation you can use as an ingress.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/10/architecture.png" alt=""></p>

<p>But I have chosen nginx ingress controller instead as it supports sticky sessions and as a reverse proxy is extremely popular solution.</p>

<p>So lets get to the interesting part; <strong>coding</strong>!!!</p>

<h2 id="demo">Demo</h2>

<p>I am going to setup a kubernetes gossip cluster on AWS using kops. Then create nginx ingress controller and reverse proxy to a sample app called echoheader. </p>

<p>To setup a k8s cluster on AWS, follow the guide at <a href="https://github.com/shavo007/k8s-ingress">https://github.com/shavo007/k8s-ingress</a></p>

<p>If you do not want to install kops and the other tools needed, I have built a simple docker image that you can use instead. </p>

<p><a href="https://store.docker.com/community/images/shanelee007/alpine-kops">https://store.docker.com/community/images/shanelee007/alpine-kops</a></p>

<p>This includes:</p>

<ul>
<li>Kops</li>
<li>Kubectl</li>
<li>AWS CLI</li>
<li>Terraform</li>
</ul>

<p>Once you have the cluster what we need to do is setup a default backend service for nginx.</p>

<p>The default backend is the default service that nginx falls backs to if if cannot route a request successfully. The default backend needs to satisfy the following two requirements :</p>

<p>serves a 404 page at / <br>
serves 200 on a /healthz</p>

<p>See more at <a href="https://github.com/kubernetes/ingress-nginx/tree/master/deploy">https://github.com/kubernetes/ingress-nginx/tree/master/deploy</a></p>

<p>Run the mandatory commands and install without RBAC roles.</p>

<p>Then install layer 7 service on AWS <br>
<a href="https://github.com/kubernetes/ingress-nginx/tree/master/deploy#aws">https://github.com/kubernetes/ingress-nginx/tree/master/deploy#aws</a> or install the service defined in my repo</p>

<pre><code class="language-bash">kubectl apply -f ingress-nginx-svc.yaml  
</code></pre>

<p>When you run these commands, it created a deployment with one replica of the nginx-ingress-controller and a service for it of type LoadBalancer which created a ELB for us on AWS. Let‚Äôs confirm that. Get the service :</p>

<pre><code class="language-bash">kubectl get services -n ingress-nginx  -o wide | grep nginx  
</code></pre>

<p>We can now test the default back-end</p>

<pre><code class="language-bash">ELB=$(kubectl get svc ingress-nginx -n ingress-nginx  -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

curl $ELB  
</code></pre>

<p>You should see the following:  </p>

<pre><code class="language-bash">default backend - 404  
</code></pre>

<p>All good so far.. </p>

<p>This means everything is working correctly and the ELB forwarded traffic to our nginx-ingress-controller and the nginx-ingress-controller passed it along to the default-backend-service that we deployed.</p>

<h4 id="deployourapplication">Deploy our application</h4>

<p>Now run</p>

<pre><code class="language-bash">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/images/echoheaders/echo-app.yaml

kubectl apply -f ingress.yaml  
</code></pre>

<p>This will create deployment and service for echo-header app. This app simply returns information about the http request as output.</p>

<p>If you look at the ingress resource, you will see annotations defined. </p>

<p><code>ingress.kubernetes.io/ssl-redirect: "true"</code>  will redirect http to https.</p>

<p>To view all annotations check out <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md">https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md</a></p>

<p>One ingress rule is to route all requests for virtual host foo.bar.com to service  <strong>echoheaders</strong> on path /backend. So lets test it out!</p>

<pre><code class="language-bash">curl $ELB/backend -H 'Host: foo.bar.com'
</code></pre>

<p>You should get 200 response back with request headers and other info.</p>

<h3 id="stickysessions">Sticky sessions</h3>

<p>Now to one of the main features that nginx provides.  nginx-ingress-controller can handle sticky sessions as it bypass the service level and route directly the pods. More info can be found here <br>
<a href="https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/affinity/cookie">https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/affinity/cookie</a></p>

<p><mark>Update (17/10/2017) examples have been removed from repo! To find out more on the annotations related to sticky session go to <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md#miscellaneous</mark>">https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md#miscellaneous==</a></mark></p>

<p>To test it out we need to first scale our app echo-headers: Lets scale echo-headers deployment to three pods</p>

<pre><code class="language-bash"> kubectl scale --replicas=3 deployment/echoheaders
</code></pre>

<p>Now lets create the sticky ingress</p>

<pre><code class="language-bash">apiVersion: extensions/v1beta1  
kind: Ingress  
metadata:  
  name: nginx-test-sticky
  annotations:
    kubernetes.io/ingress.class: "nginx"
    ingress.kubernetes.io/affinity: "cookie"
    ingress.kubernetes.io/session-cookie-name: "route"
    ingress.kubernetes.io/session-cookie-hash: "sha1"

spec:  
  rules:
  - host: stickyingress.example.com
    http:
      paths:
      - backend:
          serviceName: echoheaders
          servicePort: 80
        path: /foo
</code></pre>

<p>What this setting does it, instruct nginx to use the nginx-sticky-module-ng module (<a href="https://bitbucket.org/nginx-goodies/nginx-sticky-module-ng">https://bitbucket.org/nginx-goodies/nginx-sticky-module-ng</a>) that‚Äôs bundled with the controller to handle all sticky sessions for us.</p>

<pre><code class="language-bash">kubectl apply -f sticky-ingress.yaml  
</code></pre>

<p>There is a very useful tool called kubetail that you can use to tail the logs of a pod and verify the  sticky session behaviour. To install kubetail check out <a href="https://github.com/johanhaleby/kubetail">https://github.com/johanhaleby/kubetail</a></p>

<p>Now in one terminal window, we can tail the logs </p>

<pre><code class="language-bash">kubetail -l app=echoheaders  
</code></pre>

<p>and in another send in multiple requests to the virtual host stickyingress.example.com</p>

<pre><code class="language-bash">curl -D cookies.txt $ELB/foo -H 'Host: stickyingress.example.com'


while true; do sleep 1;curl -b cookies.txt $ELB/foo -H 'Host: stickyingress.example.com';done
</code></pre>

<p>When the backend server is removed, the requests are then re-routed to another upstream server and NGINX creates a new cookie, as the previous hash became invalid.</p>

<p>As, you can see, requests are sent to the same pod for every subsequent request.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/10/stickySession.gif" alt=""></p>

<h3 id="proxyprotocol">Proxy protocol</h3>

<p>Lots of times you need to pass a user‚Äôs IP address / hostname through to your application. A example would be, to have the hostname of the user in your application logs.</p>

<p>To enable passing along the hostname, enable the below annotation</p>

<pre><code class="language-bash"> service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'
</code></pre>

<p><strong>Update 1(7/10/2017) It looks like this is not needed anymore</strong></p>

<p>For more see <a href="https://github.com/kubernetes/ingress-nginx#source-ip-address">https://github.com/kubernetes/ingress-nginx#source-ip-address</a></p>

<p>To conclude, i have showcased above a subset of features for ingress. Others include path-rewrite, TLS termination, path routing, scaling, rbac, auth and prometheus metrics. For more info check out resources below.</p>

<h2 id="resources">Resources</h2>

<p>For more information visit:</p>

<p>Github project : <a href="https://github.com/shavo007/k8s-ingress">https://github.com/shavo007/k8s-ingress</a></p>

<p>Kubernetes nginx ingress: <a href="https://github.com/kubernetes/ingress-nginx">https://github.com/kubernetes/ingress-nginx</a></p>

<p>External DNS: <a href="https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/nginx-ingress.md">https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/nginx-ingress.md</a></p>

<p>Kubernetes faqs: <br>
<a href="https://github.com/hubt/kubernetes-faq/blob/master/README.md#kubernetes-faq">https://github.com/hubt/kubernetes-faq/blob/master/README.md#kubernetes-faq</a></p>

<p>Alpine-kops: <a href="https://store.docker.com/community/images/shanelee007/alpine-kops">https://store.docker.com/community/images/shanelee007/alpine-kops</a></p>]]></content:encoded></item><item><title><![CDATA[How to save costs with serverless and AWS Lambda]]></title><description><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Recently AWS <a href="https://aws.amazon.com/about-aws/whats-new/2017/06/amazon-rds-supports-stopping-and-starting-of-database-instances/">announced</a> new feature to stop/start your RDS instances. This is something I have been looking forward to for a long while..</p>

<h2 id="awsrdsstopstart">AWS RDS stop/start</h2>

<p><img src="http://blog.shanelee.name/content/images/2017/07/rds.png" alt="">
Companies I have worked for in the past have scaled down their RDS instances for lower end envs to save</p>]]></description><link>http://blog.shanelee.name/2017/07/28/how-to-save-costs-with-serverless-and-aws-lambda/</link><guid isPermaLink="false">cd5587b1-d0bc-45cb-bda4-5436ed764cd8</guid><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[rds]]></category><category><![CDATA[serverless]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Fri, 28 Jul 2017 03:11:00 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Recently AWS <a href="https://aws.amazon.com/about-aws/whats-new/2017/06/amazon-rds-supports-stopping-and-starting-of-database-instances/">announced</a> new feature to stop/start your RDS instances. This is something I have been looking forward to for a long while..</p>

<h2 id="awsrdsstopstart">AWS RDS stop/start</h2>

<p><img src="http://blog.shanelee.name/content/images/2017/07/rds.png" alt="">
Companies I have worked for in the past have scaled down their RDS instances for lower end envs to save costs. Now you can schedule to stop/start, sweet!</p>

<blockquote>
  <p>The stop/start feature is available for database instances running in a Single-AZ deployment</p>
</blockquote>

<p>This may not suit your needs but if you have multiple RDS instances running in lower end envs in a single A-Z; this potentially could save you $100s if not $1000s per annum..</p>

<h2 id="rdspricingexample">RDS Pricing example</h2>

<p>Lets take a medium sized RDS instance <mark>db.m4.xlarge    (4CPU and 16GB RAM)</mark> in <strong>Sydney</strong> region for <strong>MySQL</strong> engine costs <strong>$0.492</strong> per hour</p>

<p>Now if we use the <a href="https://calculator.s3.amazonaws.com/index.html">calculator</a>, <br>
usage running 24/7, costs on average <strong>$360.15</strong> per month</p>

<p>Now if we only had that running business hours <strong>(9 hrs a day MON-FRI)</strong> , talking on average <strong>$92.99</strong>.</p>

<p>Thats a cost saving of <strong>$267.16</strong>. That is just for <strong>one</strong> <strong>instance</strong> <strong>per</strong> <strong>month</strong>/ OR <strong>$3,205.92 per annum</strong>!!!!! </p>

<p>What if you were working in a large enterprise with 10s or 100s of RDS instances... üòâ</p>

<p><img src="http://blog.shanelee.name/content/images/2017/07/neat.gif" alt=""></p>

<p>So now that we know how much you can save, how to automate the process of stopping/starting instances and run it at low cost.</p>

<p>Well, the answer is serverless!</p>

<h2 id="serverlessandawslambda">Serverless and AWS Lambda</h2>

<p><img src="http://blog.shanelee.name/content/images/2017/07/lambda1.png" alt=""></p>

<p>Over the past few years serverless (or FaaS - Function as a Service) computing has became more and more popular.</p>

<blockquote>
  <p>Run code without thinking about servers.
  Pay for only the compute time you consume.</p>
</blockquote>

<p><mark>In the past years, most of the cloud providers started to offer their own version of serverless: Microsoft launched Azure Functions while Google launched Cloud Functions. IBM released an open-source version of serverless, called OpenWhisk.</mark></p>

<h3 id="callinganewlambdafunctionforthefirsttime">Calling a new Lambda function for the first time</h3>

<p>When you deploy your Lambda function (or update an existing one), a new container will be created for it.</p>

<p>Your code will be moved into the container, and the initialization code will run before the first request arrives to the exposed handler function.</p>

<h4 id="consequentcallstoanexistinglambdafunction">Consequent calls to an existing Lambda function</h4>

<p>For the next calls, Lambda may decide to create new containers to serve your requests. In this case, the same process will happen as described above, with initialization.</p>

<p>However, if you have not changed your Lambda function and only a little time passed since the last call, Lambda may reuse the container. This way it saves the initialization time required to spin up the new container and your code inside it.</p>

<h4 id="benefits">Benefits</h4>

<p>AWS lambda has a number of benefits including:</p>

<ul>
<li>Reduced operational cost</li>
<li>Reduced scaling cost</li>
<li>Easier operational management</li>
<li>Cheap! 1,000,000 free requests per month.
Up to 3.2 million seconds of compute time per month</li>
</ul>

<p>To save all the headaches of building, zipping and deploying your function I will use serverless framework. </p>

<h2 id="serverlessframework">Serverless framework</h2>

<p><img src="http://blog.shanelee.name/content/images/2017/07/serverless.png" alt=""></p>

<blockquote>
  <p>Build auto-scaling, pay-per-execution, event-driven apps on AWS Lambda and more..</p>
</blockquote>

<p>The Serverless framework is an open-source, MIT-licensed solution which helps with creating and managing AWS Lambda functions easier.</p>

<p>Now there is other options such as <a href="https://github.com/awslabs/serverless-application-model">AWS SAM</a> but serverless is quite mature and releases often.</p>

<p>In my previous <a href="https://blog.shanelee.name/2016/12/17/serverless-and-scheduled-lambda-function/">post</a> I talked about serverless and how to install and configure it locally.</p>

<h3 id="codingtime">Coding time..</h3>

<p>Here I am going to show you how to use serverless to schedule two functions for stopping and starting RDS instances.</p>

<p>The src code for both functions exist on github:</p>

<ul>
<li><a href="https://github.com/shavo007/lambda-start-rds">https://github.com/shavo007/lambda-start-rds</a></li>
<li><a href="https://github.com/shavo007/lambda-stop-rds">https://github.com/shavo007/lambda-stop-rds</a></li>
</ul>

<p>Both lambda functions are developed using nodeJS. I decided to use <strong>yarn</strong> rather than npm. </p>

<blockquote>
  <p><a href="https://yarnpkg.com/">Yarn</a> is a Node.js package manager which is much faster than NPM, has offline support, and fetches dependencies more <a href="https://yarnpkg.com/en/docs/yarn-lock">predictably</a>.</p>
</blockquote>

<p>I use some <a href="http://es6-features.org/">ES6</a> syntax, which is a great improvement over the "old" ES5 syntax. There are too many ES6 features to list them here but typical ES6 code uses classes with <code>class, const and let, template strings, and arrow functions ((text) =&gt; { console.log(text) })</code>.</p>

<p>As of now, the current runtime supported by AWS Lambda is <strong>node 6.10</strong></p>

<h4 id="babel">Babel</h4>

<p><a href="https://babeljs.io/">Babel</a> is a compiler that transforms ES6 code into ES5 code. It is very modular and can be used in tons of different environments.</p>

<h4 id="eslint">ESLint</h4>

<p><a href="http://eslint.org/">ESLint</a> is the linter of choice for ES6 code. A linter gives you recommendations about code formatting, which enforces style consistency in your code, and code you share with your team. It's also a great way to learn about JavaScript by making mistakes that ESLint will catch.</p>

<p>Instead of configuring the rules we want for our code ourselves, I use the config created by <a href="https://www.npmjs.com/package/eslint-config-airbnb">Airbnb</a>.</p>

<h4 id="atom">Atom</h4>

<p>You can use any IDE of your choice. I use <a href="https://atom.io/">Atom</a> and it has many plugins such as support for <a href="https://atom.io/packages/linter-eslint">eslint</a></p>

<h2 id="functions">Functions</h2>

<p>Both functions are triggered by a cron job. You can view this config in the respective <code>serverless.yaml</code> file</p>

<p>I create my RDS instances with custom tags. <code>autoStopInstance</code> and <code>autoStartInstance</code>.</p>

<p>The start function runs at <strong>9PM SUN-THU (UTC time)</strong>  and starts any instances with <code>autoStartInstance=true</code></p>

<p>The stop function runs at  <strong>8AM MON-FRI (UTC time)</strong> and stops any instances with <code>autoStopInstance=true</code></p>

<p><mark><strong>NB:</strong> I am running both lambda fns in Sydney</mark></p>

<h2 id="readmore">Read more</h2>

<p>Check out the following extra resources:</p>

<ul>
<li><p><a href="https://martinfowler.com/articles/serverless.html">Serverless Architectures by Martin Fowler</a></p></li>
<li><p><a href="https://serverless.com/blog/">The Serverless blog</a></p></li>
<li><a href="https://github.com/dwyl/learn-aws-lambda">Learn AWS Lambda</a></li>
</ul>

<p>If you have any questions, let me know in the comments below!</p>]]></content:encoded></item><item><title><![CDATA[JVM Microservice with spring boot, docker and kubernetes]]></title><description><![CDATA[How to build and deploy a spring boot application using docker and kubernetes.]]></description><link>http://blog.shanelee.name/2017/07/15/jvm-microservice-with-spring-boot-docker-and-kubernetes/</link><guid isPermaLink="false">357eedf7-58d4-48b6-ab00-a8013d81dce5</guid><category><![CDATA[docker]]></category><category><![CDATA[spring-boot]]></category><category><![CDATA[kubernetes]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 15 Jul 2017 04:10:22 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>That title is a bit of a mouthful...</p>

<p>Over the last two weeks I have been playing with kubernetes. I have extensive experience building microservices and below I will demonstrate how to build a microservice, contain it using docker and deploy on kubernetes.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/07/kubernetes.png" alt="Kubernetes"></p>

<h2 id="creatingamicroserviceprojectusingspringboot">Creating a microservice project using spring boot</h2>

<p>Spring boot allows a developer to build a production-grade stand-alone application, like a typical CRUD application exposing a RESTful API, with minimal configuration, reducing the learning curve required for using the Spring Framework drastically. </p>

<blockquote>
  <p>Spring Boot favors convention over configuration and is designed to get you up and running as quickly as possible.</p>
</blockquote>

<h3 id="createspringbootapplication">Create spring boot application</h3>

<p>To create your spring boot app we will use  <a href="https://start.spring.io/">Spring Initializr</a> web page and generate a Gradle Project with the pre-selected Spring Boot Version. <br>
We define <strong>name.shanelee</strong> as Group (if applicable) and define the artifact name. From here you can choose whatever dependencies you need for your microservice. We use <strong>Web</strong> for supporting tomcat and restful API. <br>
 <strong>Actuator</strong> dependency which implements some production-grade features useful for monitoring and managing our application like health-checks and HTTP requests traces.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/07/Screen-Shot-2017-07-15-at-2-25-28-pm.png" alt="Spring boot"></p>

<p>Spring Initializr has already created everything for us. We just need to have a Java JDK 1.8 or later installed on our machine and the JAVA_HOME environment variable set accordingly.</p>

<pre><code class="language-bash">### Extracting and launching the application
shanelee at shanes-MacBook-Air in ~/java-projects  
$ unzip   ~/Downloads/demo.zip -d microservice
$ cd microservice/demo/
$ ./gradlew bootRun
</code></pre>

<p><strong>The application is up and running and we did not write one line of code!</strong></p>

<p>Spring Boot is opinionated and auto-configures the application with sane default values and beans. It also scans the classpath for known dependencies and initializes them. In our case, we immediately enjoy all the production-grade services offered by <a href="http://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html">Spring Actuator</a>.</p>

<pre><code class="language-bash">~$ curl http://localhost:8080/health
{"status":"UP","diskSpace":{"status":"UP","total":981190307840,"free":744776503296,"threshold":10485760}}
</code></pre>

<p><strong>NB: Actuator endpoints is important when we deploy the container in kubernetes. It needs to know when the microservice is ready to handle network traffic.</strong></p>

<p>For more information see <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</a></p>

<h2 id="packagingaspringbootapplicationasadockercontainer">Packaging a Spring Boot application as a Docker container</h2>

<p>Let's start by creating the Dockerfile in the root directory of our project.</p>

<pre><code class="language-Docker">FROM openjdk:8u131-jdk-alpine  
VOLUME /tmp  
WORKDIR /app  
COPY ./build/libs/demo-0.0.1-SNAPSHOT.jar .  
ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","/app/demo-0.0.1-SNAPSHOT.jar"]  
</code></pre>

<p>The FROM keyword defines the base Docker image of our container. We chose <a href="http://openjdk.java.net/">OpenJDK</a> installed on <a href="https://alpinelinux.org/">Alpine Linux</a> which is a lightweight Linux distribution. To understand why I use alpine as base image check out these <a href="https://diveintodocker.com/blog/the-3-biggest-wins-when-using-alpine-as-a-base-docker-image">benefits</a></p>

<p>The <strong>VOLUME</strong> instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from the native host or other containers. <strong>ENTRYPOINT</strong> defines the command to execute when the container is started. Since Spring Boot produces an executable JAR with embedded Tomcat, the command to execute is simply <code>java -jar microservice.jar</code>. The additional flag <code>java.security.edg=file:/dev/./urandom</code> is used to speed up the application start-up and avoid possible freezes. By default, Java uses <code>/dev/random</code> to seed its SecureRandom class which is known to block if its entropy pool is empty.</p>

<h2 id="logging">Logging</h2>

<blockquote>
  <p>Treat logs as event streams</p>
</blockquote>

<p>This is what is recommended by <strong>12factor</strong> principles. </p>

<p>Microservice should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. During local development, the developer will view this stream in the foreground of their terminal to observe the app‚Äôs behavior.</p>

<p>In staging or production deploys, each process‚Äô stream will be captured by the execution environment and routed to one or more final destinations for viewing and long-term archival. <br>
As I will be using <strong>kubernetes</strong>, I will define a daemonset logging shipper called <strong>fluentd</strong>. </p>

<h3 id="daemonsetandfluentd">Daemonset and Fluentd</h3>

<p><img src="http://blog.shanelee.name/content/images/2017/07/kubernetes-elastic-fluentd.png" alt="FluentD"></p>

<p>A <strong>DaemonSet</strong> ensures that a certain pod is scheduled to each kubelet exactly once. The <strong>fluentd</strong> pod mounts the <code>/var/lib/containers/</code> host volume to access the logs of all pods scheduled to that <strong>kubelet</strong></p>

<p>Daemonset for fluentd can be found <a href="https://github.com/fluent/fluentd-kubernetes-daemonset">here</a></p>

<p><strong>Kubernetes</strong> logs the content of the <code>stdout</code> and <code>stderr</code> streams of a pod to a file. It creates one file for each container in a pod. The default location for these files is <code>/var/log/containers</code> . The filename contains the pod name, the namespace of the pod, the container name, and the container id. The file contains one JSON object per line of the two streams stdout and stderr. </p>

<p>Fluentd is a flexible log data collector. It supports various inputs like log files or syslog and supports many outputs like <strong>elasticsearch</strong> or Hadoop. Fluentd converts each log line to an event. Those events can be processed and enriched in the fluentd pipeline.</p>

<h4 id="considerationsforproductiondeployments">Considerations for Production Deployments</h4>

<p>In a production environment you have to implement a log rotation of the stored log data. Since the above fluentd configuration generally will generate one index per day this is easy. <a href="https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html">Elasticsearch Curator</a> is a tool made for exactly this job. <br>
Curator can run as a container similar to one I defined <a href="https://hub.docker.com/r/shanelee007/docker-es-curator-cron/">here</a> also or a scheduled <a href="https://www.elastic.co/blog/serverless-elasticsearch-curator-on-aws-lambda">lambda</a> function. <br>
Logs to <code>stdout</code> have to be in <strong>JSON</strong> format. </p>

<h2 id="kubernetes">Kubernetes</h2>

<blockquote>
  <p>Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.</p>
</blockquote>

<p>I will discuss how to run kubernetes locally using minikube and how to define resource objects for the microservice above. In a later post I will talk about creating cluster on aws using kops.</p>

<h2 id="howtorunkuberneteslocally">How to run kubernetes locally</h2>

<p>To run kubernetes locally you need <a href="https://kubernetes.io/docs/getting-started-guides/minikube/">minikube</a>. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.</p>

<p>To install locally follow the steps <a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">here</a></p>

<p><mark>Increase the storage size when starting</mark></p>

<pre><code class="language-bash">minikube start --disk-size="10g" --memory="4096"  
#Switch to minikube context
kubectl config use-context minikube  
</code></pre>

<p>After cluster created, open the dashboard. Dashboard is an <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons">addon</a> for kubernetes. </p>

<p><code>minikube dashboard</code></p>

<h2 id="createdeploymentandservicefordemomicroservice">Create deployment and service for demo microservice</h2>

<p>To test locally build and tag your docker image <br>
You can point your docker client to the VM's docker daemon by running</p>

<p><code>eval $(minikube docker-env)</code></p>

<pre><code class="language-Docker">docker build -t demo .  
</code></pre>

<p>You should see output like below  </p>

<pre><code class="language-bash">Sending build context to Docker daemon  15.76MB  
Step 1/5 : FROM openjdk:8u131-jdk-alpine  
8u131-jdk-alpine: Pulling from library/openjdk  
88286f41530e: Pull complete  
009f6e766a1b: Pull complete  
86ed68184682: Pull complete  
Digest: sha256:2b1f15e04904dd44a2667a07e34c628ac4b239f92f413b587538f801a0a57c88  
Status: Downloaded newer image for openjdk:8u131-jdk-alpine  
 ---&gt; 478bf389b75b
Step 2/5 : VOLUME /tmp  
 ---&gt; Running in ff8bd4023ec3
 ---&gt; 61232f70a630
Removing intermediate container ff8bd4023ec3  
Step 3/5 : WORKDIR /app  
 ---&gt; 79ea27f4f4ea
Removing intermediate container 01fac4d0f9a3  
Step 4/5 : COPY ./build/libs/demo-0.0.1-SNAPSHOT.jar .  
 ---&gt; f9aa60a3ac4a
Removing intermediate container d90236650b23  
Step 5/5 : ENTRYPOINT java -Djava.security.egd=file:/dev/./urandom -jar /app/demo-0.0.1-SNAPSHOT.jar  
 ---&gt; Running in 7c8a6c01cef0
 ---&gt; abdcba6bf841
Removing intermediate container 7c8a6c01cef0  
Successfully built abdcba6bf841  
Successfully tagged demo:latest
</code></pre>

<h2 id="addons">Addons</h2>

<p>Minikube has a set of built in addons that can be used enabled, disabled, and opened inside of the local k8s environment. </p>

<p>To enable addon for minkube run <code>minikube addons enable &lt;addon&gt;</code></p>

<p>To verify get the list of addons  </p>

<pre><code class="language-bash">$ minikube addons list
- dashboard: enabled
- default-storageclass: enabled
- kube-dns: enabled
- heapster: disabled
- ingress: enabled
- registry: enabled
- registry-creds: disabled
- addon-manager: enabled
</code></pre>

<p>Below is a sample deployment config. Here I defined the service and deployment resource objects. </p>

<pre><code class="language-yaml">apiVersion: v1  
kind: Service  
metadata:  
  name: demo-microservice
  labels:
    app: demo
spec:  
  ports:
    - port: 8081
  selector:
    app: demo
    tier: microservice
  type: LoadBalancer
---

---
apiVersion: extensions/v1beta1  
kind: Deployment  
metadata:  
  name: demo-microservice
  creationTimestamp: null
  labels:
     app: demo
spec:  
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: demo
        tier: microservice
        env: dev
    spec:
      containers:
        - name: demo
          image: demo
          imagePullPolicy: Never
          ports:
          - containerPort: 8081
          env:
            - name: SERVER_PORT
              value: "8081"
</code></pre>

<p>This is the definition of a Kubernetes <a href="http://kubernetes.io/docs/user-guide/deployments/">Deployment</a> named <strong>demo-microservice</strong>. The replicas element defines the target number of Pods. Kubernetes performs automated binpacking and self-healing of the system to comply with the deployment specifications while achieving optimal utilization of compute resources. A Pod can be composed of multiple containers. In this scenario, I included one container: one for demo microservice image.</p>

<p>If using private docker registry  you need to set an entry under the <strong>imagePullSecrets</strong> which is used to authenticate to the docker Registry.</p>

<p><mark>For a detailed explanation of Kubernetes resources and concepts refer to the <a href="http://kubernetes.io/">official documentation</a>.</mark></p>

<h2 id="democreation">Demo creation</h2>

<p>Now create the service and deployment</p>

<pre><code>$ kubectl apply -f deployment.yml --record
service "demo-microservice" configured  
deployment "demo-microservice" created  
</code></pre>

<p>Kubernetes will create one pod with one container inside.</p>

<p>To verify the pod is running  </p>

<pre><code class="language-bash">kubectl get pods  
#To tail the logs of the microservice container run
kubectl logs -f &lt;pod&gt;  
</code></pre>

<p>Now that the container is running in the pod, we can verify the health of the microservice. <br>
The pod is exposed through a service. <br>
To get information about the service run <br>
<code>$ kubectl describe svc demo-microservice</code></p>

<p>To access locally get the public url  </p>

<pre><code class="language-bash">$ minikube service demo-microservice --url
http://192.168.99.100:31511  
</code></pre>

<h3 id="verifyhealth">Verify health</h3>

<p>Then hit the health endpoint to verify the status of the microservice  </p>

<pre><code class="language-bash">$ curl http://192.168.99.100:31511/health
{"status":"UP","diskSpace":{"status":"UP","total":19163156480,"free":9866866688,"threshold":10485760}}
</code></pre>

<h4 id="configurelivenessandreadinessprobes">Configure Liveness and Readiness Probes</h4>

<p>Now that we are happy with the deployment, we are going to add an additional feature. </p>

<p>The <a href="https://kubernetes.io/docs/admin/kubelet/">kubelet</a> uses liveness probes to know when to restart a Container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a Container in such a state can help to make the application more available despite bugs.</p>

<p>The kubelet uses readiness probes to know when a Container is ready to start accepting traffic. A Pod is considered ready when all of its Containers are ready. One use of this signal is to control which Pods are used as backends for Services. When a Pod is not ready, it is removed from Service load balancers.</p>

<p>The right combination of liveness and readiness probes used with Kubernetes deployments can:</p>

<ul>
<li><p>Enable zero downtime deploys</p></li>
<li><p>Prevent deployment of broken images</p></li>
<li><p>Ensure that failed containers are automatically restarted</p></li>
</ul>

<pre><code class="language-yaml"> livenessProbe:
              httpGet:
                path: /health
                port: 8081
                httpHeaders:
                  - name: X-Custom-Header
                    value: Awesome
              initialDelaySeconds: 30
              periodSeconds: 3
</code></pre>

<p>The <strong>livenessProbe</strong> field specifies that the kubelet should perform a liveness probe every 3 seconds for demo. The initialDelaySeconds field tells the kubelet that it should wait 30 seconds before performing the first probe. </p>

<p>To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8081. If the handler for the server‚Äôs /health path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it.</p>

<p>To try the HTTP liveness check, update the deployment</p>

<pre><code class="language-bash">$ kubectl apply -f kubernetes/deployment.yml --record
</code></pre>

<p>If you describe the pod, you will see the liveness http request <br>
 <code>Liveness: http-get http://:8081/health delay=30s timeout=1s period=3s #success=1 #failure=3</code></p>

<p>The readiness probe has similar configuration:  </p>

<pre><code class="language-yaml ">readinessProbe:  
   httpGet:
     path: /health
     port: 8081
   initialDelaySeconds: 30
   periodSeconds: 10
</code></pre>

<p>Readiness and liveness probes can be used in parallel for the same container. Using both can ensure that traffic does not reach a container that is not ready for it, and that containers are restarted when they fail.</p>

<p>To verify these changes, spring boot actuator has a production ready endpoint called trace.</p>

<blockquote>
  <p>Displays trace information (by default the last 100 HTTP requests).</p>
</blockquote>

<p>If you access this endpoint, you will see the health requests like below <br>
<a href="http://192.168.99.100:31511/trace">http://192.168.99.100:31511/trace</a></p>

<pre><code class="language-bash">{timestamp: 1499927068835,info: {method: "GET",path: "/health",headers: {request: {host: "172.17.0.3:8081",user-agent: "Go-http-client/1.1",x-custom-header: "Awesome",accept-encoding: "gzip",connection: "close"},response: {X-Application-Context: "application:local:8081",Content-Type: "application/vnd.spring-boot.actuator.v1+json;charset=UTF-8",Transfer-Encoding: "chunked",Date: "Thu, 13 Jul 2017 06:24:28 GMT",Connection: "close",status: "200"}},timeTaken: "4"}},
</code></pre>

<p>The actuator <a href="https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html">endpoints</a> provide a wealth of information for your microservice. Make sure you become accustomed to them. </p>

<p>There you have it!</p>

<p>A working example of using spring boot, docker and kubernetes.</p>

<p>Stay tuned for more kubernetes goodness... ;-)</p>

<p>If you want to view the sample code check out github repo <a href="https://github.com/shavo007/spring-boot-k8s">here</a></p>]]></content:encoded></item><item><title><![CDATA[Test drive docker health check]]></title><description><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Healthcheck in docker was introduced in docker compose version 2.1 and up.</p>

<p>In version <a href="https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/">1.12</a>, Docker added the ability to perform health checks directly in the Docker engine ‚Äî without needing external monitoring tools or sidecar containers.</p>

<blockquote>
  <p>Configure a check that‚Äôs run to determine whether or</p></blockquote>]]></description><link>http://blog.shanelee.name/2017/06/12/test-drive-docker-health-check/</link><guid isPermaLink="false">92dff96b-d487-442b-98bb-b77722028f90</guid><category><![CDATA[pact]]></category><category><![CDATA[docker]]></category><category><![CDATA[docker-compose]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Mon, 12 Jun 2017 05:06:53 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Healthcheck in docker was introduced in docker compose version 2.1 and up.</p>

<p>In version <a href="https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/">1.12</a>, Docker added the ability to perform health checks directly in the Docker engine ‚Äî without needing external monitoring tools or sidecar containers.</p>

<blockquote>
  <p>Configure a check that‚Äôs run to determine whether or not containers for this service are ‚Äúhealthy‚Äù</p>
</blockquote>

<p>This is a great addition because a container reporting status as Up 1 hour may return errors. The container may be up but there is no way for the application inside the container to provide a status.</p>

<h2 id="dockercomposeexample">Docker compose example</h2>

<p>I will guide you through an example of using healthcheck in my pact broker demo.</p>

<p>Github repo can be found at <a href="https://github.com/shavo007/pact-demo">https://github.com/shavo007/pact-demo</a></p>

<p>Pact broker has two containers:</p>

<ul>
<li>Postgres</li>
<li>Pact broker</li>
</ul>

<h3 id="healthcheckoptions">Health check options</h3>

<p>The health check related options are:</p>

<ul>
<li><p><strong>test</strong>: must be either a string or a list. If it‚Äôs a list, the first item must be either NONE, CMD or CMD-SHELL. Health check commands should return 0 if healthy and 1 if unhealthy. </p></li>
<li><p><strong>interval</strong>: this controls the initial delay before the first health check runs and then how often the health check command is executed thereafter. The default is 30 seconds.</p></li>
<li><strong>retries</strong>: the health check will retry up to this many times before marking the container as unhealthy. The default is 3 retries.</li>
<li><strong>timeout</strong>: if the health check command takes longer than this to complete, it will be considered a failure. The default timeout is 30 seconds.</li>
</ul>

<p>Below is the docker compose file</p>

<script src="https://gist.github.com/shavo007/754a23247826a346ca79593bef44c172.js"></script>

<blockquote>
  <p>The exit code has to be binary, which means 0 or 1 - any other value is not supported. The code || exit 1 makes sure we only get a binary exit code and nothing more exotic.</p>
</blockquote>

<h3 id="waitingforpostgresqltobehealthy">Waiting for PostgreSQL to be "healthy"</h3>

<p>A particularly common use case is a service that depends on a database, such as PostgreSQL. We can configure docker-compose to wait for the PostgreSQL container to startup and be ready to accept requests before continuing.</p>

<p>The following healthcheck has been configured to periodically check if PostgreSQL reponds to the \l list query.</p>

<p>Now that we have defined the instructions, lets kick it off:</p>

<pre><code class="language-bash">docker-compose up --build  
</code></pre>

<p>docker-compose waits for the PostgreSQL service to be "healthy" before starting pact broker.</p>

<h3 id="arewehealthythen">Are we healthy then?</h3>

<p>Once you start the container, you will be able to see the health status in the <code>docker ps</code> output.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/06/Screen-Shot-2017-06-12-at-3-03-03-pm.png" alt=""></p>

<p>You can see the health check status of the postgres container is healthy. Pact broker container depends on this container and waits until it is healthy.</p>

<h3 id="howdoweinspectit">How do we inspect it?</h3>

<p>Using  </p>

<pre><code class="language-bash"> docker inspect
</code></pre>

<p>we can view the output from the command.</p>

<pre><code class="language-bash">docker inspect --format "{{json .State.Health.Status }}" pactdemo_postgres_1  
</code></pre>

<pre><code class="language-bash">"healthy"
</code></pre>

<p>You can use <a href="https://stedolan.github.io/jq/">jq</a> if you find the docker inspect command verbose.</p>]]></content:encoded></item><item><title><![CDATA[How To Import and Export Databases in MySQL or MariaDB with Docker]]></title><description><![CDATA[<h1 id="introduction">Introduction</h1>

<p>You can use data dumps for backup and restoration purposes, or you can use them to migrate data to a new server or development environment.</p>

<p>Working with database dumps in MySQL and MariaDB is straightforward. This tutorial will cover how to export the database as well as import it</p>]]></description><link>http://blog.shanelee.name/2017/04/09/how-to-import-and-export-databases-in-mysql-or-mariadb-with-docker/</link><guid isPermaLink="false">293badb0-924e-402f-8236-eee1c16210de</guid><category><![CDATA[mariadb]]></category><category><![CDATA[docker]]></category><category><![CDATA[mysql]]></category><category><![CDATA[sql]]></category><category><![CDATA[dump]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 09 Apr 2017 04:09:25 GMT</pubDate><content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>

<p>You can use data dumps for backup and restoration purposes, or you can use them to migrate data to a new server or development environment.</p>

<p>Working with database dumps in MySQL and MariaDB is straightforward. This tutorial will cover how to export the database as well as import it from a dump file in MySQL and MariaDB.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/04/mariadb.png" alt="MariaDB Icon"></p>

<h1 id="prerequisites">Prerequisites</h1>

<p>To import and/or export a MySQL or MariaDB database, you will need:</p>

<ul>
<li>Access to the Linux server running MySQL or MariaDB</li>
<li>The database name and user credentials for it</li>
</ul>

<h1 id="exportingthedatabase">Exporting the Database</h1>

<p>The mysqldump console utility is used to export databases to SQL text files. These files can easily be transferred and moved around. You will need the database name itself as well as the username and password to an account with privileges allowing at least full read only access to the database.</p>

<p>Export your database using the following command.</p>

<pre><code class="language-bash">mysqldump -u username -p database_name &gt; dump.sql  
</code></pre>

<ul>
<li><p><strong>username</strong> is the username you can log in to the database with</p></li>
<li><p><strong>database_name</strong> is the name of the database that will be exported</p></li>
<li><p><strong>dump.sql</strong> is the file in the current directory that the output will be saved to</p></li>
</ul>

<p>The command will produce no visual output, but you can inspect the contents of sql file to check if it's a legitimate SQL dump file by using:</p>

<pre><code class="language-bash">head -n 5 dump.sql  
</code></pre>

<p>The top of the file should look similar to this, mentioning that it's a mariadb dump for a database named database_name.</p>

<p><strong>SQL dump fragment</strong></p>

<pre><code class="language-bash">-- MySQL dump 10.16  Distrib 10.1.20-MariaDB, for debian-linux-gnu (x86_64)
--
-- Host: localhost    Database: localhost
-- ------------------------------------------------------
-- Server version    10.1.20-MariaDB-1~jessie
</code></pre>

<p>If any errors happen during the export process, mysqldump will print them clearly to the screen instead.</p>

<h1 id="importingthedatabaseintodockercontainer">Importing the Database into docker container</h1>

<p>To import an existing dump file into MySQL or MariaDB, you will have to create the new database. This is where the contents of the dump file will be imported.</p>

<p>I have an existing mariadb docker container running already locally.</p>

<pre><code class="language-bash">docker run --name mariadb -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password1 -e MYSQL_DATABASE=db -d mariadb:latest  
</code></pre>

<p>Now verify it is running</p>

<pre><code class="language-bash">docker ps  
</code></pre>

<p>You will see output similar to below:</p>

<pre><code class="language-bash">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                            NAMES  
7cfaabda3346        mariadb:latest      "docker-entrypoint..."   5 days ago          Up 5 days           0.0.0.0:3306-&gt;3306/tcp                           mariadb  
</code></pre>

<p>First, start a bash session inside container.</p>

<pre><code class="language-bash">docker exec -it mariadb bash  
</code></pre>

<p>Second, log in to the database as root or another user with sufficient privileges to create new databases.</p>

<pre><code class="language-bash">mysql -u root -ppassword1  
</code></pre>

<p>This will bring you into the mariadb shell prompt. Next, create a new database called new_database.</p>

<pre><code class="language-sql">CREATE DATABASE new_database;  
</code></pre>

<p>Now exit the MySQL shell by pressing CTRL+D. Exit the docker container also.On the normal command line, you can import the dump file with the following command:</p>

<pre><code class="language-bash">docker exec -i mariadb mysql -uroot -ppassword1 --database=new_database &lt; dump.sql  
</code></pre>

<ul>
<li>username is the username you can log in to the database with</li>
<li>new_database is the name of the freshly created database</li>
<li>dump.sql is the data dump file to be imported, located in the current directory</li>
</ul>

<p>The successfully run command will produce no output. If any errors occur during the process, mysql will print them to the terminal instead. You can check that the database was imported by logging in to the MySQL shell again and inspecting the data.</p>

<h1 id="conclusion">Conclusion</h1>

<p>You now know how to create database dumps from MySQL databases as well as how to import them again. mysqldump has multiple additional settings that may be used to alter how the dumps are created, which you can learn more about from the <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html">official mysqldump documentation page</a>.</p>

<p>To find out more about docker commands check out the docker doc <a href="https://docs.docker.com/engine/reference/commandline/exec/">here</a></p>]]></content:encoded></item><item><title><![CDATA[Serverless and scheduled lambda function]]></title><description><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Continuing on from my previous post, I decided to try out serverless to build my scheduled lambda function.</p>

<blockquote>
  <p>Serverless is an open-source, application framework to easily build serverless architectures on AWS Lambda.</p>
</blockquote>

<p>The Serverless Framework is the world‚Äôs leading development framework for building serverless architectures.</p>

<h2 id="australiapubliccalendarcustomalexaskill">Australia public</h2>]]></description><link>http://blog.shanelee.name/2016/12/17/serverless-and-scheduled-lambda-function/</link><guid isPermaLink="false">337574ed-d9ac-4efb-a0ac-a8a248ef6545</guid><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[serverless]]></category><category><![CDATA[alexa]]></category><category><![CDATA[node]]></category><category><![CDATA[australia]]></category><category><![CDATA[calendar]]></category><category><![CDATA[dot]]></category><category><![CDATA[amazon echo]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 17 Dec 2016 10:19:04 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Continuing on from my previous post, I decided to try out serverless to build my scheduled lambda function.</p>

<blockquote>
  <p>Serverless is an open-source, application framework to easily build serverless architectures on AWS Lambda.</p>
</blockquote>

<p>The Serverless Framework is the world‚Äôs leading development framework for building serverless architectures.</p>

<h2 id="australiapubliccalendarcustomalexaskill">Australia public calendar custom alexa skill</h2>

<p><img src="http://blog.shanelee.name/content/images/2016/12/AustraliaCalendar.png" alt="Amazon Alexa"></p>

<p>You can find the skill on the amazon marketplace <a href="https://www.amazon.com/shane-Australia-Public-Holiday-Calendar/dp/B01N0MY1B6/ref=cm_rdp_product">here</a></p>

<h2 id="quickrecap">Quick recap</h2>

<p>There is the pattern leveraging calendar files where the data is relatively static, and it‚Äôs more important to organize it in such a manner that allows navigation through voice commands. There's an ongoing process that can refresh it over time. Going deeper, let's explore the following utterance.</p>

<blockquote>
  <p>‚ÄúAlexa, ask Australia Calendar to find next public holiday by state Victoria‚Äù</p>
</blockquote>

<p>The dialog with this question will be determining what calendar file to get. In this example, the data is static (there aren't new holidays being created every day), and the interaction will be around navigating a list of calendar events.</p>

<p>In this use case, we can invoke the API ahead of an individual user request, and organize and cache the data in s3 bucket. This improves performance, and simplifies the runtime model. Here‚Äôs a view of how this looks using the websites calendar files and how the data is staged.</p>

<p><img src="http://blog.shanelee.name/content/images/2016/12/AustraliaCalendarFlow--1-.svg" alt=""></p>

<p>An S3 bucket is used to store the data, and is persisted in a ical file object. Given the durability of S3, this ensures that the data is always accessible at runtime for the skill and we don't have to hammer the website again and again for what the types of calendar events are in each state.</p>

<h2 id="scheduledlambdafunction">Scheduled lambda function</h2>

<p>Below I will define the steps needed to build my scheduled function using serverless.</p>

<h3 id="installation">Installation</h3>

<p>Install serverless using npm <br>
<code>
npm install serverless -g <br>
</code></p>

<p>The current version is 1.4.0</p>

<p>Now create a new node.js service via the command below: <br>
<code>
serverless create --template aws-nodejs <br>
</code></p>

<p>Here you have defined the runtime to be nodejs. Now for my function I want it to run every 7 days and to put the ical files onto s3 bucket.</p>

<p>You will see in the serverless.yml I have defined the schedule and resource access.</p>

<pre><code class="language-yaml"># Welcome to Serverless!
#
# This file is the main config file for your service.
# It's very minimal at this point and uses default values.
# You can always add more config options for more control.
# We've included some commented out config examples here.
# Just uncomment any of them to get that config option.
#
# For full config options, check the docs:
#    docs.serverless.com
#
# Happy Coding!

service: aus-calendar-cron # NOTE: update this with your service name

provider:  
  name: aws
  runtime: nodejs4.3

  iamRoleStatements:
    - Effect: Allow
      Action:
        - s3:*
      Resource: "*"


# you can define service wide environment variables here
environment:  
   BUCKET: slee-calendar

functions:  
  cron:
    handler: handler.hello

#    The following are a few example events you can configure
#    NOTE: Please make sure to change your handler code to work with those events
#    Check the event documentation for details
    events:
     - schedule: rate(7 days)


#    Define function environment variables here
    environment:
       BUCKET: slee-calendar
</code></pre>

<p>Once I defined the handler logic, all I had to do then is call:</p>

<p><code>
serverless deploy --verbose <br>
</code></p>

<h3 id="howitworks">How it works</h3>

<ul>
<li>An AWS CloudFormation template is created from your serverless.yml.</li>
<li>If a Stack has not yet been created, then it is created with no resources except for an S3 Bucket, which will store zip files of your Function code.</li>
<li>The code of your Functions is then packaged into zip files.</li>
<li>Zip files of your Functions' code are uploaded to your Code S3 Bucket.</li>
<li>Any IAM Roles, Functions, Events and Resources are added to the AWS CloudFormation template.</li>
<li>The CloudFormation Stack is updated with the new CloudFormation template.</li>
<li>Each deployment publishes a new version for each function in your service.</li>
</ul>

<p>To test your function out you can then run <br>
<code>
serverless invoke --function cron --log <br>
</code></p>

<p>It allows to send event data to the function, read logs and display other important information of the function invocation.</p>

<p>And voil√†. Thats your scheduled lambda function up and running! üòã</p>

<p>Stay tuned for more!!</p>]]></content:encoded></item><item><title><![CDATA[Alexa, ask australia calendar for next public holiday in state Victoria]]></title><description><![CDATA[Alexa skill to find next public holiday by state in Australia. ]]></description><link>http://blog.shanelee.name/2016/12/13/alexa-ask-australia-calendar-for-next-public-holiday-in-state-victoria/</link><guid isPermaLink="false">5e87a980-7a41-41be-a620-f0218a7ae396</guid><category><![CDATA[alexa]]></category><category><![CDATA[node]]></category><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[australia]]></category><category><![CDATA[calendar]]></category><category><![CDATA[dot]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 13 Dec 2016 08:52:13 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Voice interfaces are taking off, but how advanced are they becoming? Are we at a point where they can become an automated agent, allowing us to put down our keypads and have a dialog just as if they were a friend or trusted colleague? I‚Äôve set out to test this using Alexa, and building a custom skill (that  recently got certified on amazon marketplace).</p>

<p><img src="http://blog.shanelee.name/content/images/2016/12/AustraliaCalendar.png" alt="Amazon Alexa"></p>

<p>You can find the skill on the amazon marketplace <a href="https://www.amazon.com/shane-Australia-Public-Holiday-Calendar/dp/B01N0MY1B6/ref=cm_rdp_product">here</a></p>

<h2 id="alexamachinelearning">Alexa Machine Learning</h2>

<p>Amazon has enabled software developers to write custom skills that are published to the Alexa platform. These custom skills act as an application that can be invoked from the voice interface similar to basic features that come with the Alexa. </p>

<p>Here is an example of a phrase that invokes the Australia calendar application.  </p>

<blockquote>
  <p>Alexa, ask Australia Calendar whats the next holiday in state Victoria?</p>
</blockquote>

<p>The Alexa Skills Kit establishes the framework for how the powerful machine learning algorithms that Amazon has developed can be leveraged. <br>
A key concept within the platform is around the learning algorithms, and teaching Alexa what ‚Äúmight‚Äù be the expected outcome of a particular phrase. A simple example to understand this pattern is the following.</p>

<blockquote>
  <p>State {AUSPOST_STATE-SLOT}</p>
</blockquote>

<p>In Alexa terminology, the overall phrase is called an ‚ÄúUtterance‚Äù, and what‚Äôs in the brackets is referred to as a ‚ÄúSlot‚Äù. So when someone states ‚ÄúState Victoria‚Äù or ‚ÄúState Queensland‚Äù, both of these have the same intent, it‚Äôs just that the state is a variable that is defined in the slot. When modeling the application, the developer will establish possible choices that might be in the slot (i.e. Victoria, Tasmania, Queensland etc.) <br>
This structure is outlined by the developer as part of writing a custom skill, and once approved by the Alexa team, the model (including custom slots) is ingested into the platform which then influences the algorithms. This type of teaching is common in machine learning, and is useful for establishing patterns.</p>

<p>Once Alexa deciphers the spoken word, it translates into one of these patterns, then invokes the API provided by the developer, passing over which pattern was uttered, along with any variables from slots.</p>

<p>More on custom slots can be seen <a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/migrating-to-the-improved-built-in-and-custom-slot-types">here</a></p>

<p>When authoring an Alexa Skill (the custom ‚Äúapp‚Äù) the challenge is how to establish all the different ways in which a question or statement could be made, then building custom slots for variables within the utterance. That‚Äôs where the engineering comes in by the developer (me!) to take advantage of the underlying machine learning in the Alexa platform. If the pattern matching isn‚Äôt effective, that‚Äôs a problem with the machine learning and the platform itself.</p>

<h2 id="voiceuserinterfacedesignthoughtfulcodewritingmatters">Voice User Interface Design (Thoughtful code writing matters)</h2>

<p>Once the processing is done on Alexa, an API call is made to a micro-service developed by me using aws lambda (serverless compute service). The quality of the user interaction is very dependent upon how ‚Äúflexible‚Äù the skill is written, and that effort has been put into understanding the difference between a visual/keyboard interface and a voice driven one.</p>

<h2 id="alexaflow">Alexa flow</h2>

<p>Trying to create a good skill requires picking some potential flows that a narrative may go through between a user and Alexa. The current version of the skill splits into one direction.</p>

<h3 id="architectingthesolution">Architecting the solution</h3>

<p>There is the pattern leveraging calendar files where the data is relatively static, and it‚Äôs more important to organize it in such a manner that allows navigation through voice commands. There's an ongoing process that can refresh it over time. Going deeper, let's explore the following utterance.</p>

<blockquote>
  <p>‚ÄúAlexa, ask Australia Calendar to find next public holiday by state Victoria‚Äù</p>
</blockquote>

<p>The dialog with this question will be determining what calendar file to get. In this example, the data is static (there aren't new holidays being created every day), and the interaction will be around navigating a list of calendar events.</p>

<p>In this use case, we can invoke the API ahead of an individual user request, and organize and cache the data in s3 bucket. This improves performance, and simplifies the runtime model. Here‚Äôs a view of how this looks using the websites calendar files and how the data is staged.</p>

<p><img src="http://blog.shanelee.name/content/images/2016/12/AustraliaCalendarFlow--1-.svg" alt=""></p>

<p>An S3 bucket is used to store the data, and is persisted in a ical file object. Given the durability of S3, this ensures that the data is always accessible at runtime for the skill and we don't have to hammer the website again and again for what the types of calendar events are in each state.</p>

<h2 id="buildingtheskill">Building the Skill</h2>

<p>The code is organized into a series of functions that are invoked based on the intent. Here are sample mappings from utterances to intent for this skill.</p>

<pre><code class="language-javascript">GetNextHolidayIntent what is the next holiday for {State}  
GetNextHolidayIntent what is the next holiday for state {State}  
GetNextHolidayIntent what is the next public holiday for {State}  
GetNextHolidayIntent what is the next public holiday for state {State}  
GetNextHolidayIntent find next public holiday in state {State}  
GetNextHolidayIntent find next public holiday by state {State}  
GetNextHolidayIntent find next public holiday by state
</code></pre>

<p>Each of the intents have their own functions, and then gather the data for the response by leveraging cached data in the local memory of the skill, or by calling out to a S3 bucket. Here's a mapping of where the data is retrieved from:</p>

<table>  
  <tr>
    <th>Intent</th>
    <th>Function</th>
    <th>Data</th>
  </tr>
  <tr>
    <td>GetNextHolidayIntent</td>
    <td>GetNextHolidayIntent()</td>
    <td>S3 Bucket</td>
  </tr>
</table>

<h2 id="homecards">Home Cards</h2>

<p>Interactions between a user and an Alexa device can include home cards displayed in the Amazon Alexa App, the companion app available for Fire OS, Android, iOS, and desktop web browsers. These are graphical cards that describe or enhance the voice interaction. A custom skill can include these cards in its responses.</p>

<p>I have created a card for the returned holiday response for future reference. You can see an example below:</p>

<p><img src="http://blog.shanelee.name/content/images/2016/12/Screen-Shot-2016-12-13-at-6-09-21-pm.png" alt=""></p>

<p>Stay tuned for more skills!!</p>]]></content:encoded></item><item><title><![CDATA[Alexa, ask auspost for post offices by postcode]]></title><description><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>I will describe below my custom alexa skill.</p>

<h2 id="amazonalexa">Amazon ALexa</h2>

<p><img src="http://blog.shanelee.name/content/images/2016/11/26452717290_b51f1a1f04_z-1.jpg" alt="Amazon Alexa"></p>

<p>Recently, I have started developing alexa skills.</p>

<p>Alexa, the voice service that powers Echo, provides capabilities, or skills, that enable customers to interact with devices in a more intuitive way using voice. Examples of these skills include the</p>]]></description><link>http://blog.shanelee.name/2016/11/15/alexa-ask-auspost-for-post-offices-by-postcode/</link><guid isPermaLink="false">fcfc1f6e-dfe2-4002-83a4-2de0af5720e5</guid><category><![CDATA[alexa]]></category><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[node]]></category><category><![CDATA[amazon]]></category><category><![CDATA[amazon echo]]></category><category><![CDATA[serverless]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 15 Nov 2016 11:53:25 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>I will describe below my custom alexa skill.</p>

<h2 id="amazonalexa">Amazon ALexa</h2>

<p><img src="http://blog.shanelee.name/content/images/2016/11/26452717290_b51f1a1f04_z-1.jpg" alt="Amazon Alexa"></p>

<p>Recently, I have started developing alexa skills.</p>

<p>Alexa, the voice service that powers Echo, provides capabilities, or skills, that enable customers to interact with devices in a more intuitive way using voice. Examples of these skills include the ability to play music, answer general questions, set an alarm or timer and more.</p>

<p>Amazon says:</p>

<blockquote>
  <p>Natural user interfaces, like those based on speech, represent the next major disruption in computing.</p>
</blockquote>

<p>You can now find all alexa skills on amazon.com. See the <a href="https://www.amazon.com/b?ie=UTF8&amp;node=13727921011">experience</a></p>

<p>To see some examples of skills to use check out this <a href="http://www.wired.co.uk/article/the-best-features-of-amazon-echo">article</a></p>

<p>There is now over 3000 skills in the marketplace and growing <a href="https://techcrunch.com/2016/09/13/amazons-alexa-app-store-hits-3000-skills-up-from-1000-in-june/">fast</a></p>

<h2 id="skill">Skill</h2>

<p><img src="http://blog.shanelee.name/content/images/2016/11/noun_666106_cc-2.png" alt="Auspost post office locations"></p>

<p>I decided to build a skill that allows consumers of an alexa enabled device to find post offices by post code in Australia.</p>

<h2 id="alexaflow">Alexa flow</h2>

<p><img src="http://blog.shanelee.name/content/images/2016/11/alexaFlow.svg" alt=""></p>

<h2 id="interactwithauspostskill">Interact with auspost skill</h2>

<p>Now the fun part... üòõ</p>

<p>Use the service simulator to test it out <a href="https://echosim.io/">here</a> or use an amazon echo or dot.</p>

<blockquote>
  <p>Alexa, open auspost</p>
</blockquote>

<p>Alexa will guide you in interacting with the skill.</p>

<p>Sample utterances are:</p>

<p>Try to find post offices by post code</p>

<blockquote>
  <p>Alexa, ask auspost to find post offices by postcode</p>
</blockquote>

<p>Follow the dialog flow to get post offices in your vicinity and the opening hours for today.</p>

<p>Here is a sneak preview!!</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Lf1UUJm0R4M" frameborder="0" allowfullscreen></iframe>

<h3 id="homecards">Home Cards</h3>

<p>Interactions between a user and an Alexa device can include home cards displayed in the Amazon Alexa App, the companion app available for Fire OS, Android, iOS, and desktop web browsers. These are graphical cards that describe or enhance the voice interaction. A custom skill can include these cards in its responses.</p>

<p>I have created a card for the post offices in the response for future reference. You can see an example below:</p>

<p><img src="http://blog.shanelee.name/content/images/2016/11/Screenshot_20161115-230725-1.png" alt=""></p>

<h3 id="disclaimer">Disclaimer</h3>

<p>This skill is not currently on the marketplace.</p>

<p>Stay tuned for more skills!!</p>]]></content:encoded></item><item><title><![CDATA[Alexa, ask home theater to put on Mr. Robot]]></title><description><![CDATA[Integrate alexa skill with plex media server using serverless technology aws lambda (FAAS)]]></description><link>http://blog.shanelee.name/2016/09/06/alexa-ask-home-theater-to-put-on-mr-robot/</link><guid isPermaLink="false">49266c7f-733b-46eb-b62f-0b73847f2b46</guid><category><![CDATA[alexa]]></category><category><![CDATA[plex]]></category><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[node]]></category><category><![CDATA[dynamodb]]></category><category><![CDATA[amazon]]></category><category><![CDATA[amazon echo]]></category><category><![CDATA[serverless]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 06 Sep 2016 10:45:03 GMT</pubDate><content:encoded><![CDATA[<h1 id="plexforalexa">Plex for Alexa</h1>

<p><img src="http://blog.shanelee.name/content/images/2016/09/maxresdefault.jpg" alt=""></p>

<p>Recently, I have started playing around with Alexa skill services. I currently use plex for my media content and found a project on github that has built an alexa skill already. (<strong>thanks to @overloadut</strong>)</p>

<p><a href="https://overloadut.github.io/alexa-plex/">https://overloadut.github.io/alexa-plex/</a></p>

<p>As a pet project, I decided to get this up and running.</p>

<p>Below I will go into detail on the steps involved. </p>

<h1 id="process">Process</h1>

<p><img src="http://blog.shanelee.name/content/images/2016/09/PlexToAlexa.png" alt="Alexa flow"></p>

<h3 id="prerequisites">Prerequisites</h3>

<ul>
<li>Plex media server running</li>
<li>AWS account</li>
<li>Plex Home theater (PHT) client </li>
<li>Have basic knowledge of aws services, nodeJS and setting IAM roles.</li>
<li><a href="https://support.plex.tv/hc/en-us/articles/200484543-Enabling-Remote-Access-for-a-Server">Enable remote access for Plex</a></li>
</ul>

<h3 id="locally">Locally</h3>

<p>Install node, npm and git</p>

<p>Clone the github repo <a href="https://github.com/OverloadUT/alexa-plex">https://github.com/OverloadUT/alexa-plex</a></p>

<p>Run  </p>

<pre><code class="language-bash">npm install  
</code></pre>

<p>to install dependencies
Create a .env file in project root with similar properties as below:</p>

<pre><code class="language-bash">APP_PRODUCT=Alexa Plex  
APP_VERSION=2.0  
APP_DEVICE=Amazon Echo  
APP_DEVICE_NAME=Alexa  
APP_IDENTIFIER=  
ALEXA_APP_ID=  
AWS_ACCESS_KEY_ID=&lt;your own key&gt;  
AWS_SECRET_ACCESS_KEY=&lt;your own secret key&gt;  
</code></pre>

<p>There is a deploy script at the root of the project. Before you run this and update your lambda function, you need to first create it.</p>

<h3 id="awslambda">AWS Lambda</h3>

<p><img src="http://blog.shanelee.name/content/images/2016/09/AWS_Simple_Icons_Compute_AWSLambda-svg--1-.png" alt=""></p>

<p>Create IAM role that has access to lambda and to dynamodb. </p>

<p>If you have aws cli setup try the sample script below. <br>
<strong><em>NB: Make sure and change the role val.</em></strong></p>

<pre><code class="language-bash">aws lambda create-function --function-name alexa-plex --region us-east-1 --runtime  nodejs4.3 --role arn:aws:iam::xxxxxx:role/lambda_basic_execution --handler  index.handler  
</code></pre>

<p>Verify you can view the lambda function in the aws console. </p>

<p>Change to trigger to be "Alexa skills kit".</p>

<h3 id="dynamodb">DynamoDB</h3>

<p><img src="http://blog.shanelee.name/content/images/2016/09/AWS_Simple_Icons_Database_AmazonDynamoDB-svg.png" alt=""></p>

<p>Create a table named <em>AlexaPlexUsers</em> with a primary string key of <em>userid</em>.</p>

<h3 id="createalexaskill">Create alexa skill</h3>

<p><img src="http://blog.shanelee.name/content/images/2016/09/icon_ask_120x120-_CB295607968_--1-.png" alt=""></p>

<p>Navigate to <a href="https://developer.amazon.com/">Amazon Developer Portal</a> to begin the Alexa Skill process. </p>

<p>Sign In >> Alexa >> Alexa Skills Kit >> Add new skill</p>

<h4 id="skillinformation">Skill Information</h4>

<p>Fill in the Alexa Skill store details. Define the invocation name as home theater.  </p>

<h4 id="interactionmodel">Interaction Model</h4>

<p>This will setup what commands "Plex" can process. The sample utterances enable Alexa to process a variety of phrases for the same interaction.</p>

<p>Copy in the intent schema, sample utterances and custom slot. The files are located under directory ask_configuration. </p>

<h4 id="configuration">Configuration</h4>

<p>You'll want to fill in your Lambda <strong>ARN</strong> here. <br>
Set Account Linking to No.</p>

<h4 id="test">Test</h4>

<p>Now the fun part... üòõ</p>

<p>Use the service simulator to test it out.</p>

<p>You'll need to link Alexa to your Plex account. Open your web browser to <a href="http://plex.tv/link">http://plex.tv/link</a> and ask Alexa:</p>

<blockquote>
  <p>"Alexa, tell home theater to continue setup"</p>
</blockquote>

<p>Alexa will guide you through the steps to link your account. Simply keep saying that same command and it should move to the next step.</p>

<p>Once all is good, try playing a tv show</p>

<blockquote>
  <p>"Alexa, tell home theater to play Mr. Robot"</p>
</blockquote>

<h2 id="troubleshooting">Troubleshooting</h2>

<ul>
<li><p>If you receive no response from lambda function, make sure you increase the timeout greater than ten seconds.</p></li>
<li><p>Any errors in the function, review the cloudwatch logs.</p></li>
<li><p>I had an issue where the default player was Plex Web for Chrome and not my Rasplex.
So I needed to change this entry in the dynamodb table. <br>
You will see there is a player object. You need to change the machine identifier to the client that is running plex home theater. How you find that out, is by logging into plex.tv, launching the webapp and inspecting the network requests. <br>
You can do this in Chrome by launching developer tools. You will see a XHR request similar to  <a href="https://plex.tv/devices.xml">https://plex.tv/devices.xml</a>. If you review the response you will find the client identifier value for the device you need. For me it was rasplex.</p></li>
</ul>

<h2 id="versions">Versions</h2>

<ul>
<li>PMS Version 1.0.3.2461  </li>
<li>Rasplex Version 1.6.2.123-e23a7eef</li>
</ul>

<p><strong>TODO</strong>
Add in video showing how it plays</p>]]></content:encoded></item><item><title><![CDATA[Consumer driven contract testing using PACT]]></title><description><![CDATA[pact framework to implement consumer driven contract testing against internal restful APIs]]></description><link>http://blog.shanelee.name/2016/07/19/consumer-driven-contract-testing-using-pact/</link><guid isPermaLink="false">1b17dbf5-5c11-4bfd-b1b7-18199f9e7fbd</guid><category><![CDATA[docker]]></category><category><![CDATA[docker-compose]]></category><category><![CDATA[pact]]></category><category><![CDATA[contract]]></category><category><![CDATA[testing]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 19 Jul 2016 11:26:24 GMT</pubDate><content:encoded><![CDATA[<h1 id="allaboutpact">All about PACT....</h1>

<p><img src="https://tech.affinitas.de/wp-content/uploads/2015/04/pact_two_parts.png" alt="The PACT Logo"></p>

<p>PACT enables consumer driven contract testing, providing a mock service and DSL for the consumer project, and interaction playback and verification for the service provider project.</p>

<p>The Pact family of testing frameworks (Pact-JVM, Pact Ruby, Pact .NET, Pact Go, Pact.js, Pact Swift etc.) provide support for Consumer Driven <br>
Contract Testing between dependent systems where the integration is based on HTTP (or message queues for some of the implementations).</p>

<p>See <a href="https://github.com/realestate-com-au/pact/wiki#implementations-in-other-languages">implementations</a></p>

<h2 id="dockerise">Dockerise...</h2>

<p><img src="https://www.docker.com/sites/default/files/docker_banner_image_12312.svg" alt="Docker"></p>

<p>To host your pacts, you need a pact broker. <br>
The Pact Broker provides a repository for consumer driven contracts created using the pact gem.</p>

<p>It:</p>

<ul>
<li>solves the problem of how to share pacts between consumer and provider projects</li>
<li>allows you to decouple your service release cycles</li>
<li>provides API documentation that is guaranteed to be up-to date</li>
<li>shows you real examples of how your services interact</li>
<li>allows you to visualise the relationships between your services </li>
</ul>

<p>There is a docker image already for pact broker, so i decided to create an image for the datastore; postgres. </p>

<p>You can find the image <a href="https://hub.docker.com/r/shanelee007/docker-pact-postgres/">here</a></p>

<p>I also defined a docker compose file to <em>orchestrate</em> starting the two containers within the same network. </p>

<p>Docker Compose is an orchestration tool that makes spinning up multi-container applications effortless.</p>

<p>See the file below:</p>

<pre><code class="language-Docker">version: '2'  
services:  
  postgres:
    image: shanelee007/docker-pact-postgres
    environment:
      -  POSTGRES_PASSWORD=ThePostgresPassword
      -  POSTGRES_USER=admin
    ports:
      - "5432:5432"
  pact:
    image: dius/pact_broker
    environment:
      -  PACT_BROKER_DATABASE_NAME=pactbroker
      -  PACT_BROKER_DATABASE_PASSWORD=TheUserPassword
      -  PACT_BROKER_DATABASE_HOST=postgres
      -  PACT_BROKER_DATABASE_USERNAME=pactbrokeruser
    ports:
      - "80:80"
    depends_on:
        - postgres
#    entrypoint: ./wait-for-it.sh postgres:5432 -- echo "postgres is up"
</code></pre>

<p>Docker command to run is  </p>

<pre><code class="language-bash">docker-compose --file docker-compose-pact.yml  up --build
</code></pre>

<p>One simple command and you have a pact broker up and running locally at <a href="http://localhost/ui/relationships">http://localhost/ui/relationships</a> üòé</p>

<h2 id="acloserlookatpact">A closer look at PACT</h2>

<p>I have created a github project that demonstrates Pact end-to-end.</p>

<p>You can find the project <a href="https://github.com/shavo007/pact-demo">here</a></p>

<p>Steps are:</p>

<ul>
<li>Spin up pact broker using docker containers</li>
<li>Start the microservice</li>
<li>Run js consumer test using pact mock service and publish to pact broker</li>
<li>Run jvm consumer and publish to pact broker</li>
</ul>

<p>You should then see the contracts published at <br>
<a href="http://localhost/ui/relationships">http://localhost/ui/relationships</a></p>

<h3 id="pactbrokerrelationships">Pact broker relationships</h3>

<p><img src="http://blog.shanelee.name/content/images/2016/07/Screen-Shot-2016-07-30-at-8-18-09-PM.png" alt="Pact broker"></p>

<p>You can then verify the contracts against the running microservice by running  </p>

<pre><code class="language-bash">./gradlew pactVerify
</code></pre>

<p><img src="http://blog.shanelee.name/content/images/2016/07/Screen-Shot-2016-07-30-at-8-03-16-PM.png" alt="Microservice console"></p>

<h3 id="consoleverification">Console verification</h3>

<p><img src="http://blog.shanelee.name/content/images/2016/07/Screen-Shot-2016-07-30-at-8-20-00-PM.png" alt=""></p>

<p>Reports are outputted in json and markdown form. Which is very useful in your CI build pipeline.  </p>

<h2 id="benefits">Benefits</h2>

<p>Pact is most valuable for designing and testing integrations where you (or your team/organisation/partner organisation) control the development of both the consumer and the provider, and the requirements of the consumer are going to be used to drive the features of the provider. It is a fantastic tool for developing and testing intra-organisation microservices.</p>

<h2 id="usefulresources">Useful resources</h2>

<ul>
<li><p><a href="http://dius.com.au/2014/05/19/simplifying-micro-service-testing-with-pacts/">http://dius.com.au/2014/05/19/simplifying-micro-service-testing-with-pacts/</a></p></li>
<li><p><a href="http://thoughtworks.github.io/pacto/patterns/cdc/">http://thoughtworks.github.io/pacto/patterns/cdc/</a></p></li>
<li><p><a href="http://docs.pact.io/">http://docs.pact.io/</a></p></li>
<li><p>Recently discussed at microservice meetup : <a href="http://www.meetup.com/Melbourne-Microservices/events/231874304/">http://www.meetup.com/Melbourne-Microservices/events/231874304/</a></p></li>
<li><p><a href="http://martinfowler.com/articles/microservice-testing/#testing-contract-introduction">http://martinfowler.com/articles/microservice-testing/#testing-contract-introduction</a></p></li>
</ul>]]></content:encoded></item></channel></rss>
