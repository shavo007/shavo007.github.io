<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Tech Blog (mainly!)]]></title><description><![CDATA[Thoughts, stories and ideas.]]></description><link>http://blog.shanelee.name/</link><generator>Ghost 0.8</generator><lastBuildDate>Sun, 29 Aug 2021 07:25:26 GMT</lastBuildDate><atom:link href="http://blog.shanelee.name/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Mocking a REST API the "API first" approach with Mockoon]]></title><description><![CDATA[How to mock an API with mockoon. Showcase using open API spec and mockoon to mock an API and run with docker.]]></description><link>http://blog.shanelee.name/2021/08/29/mocking-a-rest-api-the-api-first-way-with-mockoon/</link><guid isPermaLink="false">a64b3164-634b-4cd1-8761-2af6a3c1c2c1</guid><category><![CDATA[swagger]]></category><category><![CDATA[docker]]></category><category><![CDATA[openapi]]></category><category><![CDATA[oas]]></category><category><![CDATA[github]]></category><category><![CDATA[api]]></category><category><![CDATA[rest]]></category><category><![CDATA[mock]]></category><category><![CDATA[stub]]></category><category><![CDATA[mockoon]]></category><category><![CDATA[insomnia]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 29 Aug 2021 07:27:00 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2021/08/chris-ensminger-gWo-hfRotrI-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://blog.shanelee.name/content/images/2021/08/chris-ensminger-gWo-hfRotrI-unsplash.jpg" alt="Mocking a REST API the "API first" approach with Mockoon"><p>Recently, I have spent considerable time researching and analyzing the tooling available for "API first". At the core of this eco-system is the OAS (Open API specification) or interface as we normally like to call it. </p>

<h2 id="oasopenapispecification">OAS (Open API Specification)</h2>

<p>OAS (or what was commonly known as swagger spec) is the industry standard for defining REST interfaces. I had previously worked with many others such as RAML and Blueprint, but it's good to see we have a winner! <a href="https://github.com/OAI/OpenAPI-Specification/releases/tag/3.1.0">Latest</a> release of OAS (V3.x) includes webhook support and the latest JSON schema draft. Open API is now under the governance of the Linux Foundation. The OpenAPI Specification was originally based on the Swagger Specification, donated by SmartBear Software.</p>

<h2 id="mocking">Mocking</h2>

<p>The term <strong>"mock"</strong> for a lot of developers will have unit-testing connotations. In unit-testing, a mock is a fake implementation of a class or function, which accepts the same arguments as the real thing. It might return something pretty similar to the expected output, and different test cases might even modify those returns to see how the code under test works.</p>

<p>This is almost exactly the concept here, just at a HTTP level instead. This is done using a "mock server", which will respond to the expected endpoints, error for non-existent endpoints, often even provide realistic validation errors if a client sends it an invalid request.</p>

<p>So today, I am going to talk about mocking REST APIs. Anyone that has worked in cross-functional teams before would be very used to mocking APIs for local development. If for example, your team consisted of front and back-end devs, normally the BE devs would aim to design the interface upfront (API first) and provide a mock API to FE devs to commence development in parallel. </p>

<p>Why? <br>
Both streams of work should occur in parallel, rather than sequentially. Less waterfall!</p>

<p>But there are many other benefits of mocking APIs such as:</p>

<ul>
<li>Showcasing to stakeholders the interactions as part of the API design process</li>
<li>Showcase to external consumers</li>
<li>Use as a sandbox on the dev portal</li>
<li>Integration testing on CI ("shift testing left")</li>
<li>Performance testing on CI ("shift perf left")</li>
</ul>

<h3 id="tools">Tools</h3>

<p>I have used many frameworks and tools in the past to mock APIs. When I developed with typescript or nodeJS, a framework I used heavily was expressJS. Other tools out there include mountebank or wiremock. But now there is a new breed of mock API tools that are <strong>OAS compliant</strong>. Two that I have found recently are prism from stoplight and Mockoon.</p>

<h2 id="mockoon">Mockoon</h2>

<p><img src="http://blog.shanelee.name/content/images/2021/08/Mockoon.svg" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<blockquote>
  <p>Mockoon lets you mock an API in seconds</p>
</blockquote>

<p>Some features include:</p>

<ul>
<li>Intuitive interface to create your mock API and run anywhere via CLI</li>
<li>Integrates with your workflow - Compatible with the OpenAPI specification, Mockoon integrates perfectly with your existing applications and API design workflow.</li>
<li>Advanced features and tackle the most complex situation with HTTP requests recording, proxying, integration testing, etc.</li>
<li>Complex rules system and dynamic body templating</li>
<li>Powerful forwarding and debugging</li>
</ul>

<p><insert img="" here=""></insert></p>

<h2 id="creatingourfirstapiwithmockoon">Creating our first API with Mockoon</h2>

<h3 id="step1installation">Step1. Installation</h3>

<p>Mockoon is available on the three major operating systems: Windows, macOS, and Linux. <br>
You can install the native app <a href="https://mockoon.com/download/">here</a>.</p>

<h3 id="step2createyourfirstmockapi">Step 2. Create your first mock API</h3>

<p>After launching the application for the first time, you will find a demo mock API, also called "environment" in Mockoon. You can keep it and build from here or create a new one. To create a new mock API, open the collapsible environments menu on the left and press the blue "plus" button:</p>

<p><img src="http://blog.shanelee.name/content/images/2021/08/Screen-Shot-2021-08-27-at-11-42-09-am.png" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<h3 id="step3createyourfirstapiroute">Step 3. Create your first API route</h3>

<p>The newly created mock API already includes a route on <code>/</code>. You can modify it by setting up the method and path of your choice.</p>

<p>You can also create a new endpoint by clicking on the blue "plus" button at the top of the endpoint list:</p>

<p><img src="http://blog.shanelee.name/content/images/2021/08/Screen-Shot-2021-08-27-at-11-55-50-am-5.png" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<h3 id="step4apiendpointconfiguration">Step 4. API endpoint configuration</h3>

<p>You can further customize your endpoint by adding a custom header and the following sample body (which makes use of Mockoon's templating system).</p>

<h3 id="step5runandcallmockapi">Step 5. Run and call Mock API</h3>

<p>The last step is to run your mock API. For this, click on the green "play" arrow in the header:</p>

<p>Your mock server is now available on <code>http://localhost:3001</code> (but also on <code>http://127.0.0.1</code> and all your local network adapters).</p>

<p>You can do a test call to the following URL <code>http://localhost:3001/tutorials</code> using your favorite tool (here using <a href="https://insomnia.rest/">Insomnia</a>) and see the returned response:</p>

<p><img src="http://blog.shanelee.name/content/images/2021/08/Screen-Shot-2021-08-27-at-11-59-39-am-3.png" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<pre><code class="language-bash">curl --request POST \  
  --url http://localhost:3001/tutorials
</code></pre>

<p>So now you have got the hang of it, let's look at its more advanced features and its "API first" support.</p>

<h2 id="apifirstapproach">API first approach</h2>

<p>So let's import a sample OAS file with Mockoon.</p>

<h3 id="step1importoas">Step 1 Import OAS</h3>

<p>Open the app and go to <code>Import/Export &gt; Swagger/Open API &gt; Import Swagger v2/Open API v3</code></p>

<p>I have defined a sample OAS file <a href="https://github.com/shavo007/mockoon-demo/blob/main/oas3.yaml">here</a> that you can use to follow along. Once you import, it now should look like this.</p>

<p><img src="http://blog.shanelee.name/content/images/2021/08/Screen-Shot-2021-08-27-at-12-22-57-pm.png" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<p>You can see that for each route, it has already been set up for you all the responses you have defined in the spec and the examples. You can easily toggle on random responses or sequential responses based on the route (<strong>NB: This will disable the rules tho</strong>)</p>

<p><img src="http://blog.shanelee.name/content/images/2021/08/Screen-Shot-2021-08-27-at-12-36-20-pm.png" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<p>It is even smart enough to convert certain fields <code>weatherType</code> and <code>status</code> to its templating language. How cool is that! And again, based on previous steps you can start the server and test it out via Insomnia.</p>

<p><img src="http://blog.shanelee.name/content/images/2021/08/Screen-Shot-2021-08-29-at-4-34-35-pm.png" alt="Mocking a REST API the "API first" approach with Mockoon"></p>

<p>Insomnia is such a powerful app also as it supports the "API first" approach. I can import in the exact same OAS file and test the endpoints. (You will start to see a recurring trend here of "API first" tools üòÖ) </p>

<h3 id="step2runmockanywhere">Step 2 Run mock anywhere</h3>

<p>Ok, so now we are happy with the mock responses, how can I run this anywhere? </p>

<h3 id="introducingmockooncli">Introducing Mockoon CLI üéâüéâüéäü•≥</h3>

<blockquote>
  <p>Mockoon's perfect complement for all your headless and automated environments.</p>
</blockquote>

<p>Mockoon CLI Supports all Mockoon's features, Lightweight and fast, and allows you to Run your mocks everywhere.</p>

<p>NB: Also available as a Docker image, run your mock APIs in Github Actions or on your favorite CI platform!</p>

<p>The CLI is a companion application to Mockoon's main interface designed to receive an exported Mockoon data file.</p>

<p>It has been written in JavaScript/TypeScript and uses some great libraries like <code>oclif</code> and <code>PM2</code>. One of the benefits of using PM2 is that you can easily manage your running mock APIs through the CLI or by using PM2 commands if you are used to them.</p>

<h3 id="step3installmockooncli">Step3. Install mockoon CLI</h3>

<p>I installed using nodeJS</p>

<pre><code class="language-bash">npm install -g @mockoon/cli
</code></pre>

<h3 id="step4exportyourmockapitoajsonfile">Step 4. Export your mock API to a JSON file</h3>

<p>To export your environment, open the <code>"Import/export"</code> application menu and choose <code>"Mockoon's format" -&gt; "Export all environments to a file (JSON)"</code> or <code>"Export current environment to a file (JSON)"</code>.</p>

<p>You can then select a location to save the export data file. Let's name the file <code>Greetings_Mockoon.json</code>.</p>

<h3 id="step5startyourmockapi">Step 5. Start your mock API</h3>

<p>After exporting your data file, you are ready to run your API mock with the CLI.</p>

<p>In your terminal, navigate to the folder where your export data file is and run the following command:</p>

<p><code>mockoon-cli start --data ./Greetings_Mockoon.json</code></p>

<p>If you want to use a remotely hosted file, you can also provide a URL to the --data flag like this:</p>

<p><code>mockoon-cli start --data https://domain.com/data-export.json</code></p>

<h3 id="step6manageyourapimock">Step 6. Manage your API mock</h3>

<p>After running one or more API server mock, you might want to check their health and statuses. To do so you can type <code>mockoon-cli list</code>:</p>

<pre><code class="language-bash">shanelee at shanes-MacBook-Air in ~/projects/mockoon-demo on main [?]  
$ mockoon-cli list
 Name                 Id   Status    Cpu    Memory    Hostname       Port
 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
 mockoon-greeting-api 0    online    0.5    71 MB     0.0.0.0        3002
</code></pre>

<p>You can also stop all running servers at once with <code>mockoon-cli stop all</code></p>

<h3 id="step7viewarunningmockslogs">Step 7. View a running mock's logs</h3>

<p>Mockoon CLI log all events like requests and errors in your user folder in the following files: <code>~/mockoon-cli/logs/{process_name}-out.log</code> and <code>~/mockoon-cli/logs/{process_name}-error.log</code>.</p>

<h3 id="step8deploymockooncliusingdocker">Step 8. Deploy Mockoon CLI using Docker</h3>

<p>Now to the fun part! The CLI can containerize your mock API for you. The docker base image is <code>node:14-alpine</code> which is very lightweight.</p>

<h3 id="usingthedockerizecommand">Using the dockerize command</h3>

<pre><code class="language-bash">mockoon-cli dockerize --data ./Greetings_Mockoon.json --port 3000 --index 0 --output ./Dockerfile
</code></pre>

<p>Now build the image:  </p>

<pre><code class="language-bash">    docker build -t mockoon-greeting-api .
</code></pre>

<p>And then run the container:</p>

<pre><code class="language-bash">    docker run -d -p 3000:3000 mockoon-greeting-api
</code></pre>

<p><a href="https://asciinema.org/a/432855"><img src="https://asciinema.org/a/432855.svg" alt="Mocking a REST API the "API first" approach with Mockoon" title=""></a></p>

<h3 id="step9usemockooncliinacienvironmentgithubactions">Step 9. Use Mockoon CLI in a CI environment: GitHub Actions</h3>

<p>Mockoon CLI being a Javascript application, it can run on any environment where Node.js is installed, including continuous integration systems like GitHub Actions, Buildkite, or CircleCI. It is useful when you want to run a mock server while running integration tests on another application. For example, you could mock the backend when running React front-end application tests. Or when you are running IT tests for a java based application that <code>integrates</code> with Greetings API.</p>

<p>Here is an example of a GitHub Action running a mock API (via docker) before running some tests:</p>

<pre><code class="language-yaml">name: Run mock API server

on: [push]

jobs:  
  run_integ_tests:
    name: Run integ tests
    runs-on: ubuntu-latest
    services:
      greetings:
        image: shanelee007/mockoon-greeting-api:latest
        ports:
          - 3000:3000

    steps:
    - uses: actions/checkout@v2
    - name: Set up JDK 11
      uses: actions/setup-java@v2
      with:
        java-version: '11'
        distribution: 'adopt'
    - name: Cache Maven packages
      uses: actions/cache@v2
      with:
        path: ~/.m2
        key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
        restore-keys: ${{ runner.os }}-m2
    - name: Build with Maven
      run: mvn --batch-mode -ff -V --update-snapshots verify
</code></pre>

<p>So just to summarise with the "API first" approach I can:</p>

<ul>
<li>import an OAS to mockoon</li>
<li>configure the routes (if needed)</li>
<li>set some rules</li>
<li>add dynamic templating to responses (more on this later)</li>
<li>export this file, containerise using the CLI and run it as part of my CI build. </li>
</ul>

<p>All within a matter of seconds!</p>

<h2 id="generatingdynamicdata">Generating dynamic data</h2>

<p>Mocking an API can save you time. By faking the backend responses early, you don't have to worry about whether an endpoint is ready or not. You are up and running in no time and can start implementing your application. However, your mock should still be realistic. And the examples provided in the OAS are often not enough to surface UI layout problems, container overflowed by text, etc.</p>

<p>When mocking using <code>Mockoon</code>, you can easily customize your endpoints to make them look like real ones and even behave realistically, thanks to the dynamic templating system.</p>

<h3 id="generaterandomfakedata">Generate random fake data</h3>

<p>Nowadays, most developers work with JSON. Generating a massive amount of fake JSON data with Mockoon is a breeze thanks to the powerful templating system based on Handlebars syntax.</p>

<p>Mockoon also offers multiple helpers and embarks the <code>Faker.js</code> library, which can generate localized random data as various as cities, addresses, first names, phone numbers, UUID, etc.</p>

<h4 id="completejsonexamplepostslist">Complete JSON example: posts list</h4>

<p>So let's revisit our Greetings API again. The API provides a <code>GET</code> request to return all greetings. So let's override the examples provided from OAS with much richer content.</p>

<p>By using a combination of <code>repeat</code>, <code>image.avatar</code>, <code>lorem.sentences</code>, etc. you can quickly get a massive amount of random data. Combined with the latency option, you can even simulate a slow server and check how your application behaves under stress.</p>

<p>To use the templating system, you only have to use the response body editor and start adding your content. Remember to use the double curly braces to delimit your helpers <code>{{ helperName }}</code> Let's have a look at what such a body could look like:</p>

<pre><code class="language-handlebars">[
  {{#repeat (queryParam 'total' '5')}}
  {
    "id": {{@index}},
    "message": "{{faker 'lorem.sentence' 3 5}}",
    "creationDate": "{{date '2020-11-20' '2020-11-25' "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"}}",
    "label": "key",
    "isFriendly": {{faker 'random.boolean'}},
    "weatherType": "{{oneOf (array '1' '2' '3')}}",
    "status": "{{oneOf (array 'SMILEY_FACE' 'SAD_FACE')}}"
  }
  {{/repeat}}
]
</code></pre>

<p>After a call to Mockoon, this would be the kind of body generated from this template:</p>

<pre><code class="language-json">[
  {
    "id": 0,
    "message": "Earum veritatis est.",
    "creationDate": "2020-11-24T17:32:33.293Z",
    "label": "key",
    "isFriendly": false,
    "weatherType": "1",
    "status": "SMILEY_FACE"
  },
  {
    "id": 1,
    "message": "Rerum ipsa autem.",
    "creationDate": "2020-11-23T10:30:00.526Z",
    "label": "key",
    "isFriendly": true,
    "weatherType": "3",
    "status": "SAD_FACE"
  },
  {
    "id": 2,
    "message": "Porro aut dolores.",
    "creationDate": "2020-11-22T21:03:58.452Z",
    "label": "key",
    "isFriendly": true,
    "weatherType": "1",
    "status": "SMILEY_FACE"
  },
  {
    "id": 3,
    "message": "Qui repudiandae quibusdam.",
    "creationDate": "2020-11-22T09:13:08.923Z",
    "label": "key",
    "isFriendly": true,
    "weatherType": "2",
    "status": "SAD_FACE"
  },
  {
    "id": 4,
    "message": "Qui et voluptatem.",
    "creationDate": "2020-11-22T08:36:19.770Z",
    "label": "key",
    "isFriendly": true,
    "weatherType": "1",
    "status": "SMILEY_FACE"
  }
]
</code></pre>

<p>This example makes extensive usage of what Mockoon and Faker.js have to offer. First, it generates as many "greetings" items as provided in the <code>total</code> query parameter (or default to 5) when calling GET <code>/your/endpoint?total=140</code>. It is especially useful when you want to request a specific number of items depending on the pagination or a "number per pages" user setting. Second, you can see that multiple properties are defined, and random mock data is generated like sentence, date-time, boolean etc.</p>

<p>There are a lot of possibilities and combinations you can try. You can also make your template react to a lot of parameters from the entering request by using Mockoon's helpers. We've already seen <code>queryParam</code> above, but you will find many more in the templating documentation. They allow you to query the request information like <code>body</code>, <code>urlParam</code>, <code>header</code>, <code>method</code>, etc.</p>

<blockquote>
  <p>Mockoon does not limit you to JSON. The templating language based on Handlebars is compatible with any content type. It means that you can generate CSV, HTML, XML, etc. You will find below some examples of what can you can achieve with the templating system.</p>
</blockquote>

<h4 id="generatedynamictemplatingdependingontherequest">Generate dynamic templating depending on the request</h4>

<p>We just saw some interesting use-cases but still quite simple. When working on your application, you may want to go a little bit further by making the template react to the request sent to Mockoon. This is possible by using various helpers that you will find in the templating documentation: <code>body</code>, <code>queryParam</code>, <code>urlParam</code>, <code>cookie</code>, <code>header</code>, <code>hostname</code>, <code>ip</code>, <code>method</code>, etc.</p>

<p>They allow you to access the entering request's information. Combined with other helpers like <code>repeat</code>, <code>switch</code>, or <code>if</code>, you will be able to dynamically generate more complex content.</p>

<p>You will find below some examples:</p>

<h5 id="newgreetingafterapostrequest">New greeting after a POST request</h5>

<p>We will reuse in the response the various parameters present in the request:</p>

<pre><code class="language-json">{
  "id": "{{faker 'random.uuid'}}",
  "message": "{{body 'message'}}",
  "creationDate": "{{date '2020-11-20' '2020-11-25' "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"}}",
  "label": "{{body 'label'}}",
  "isFriendly": {{body 'isFriendly'}},
  "weatherType": {{body 'weatherType'}},
  "status": "{{body 'status'}}"
}
</code></pre>

<p>After a call to this endpoint with the following body:</p>

<pre><code class="language-bash">POST /greetings  
Content-Type: application/json

{
  "message": "Hello Docker",
  "label": "key",
  "isFriendly": true,
  "weatherType": 0,
  "status": "SMILEY_FACE"
}
</code></pre>

<p>We would receive this kind of response content, containing the request information plus some new fields (id and creationDate):</p>

<pre><code class="language-json">{
  "id": "6df3c0c6-bce8-4094-ae29-5cb637fc15a3",
  "message": "Hello Docker",
  "creationDate": "2020-11-24T14:49:47.139Z",
  "label": "key",
  "isFriendly": true,
  "weatherType": 0,
  "status": "SMILEY_FACE"
}
</code></pre>

<p>For more complex cases or to test various error handling scenarios, you could also create multiple responses for the same route, with different bodies, and trigger them by defining some <code>rules</code>. To learn more about using multiple responses combined with rules, you can have a look at the related <a href="https://mockoon.com/docs/latest/route-responses/dynamic-rules/">documentation</a>.</p>

<p>Src files used in this post can be found on <a href="https://github.com/shavo007/mockoon-demo">github</a></p>]]></content:encoded></item><item><title><![CDATA[Jibbing with spring boot and google cloud run]]></title><description><![CDATA[<p>I said jibbing not jiving! üòÜ</p>

<h1 id="cloudnativearchitecture">Cloud native architecture</h1>

<p>When it comes to microservices and cloud native architecture you first think about containers. Now you can of course compose your own docker file. But with jvm based microservices there is a tool from google called jib that can simplify containerization.</p>

<p>We'll</p>]]></description><link>http://blog.shanelee.name/2020/08/29/jibbing-with-spring-boot-and-google-cloud-run/</link><guid isPermaLink="false">9dc3d48c-84cb-487a-b8ca-7d14229f1fe5</guid><category><![CDATA[gcp]]></category><category><![CDATA[cloudrun]]></category><category><![CDATA[spring-boot]]></category><category><![CDATA[spring]]></category><category><![CDATA[docker]]></category><category><![CDATA[microservice]]></category><category><![CDATA[cloud]]></category><category><![CDATA[google]]></category><category><![CDATA[jib]]></category><category><![CDATA[redoc]]></category><category><![CDATA[swagger]]></category><category><![CDATA[openapi]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 29 Aug 2020 08:08:34 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2020/08/ardian-lumi-6Woj_wozqmA-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://blog.shanelee.name/content/images/2020/08/ardian-lumi-6Woj_wozqmA-unsplash.jpg" alt="Jibbing with spring boot and google cloud run"><p>I said jibbing not jiving! üòÜ</p>

<h1 id="cloudnativearchitecture">Cloud native architecture</h1>

<p>When it comes to microservices and cloud native architecture you first think about containers. Now you can of course compose your own docker file. But with jvm based microservices there is a tool from google called jib that can simplify containerization.</p>

<p>We'll take a simple Spring Boot application and build its Docker image using <a href="https://github.com/GoogleContainerTools/jib">Jib</a>. And then we'll also publish to GCR and deploy on google cloud run.</p>

<h2 id="jib">Jib</h2>

<blockquote>
  <p>Jib builds optimized Docker and OCI images for your Java applications without a Docker daemon - and without deep mastery of Docker best-practices. It is available as plugins for Maven and Gradle and as a Java library.</p>
</blockquote>

<p>You don't even need docker installed. Just use maven or gradle plugin and away you go! It uses <a href="https://github.com/GoogleCloudPlatform/distroless">distroless</a> base image under the hood, but as you expect all of this is configurable.</p>

<p>Jib supports multiple container registries,  can change the base image, jvm flags, tags, volumes and much more.</p>

<h2 id="greetingapp">Greeting App</h2>

<p>I use <strong>VSCode</strong> heavily for development so I wanted to see what support they have for java and spring boot. In fact they have many extensions. The extensions I installed are:</p>

<ul>
<li><a href="https://marketplace.visualstudio.com/items?itemName=richardwillis.vscode-gradle-extension-pack">Gradle Extension Pack</a></li>
<li><a href="https://aka.ms/vscode-java-installer-mac">VS Code for java</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=Pivotal.vscode-spring-boot">Spring boot tools</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-spring-initializr">Spring Initializr Java Support</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=vscjava.vscode-spring-boot-dashboard">Spring Boot Dashboard</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=GabrielBB.vscode-lombok">Lombok Annotations Support for VS Code</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=42Crunch.vscode-openapi">OpenAPI (Swagger) </a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=Shan.code-settings-sync">Settings Sync </a> very important to sync if I change laptop in the future...</li>
</ul>

<p>I used Spring Initializr to create a simple project in vs code. <br>
It'll expose a simple <strong>GET</strong> endpoint:</p>

<pre><code class="language-bash">http://localhost:8080/greeting  
</code></pre>

<h3 id="deployment">Deployment</h3>

<p>Now I wanted to deploy this on cloud run and not worry about defining a docker file. So I used the gradle jib plugin and configured the credentials helper to deploy to <a href="https://github.com/GoogleContainerTools/jib/tree/master/jib-gradle-plugin#configuration">GCR</a> (Google cloud registry)</p>

<h4 id="cloudrun">Cloud run</h4>

<p>You can think of cloud run as CAAS (container-as-a-service) or serverless containers. It allows you to  run your stateless HTTP containers without worrying about provisioning machines, clusters or autoscaling. The main difference with app engine flexible is that it can scale to zero - only pay per request.</p>

<p>If you are interested to learn more about cloud run check out the unofficial <a href="https://github.com/ahmetb/cloud-run-faq">faqs</a></p>

<p>There is even a <a href="https://github.com/GoogleCloudPlatform/cloud-run-button">cloud run button</a> if you want to deploy your API publicly.</p>

<p>There is a fantastic VSCode extension called <a href="https://cloud.google.com/code">cloud code</a> that can help you deploy to cloud run or GKE on google cloud. It can even pick up if you are using <a href="https://blog.shanelee.name/2019/02/20/skaffold-for-local-kubernetes-development/">skaffold</a> or jib under the hood.</p>

<p><img src="http://blog.shanelee.name/content/images/2020/08/cloudcode1.png" alt="Jibbing with spring boot and google cloud run">
<img src="http://blog.shanelee.name/content/images/2020/08/cloudcode2.png" alt="Jibbing with spring boot and google cloud run"></p>

<p>This command will build, push your image to GCR and deploy to cloud run. And that's it!</p>

<p>Now you can access the public url and test out the endpoint.</p>

<p><img src="http://blog.shanelee.name/content/images/2020/08/Cloudcodeexp.png" alt="Jibbing with spring boot and google cloud run"></p>

<p>The source code for the example is over on <a href="https://github.com/shavo007/spring-boot-jib">github</a></p>

<h2 id="apidesign">API design</h2>

<p>In my previous <a href="https://blog.shanelee.name/2019/05/17/graphql-api-google-cloud-run/">post</a> I discussed graphQL. Graphql provides a schema for introspection and type safety. </p>

<p>Now for REST, swagger or <a href="https://swagger.io/resources/open-api/">open API spec</a> is the standard for designing and documenting your API. Even <a href="https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-specification">kubernetes API </a> supports open api spec. Every time you call <code>kubectl describe &lt;resource&gt;</code> it calls this endpoint for info.</p>

<p>Tools such as <a href="https://support.insomnia.rest/article/94-introduction">insomnia designer</a> and api gateways now support open api spec.</p>

<p>I decided to design and document my greetings API using <a href="https://github.com/Redocly/redoc">redoc</a></p>

<h3 id="redoc">Redoc</h3>

<blockquote>
  <p>OpenAPI/Swagger-generated API Reference Documentation</p>
</blockquote>

<p>Redoc supports open api spec v3 and provides responsive documentation with code samples. There is many ways to deploy but again Ill go the docker way and deploy on cloud run.</p>

<h4 id="redocgenerator">Redoc generator</h4>

<p>Redoc has a <a href="https://github.com/Redocly/create-openapi-repo">generator</a> which provides a docs-like-code approach to OpenAPI definitions. It allows you to validate your spec before bundling it. I documented my greetings API above, bundled it and served it as static content from the API <a href="https://github.com/shavo007/spring-boot-jib/blob/master/src/main/resources/static/swagger.yaml">itself</a></p>

<p><mark>One thing to note is you need to enable CORS in your API to serve the static file. I also had an issue with @EnableMVC annotation so I removed that.</mark></p>

<p>To deploy, I built redoc on GCR and deployed to cloud run. You can run this in cloud shell if you like</p>

<pre><code class="language-bash">export PROJECT_ID=$(gcloud config list --format 'value(core.project)')  
#change CLOUD_RUN_SPRING_BOOT_BASE_URI to the location of your API deployed on cloud run

gcloud run deploy redoc-greetings-api --project $PROJECT_ID --image gcr.io/$PROJECT_ID/redoc --platform managed --region us-central1 --port 80 --cpu 1 --memory 256Mi --concurrency 80 --timeout 300 --update-env-vars SPEC_URL=https://&lt;CLOUD_RUN_SPRING_BOOT_BASE_URI&gt;/swagger.yaml  
</code></pre>

<p><img src="http://blog.shanelee.name/content/images/2020/08/Redoc.png" alt="Jibbing with spring boot and google cloud run"></p>

<p>Again, the src code can be found on <a href="https://github.com/shavo007/greetings-doc">github</a></p>

<p>And thats it. Happy jibbing! üï∫üèª</p>]]></content:encoded></item><item><title><![CDATA[How I passed the professional google cloud architect exam]]></title><description><![CDATA[<h1 id="sowhygooglecloud">So why Google cloud??</h1>

<p>Well, the Google Cloud Platform (GCP) is behind the 8 ball for sure in comparison to AWS. But with the huge spike in companies looking to reduce their Capex and move to the cloud, its offerings in the data analytics space for example is very enticing.</p>]]></description><link>http://blog.shanelee.name/2020/08/04/how-i-passed-the-professional-google-cloud-architect-exam/</link><guid isPermaLink="false">f54e6eec-b076-47af-959c-23f838c318f6</guid><category><![CDATA[cloud]]></category><category><![CDATA[google]]></category><category><![CDATA[kubernetes]]></category><category><![CDATA[exam]]></category><category><![CDATA[gcp]]></category><category><![CDATA[google cloud]]></category><category><![CDATA[architect]]></category><category><![CDATA[bigquery]]></category><category><![CDATA[linux academy]]></category><category><![CDATA[cloud native]]></category><category><![CDATA[hybrid]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 04 Aug 2020 06:47:49 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2020/08/morning-brew-T0qYg2nPUWM-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<h1 id="sowhygooglecloud">So why Google cloud??</h1>

<img src="http://blog.shanelee.name/content/images/2020/08/morning-brew-T0qYg2nPUWM-unsplash.jpg" alt="How I passed the professional google cloud architect exam"><p>Well, the Google Cloud Platform (GCP) is behind the 8 ball for sure in comparison to AWS. But with the huge spike in companies looking to reduce their Capex and move to the cloud, its offerings in the data analytics space for example is very enticing. Also in my view, they have the best managed kubernetes service GKE. Which makes sense, when you understand that kubernetes originated from googles own internal cluster orcherstration service called <a href="https://research.google/pubs/pub43438/">borg</a>. </p>

<p>I have worked with AWS for many years professionally. Gaining exposure to other cloud providers is important when you see companies adopting a  multi-cloud approach but fundamentally many of the underlying patterns and managed services commonly exist across the top 3 (AWS, GCP, Azure).</p>

<p>Up until recently I did not fully understand where <a href="https://www.forbes.com/sites/janakirammsv/2020/07/19/why-bigquery-omni-is-a-big-deal-for-google-cloud-customers-and-partners/#3644f89b1843">Anthos</a> fit into the picture. But now I do. By bringing compute closer to the data, existing cloud users can now use the likes of GKE and BigQuery. Nicely played Google! <strong>If the mountain won't come to muhammed...</strong> ‚ò∫Ô∏è </p>

<h2 id="certification">Certification</h2>

<p>This <a href="https://cloud.google.com/certification/cloud-architect">certification</a> allowed me to analyse and understand the majority of GCP services. But also provide a solid understanding of architectural best practices when it comes to design considerations, security, reliability and cost optimisation.</p>

<h2 id="exampreparation">Exam Preparation</h2>

<p>There are several training providers out there that provide courses specific to this exam: Coursera, Udemy, Linux academy to name but a few.</p>

<h3 id="courseras">Coursera‚Äôs:</h3>

<ul>
<li><p><a href="https://www.coursera.org/specializations/gcp-architecture">Architecting with Google Compute Engine
</a></p></li>
<li><p><a href="https://www.coursera.org/learn/preparing-cloud-professional-cloud-architect-exam/">Preparing for the Professional Cloud Architect Examination</a></p></li>
</ul>

<h3 id="linuxacademys">Linux Academy‚Äôs:</h3>

<ul>
<li><a href="https://linuxacademy.com/cp/modules/view/id/321">Google Cloud Certified Professional Cloud Architect
</a></li>
</ul>

<p>I took the updated course from Linux Academy‚Äôs Google Cloud Certified Professional Cloud Architect.</p>

<p>Linux Academy‚Äôs updated course was a good choice as it covers the majority of exam related GCP based topics and its practice sessions are an added advantage associated after each lesson. Their <a href="https://interactive.linuxacademy.com/diagrams/MasterBuildersGuide.html">master builders guide</a> was a very important document that I referred back to many times.</p>

<p>I also like to be hands on, so I signed up to <a href="https://www.qwiklabs.com/">qwiklabs</a> and completed many of the quests they provide in relation to GCP. It's extremely helpful especially if you do not have your own google cloud account. </p>

<p>Google also provides a <a href="https://lp.cloudplatformonline.com/rs/808-GJW-314/images/Professional%20Cloud%20Architect%20Journey.pdf">learning path</a> as an alternative option.</p>

<h2 id="examregistration">Exam registration</h2>

<p><a href="https://webassessor.com/wa.do?page=publicHome&amp;branding=GOOGLECLOUD">Register</a> for the exam when you are about 1 week into your study. This allows you to book in a slot not too far in the future and keeps you motivated.</p>

<h2 id="examlayout">Exam layout</h2>

<blockquote>
  <p>The exam is two hours long. You can take it remotely or at a test centre ( I took it at home). There are 50 questions - multiple choice and multiple select. The GCP case studies take up about 12 questions with the rest, GCP in general.</p>
</blockquote>

<h3 id="examscore">Exam score</h3>

<p>The passing marks are not shared by Google in any of GCP exam except it tells about pass or fail. However it has been assumed from various blogs and training providers that it‚Äôs roughly around 80% for GCP exams though no official confirmation about it.</p>

<h3 id="examreadiness">Exam readiness</h3>

<p>Check your readiness by taking the <a href="https://forms.gle/SHcLhSXckievBNBn6">google practice exam</a> and Linux academys (if you have signed up for the course). If you are not scoring 90% or above, then keep studying!</p>

<h3 id="tips">Tips</h3>

<ul>
<li><p>You will have plenty of time to go back and review marked questions. So make sure if you are stuck, mark question and move on. </p></li>
<li><p>Eliminate answers that you know are incorrect</p></li>
<li><p>Look out for main keywords in the question - speeds, low latency, failover, serverless etc etc </p></li>
</ul>

<h2 id="finalsuggestions">Final suggestions</h2>

<ul>
<li><p>Remember to deeply learn Kubernetes, Bigdata services(Big Query, BigTable), NoSQL options, App engine, Storage, IAM, BigQuery Roles, Case studies..</p></li>
<li><p>Googles new <a href="https://cloud.google.com/architecture/framework">architecture framework</a> is a very useful resource too.</p></li>
</ul>

<p><img src="http://blog.shanelee.name/content/images/2020/08/googlecert.jpg" alt="How I passed the professional google cloud architect exam"></p>

<p>Still waiting on my bag!! Best of luck.</p>]]></content:encoded></item><item><title><![CDATA[Road to clean energy and sustainability]]></title><description><![CDATA[<blockquote>
  <p>This is an overview of my journey to reducing my carbon footprint and making my home as energy efficient as possible.</p>
</blockquote>

<h2 id="energyefficiency">Energy efficiency</h2>

<p>WFH myself since March, I did not realise how cold and under insulated australian households can be! I'm european and I feel colder here than winter time</p>]]></description><link>http://blog.shanelee.name/2019/11/28/road-to-clean-energy-and-sustainability/</link><guid isPermaLink="false">e710a094-36a0-417a-980d-ad93850aa892</guid><category><![CDATA[solar]]></category><category><![CDATA[clean energy]]></category><category><![CDATA[melbourne]]></category><category><![CDATA[sustainability]]></category><category><![CDATA[cycle]]></category><category><![CDATA[bicycle]]></category><category><![CDATA[climatechange]]></category><category><![CDATA[renewables]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Thu, 28 Nov 2019 03:20:01 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2019/11/matt-duncan-IUY_3DvM__w-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
  <img src="http://blog.shanelee.name/content/images/2019/11/matt-duncan-IUY_3DvM__w-unsplash.jpg" alt="Road to clean energy and sustainability"><p>This is an overview of my journey to reducing my carbon footprint and making my home as energy efficient as possible.</p>
</blockquote>

<h2 id="energyefficiency">Energy efficiency</h2>

<p>WFH myself since March, I did not realise how cold and under insulated australian households can be! I'm european and I feel colder here than winter time in Vienna at minus 10 degrees! ü•∂</p>

<blockquote>
  <p>We‚Äôre lagging between 10-15 years behind in Europe, parts of US and Canada,‚Äù says Trivess Moore, a senior lecturer in construction at RMIT and member of the Sustainable Building Innovation Laboratory.</p>
</blockquote>

<p>You can read more on this topic where a housing estate project looked at building homes with at least an energy rating of <strong>7.5</strong> and the outcomes that came from this in relation to cost savings  but also wellbeing <br>
<a href="https://onestepoffthegrid.com.au/even-with-rooftop-solar-boom-consumers-are-paying-dearly-for-what-lies-underneath/">https://onestepoffthegrid.com.au/even-with-rooftop-solar-boom-consumers-are-paying-dearly-for-what-lies-underneath/</a></p>

<p>So what can we do about it?? Well, one recommendation I would like to see is for state and local councils to work with property developers and provide incentives to build energy efficient homes. But in the meantime, what can a homeowner do to rectify the situation... ü§î</p>

<h2 id="renewables">Renewables</h2>

<p>Last year, I installed solar panels on my house. This is something I had always planned to do when I purchased my own home. With Victorias rebate program, there is a huge uptake in residents installing solar. And why wouldn't you! Reduce your energy costs over time and become less reliant on the grid and our coal-burning plants. üåç</p>

<h3 id="solar">Solar</h3>

<p><img src="http://blog.shanelee.name/content/images/2019/11/antonio-garcia-ndz_u1_tFZo-unsplash.jpg" alt="Road to clean energy and sustainability"></p>

<p>Before I chose a solar installer to go with, I researched online to understand the best solution for my home.</p>

<p>Clean energy council (CEC) has a great guide on solar <a href="https://www.cleanenergycouncil.org.au/consumers">here</a>. I choose not to get a battery now as they are still quite expensive. But with the rapid innovation in this space, it won't be long in my view before it becomes attainable. </p>

<p><em>Victoria energy council has expanded the <a href="https://www.solar.vic.gov.au/solar-battery-rebate">battery rebate</a> to certain postcodes if applicable.</em></p>

<p>Finn at <a href="https://www.solarquotes.com.au/solar101.html">solar quotes</a> has invaluable information on choosing the right panels, inverters and provides quotes for local reviewed installers.</p>

<p>I contacted around five or six installers (all CEC accredited) and was surprised by the results. One actually refused to do a house inspection and focused solely on nearmap satellite imagery. Another stated that it was not worth installing based on their panel placement! </p>

<h4 id="systemchoice">System choice</h4>

<p>So the system I decided on was <strong>6.27KW</strong> Solar System with:</p>

<ul>
<li>19x Q-Cells Dual Cell Q.Peak Duo G5-330W</li>
<li>19x Micro-Inverters Enphase IQ7+</li>
<li>1x Envoy S-Metered + DRM</li>
<li>1x Sub-board installation to fit the Envoy relays and allow room for future use</li>
</ul>

<p>All panels were installed west facing.</p>

<p><img src="http://blog.shanelee.name/content/images/2019/11/enphase.gif" alt="Road to clean energy and sustainability"></p>

<p><strong>Enphase Micro-Inverters</strong> make each panel run independently, which is a big advantage compared to a string inverter system where when one panel starts to act faulty, it shuts down the entire array of panels.</p>

<p>It also comes with a monitoring system, the Enphase enlighten view, which allows you to monitor not only the PV production of your solar system but also your home electricity consumption.</p>

<p>More information on micro inverters can be found <a href="https://www.cleanenergyreviews.info/blog/microinverters">here</a></p>

<p>There are some myths about installing on the south for example but it really depends on where you live as Finn <a href="https://www.solarquotes.com.au/panels/direction/">explained</a>, but it will inevitably still produce energy! It all comes back to <strong>self consumption</strong></p>

<p>I have the monitoring setup using enlighten and have a cheeky look every day to see my usage and production.</p>

<p>Below is the energy produced for a typical summers day in Melbourne.</p>

<p><img src="http://blog.shanelee.name/content/images/2020/08/Screenshot_20200819-112109-1.png" alt="Road to clean energy and sustainability"></p>

<p>There is a great sense of satisfaction knowing I am using clean energy to power my house during the day.</p>

<h4 id="selfconsumption">Self consumption</h4>

<p>I mentioned earlier about self-consumption. As my wife and I both work during the day (pre-covid), we try to utilize solar power when we can.</p>

<p>For example, I turn on the dishwasher before I leave for work in the morning and set the washing machine on a timer to turn on at peak time around 12-1 pm during the day.</p>

<p>Here is a great <a href="https://www.solar.vic.gov.au/making-most-solar">case study</a> of how a young family self consume.</p>

<p>It is important to understand that the feed-in tariff (FIT) most likely will fluctuate in the future (in VIC Jul 2020 reduced from 12c to 10.2c) so try and use up as much clean energy as possible.</p>

<p>Lastly, here is a fantastic write up of a resident using <a href="https://onestepoffthegrid.com.au/analysis-of-my-home-battery-solar-systems-first-year-performance/">solar</a> for a year</p>

<h2 id="anenergyefficienthome">An energy efficient home</h2>

<p>After installing solar, I looked into other areas I could make my home more energy efficient.</p>

<p>I stumbled across <a href="https://www.victorianenergysaver.vic.gov.au/save-energy-and-money/discount-energy-saving-products/save-with-these-energy-efficient-products">vic energy saver site </a> that provides savings for energy efficient products.</p>

<p>I had existing halogen downlights that I wanted to replace with LEDs and this initiative allowed me to replace all for <strong>FREE</strong>! </p>

<p>I used an accredited <a href="https://www.easybeinggreen.com.au/">provider</a> and it was pretty painless. They replaced all 40 of my downlights (apart from my dimmers). <strong>#winning</strong></p>

<p>The program even supports showerheads, weather sealing, and hotwater systems. Buying energy efficient appliances such as your oven, heat pump dryer, washing machine will make a difference too.</p>

<h3 id="getaninspection">Get an inspection</h3>

<p>Finally, if you want you can look into an <a href="https://www.victorianenergysaver.vic.gov.au/save-energy-and-money/get-a-home-energy-assessment">energy assessment</a> of your house.</p>

<h2 id="waste">Waste</h2>

<p>I was happy to see the Vic government bring in the plastic bag ban. Definitely well overdue. I believe in Ireland it came into play about ten years ago! </p>

<p>Since China stopped accepting alot of our waste the government has been scrambling to fix the waste management issue. I am happy to see Vic government outlining their <a href="https://www.vic.gov.au/sites/default/files/2020-02/Recycling%20Victoria%20A%20new%20economy.pdf">plan.</a> Highlights are:</p>

<ul>
<li>Four bin system üôåüèª</li>
<li>Container deposit scheme</li>
<li>Circular economy</li>
</ul>

<p>I actually saw new <a href="https://www.yarracity.vic.gov.au/services/roads-and-traffic/wellington-street-bike-lanes--stage-2">popup bicycle lanes</a> in fitzroy north recently using recycled plastic üëç</p>

<p>More info on using recycled materials can be found <a href="https://www.sustainability.vic.gov.au/en/About-us/Latest-news/2020/08/14/00/50/New-funding-to-use-recycled-materials">here</a></p>

<h3 id="foodwaste">Food waste</h3>

<blockquote>
  <p>Each year in Victoria households throw out 250,000 tonnes worth of food ‚Äì enough wasted food to fill Melbourne's Eureka Tower. üò±</p>
</blockquote>

<p>My wife and I have made a joint effort when shopping to buy fruit and veg loosely if possible. And also buy organically too. We have trialed growing some herbs and vegetables at home. Trial and error! But a great learning experience.</p>

<p>I listened to a podcast recently with Dr. Sandro Demaio (CEO of VicHealth) on <a href="https://open.spotify.com/episode/6w4JIeLLrnqUIDsX4jMC3E?si=nhJmzw81RtWAyH57FoNDGA">food</a> and its impact on society, our ecosystem and health. Less red meat people!!  <strong>#votewithyournote</strong></p>

<p>We also bring our <a href="https://www.redcycle.net.au/">scrunchy plastic</a> to the supermarket. Both coles and woolworths support the initiative.</p>

<h2 id="transport">Transport</h2>

<p>After energy, transport is the biggest carbon emitter. </p>

<blockquote>
  <p>Transport in Australia contributes around 100 million tonnes of greenhouse gasses into our atmosphere every year.</p>
</blockquote>

<p>The main reasons for transport emissions trending upwards are an over-dependence on cars with high average fuel use and an over-reliance on energy-intensive road freight.</p>

<p>So it comes to my favourite topic of all active transport and cycling!</p>

<h3 id="activetransport">Active transport</h3>

<p>Anyone who knows me, knows how I love cycling. I learned to cycle from a very young age in the irish countryside before you had to worry about cars on the road. Wherever I have worked and lived, whether in europe or australia my preferred mode of transport is the bike. For me, I love the independence and time alone on the bike. Plus it's a great way to wake up in the morning (especially if you need to cycle down chapel st ü§£)</p>

<p>Anytime my wife and I go abroad, I always go check out a city's shared bicycle scheme if available and explore the city by bike. This to me is the best way to see a city. On a warm summer evening cycling down along the banks of the river seine in Paris listening to some jazz and watching the world go by is pure bliss. üá´üá∑</p>

<p><img src="http://blog.shanelee.name/content/images/2020/08/IMG-20180522-WA0000-1.jpg" alt="Road to clean energy and sustainability"></p>

<p>Since covid there has been a huge uptake in cycling. I notice it myself at the weekend and on the trails. Governments like <a href="https://ecf.com/news-and-events/news/ireland-will-invest-10-total-transport-capital-budget-cycling?s=09">ireland</a> and the <a href="https://www.forbes.com/sites/carltonreid/2020/07/27/well-build-thousands-of-miles-of-protected-cycleways-pledges-boris-johnson/">UK</a> have invested heavily in protected cycleways and pedestrian infrastructure. </p>

<p>Change is coming, there is no doubt about it. The new paris mayor Anne Hidalgo has called it the <a href="https://www.theguardian.com/world/2020/feb/07/paris-mayor-unveils-15-minute-city-plan-in-re-election-campaign">15 minute city</a> idea. That is to have all services within a 15 minute walk/ride from your home. The days of commuter towns and work in the city after covid will not be the same. Working from home and even coworking spaces in rural areas will become more mainstream I predict.</p>

<blockquote>
  <p>‚ÄúWe need to reinvent the idea of urban proximity,‚Äù Moreno says. ‚ÄúWe know it is better for people to work near to where they live, and if they can go shopping nearby and have the leisure and services they need around them too, it allows them to have a more tranquil existence.</p>
</blockquote>

<h3 id="electricvehicles">Electric vehicles</h3>

<p>Just a quick mention on EVs. I am still waiting for the national EV strategy to be released in Australia. It has been delayed many times. Nudge, nudge Angus Taylor...</p>

<p>Now take <a href="https://thedriven.io/2020/07/10/the-electric-recipe-of-norways-zero-emissions-transport-boom/">Norway</a> for example - The Norwegian Parliament has decided on a goal that all new cars sold by <strong>2025</strong> should be zero (battery electric or hydrogen) emission vehicles. </p>

<p>Unfortunately for alot of aussies, EVs are just too expensive right now and there is no incentive to switch.</p>

<h2 id="investingforthefuture">Investing for the future</h2>

<p>I just finished Ross Garnuat book recently <a href="https://books.google.com.au/books/about/Superpower.html?id=KPiPDwAAQBAJ&amp;printsec=frontcover&amp;source=kp_read_button&amp;redir_esc=y#v=onepage&amp;q&amp;f=false">Superpower: Australia's low carbon opportunity</a> and our renewable investment needs to increase dramatically. The IMF says Australia should be spending more on infrastructure, but this should be on rail, airports and seaports, <strong>rather</strong> than roads.</p>

<p>With low interest rates for the forseeable future, Deloitte, Grattan institute, and many others have stated now is the time to invest and invest smartly. The IMF has called this "great reset" an opportunity for fiscal stimulus that is <strong>smarter</strong>, <strong>greener</strong> and <strong>fairer</strong>. So lets push ahead and be bold for a change.</p>

<h2 id="finally">Finally</h2>

<p>I know reducing our carbon footprint sounds challenging and some feel overwhelmed by it all. But I hope this article helps educate others and know there is support out there. Albert Einstein once said: </p>

<blockquote>
  <p>‚ÄúIn the middle of difficulty lies opportunity.‚Äù </p>
</blockquote>

<p>We <strong>all</strong> have an opportunity to make a difference and create a more sustainable future. </p>]]></content:encoded></item><item><title><![CDATA[GraphQL API on the new Google Cloud Run]]></title><description><![CDATA[ Cloud Run, a serverless environment based on containers and Kubernetes.]]></description><link>http://blog.shanelee.name/2019/05/17/graphql-api-google-cloud-run/</link><guid isPermaLink="false">1dfdf148-8167-4016-8259-29cd1cd39f87</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[google]]></category><category><![CDATA[graphql]]></category><category><![CDATA[k8s]]></category><category><![CDATA[docker]]></category><category><![CDATA[gke]]></category><category><![CDATA[cloud]]></category><category><![CDATA[cloudrun]]></category><category><![CDATA[nodejs]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Fri, 17 May 2019 08:07:00 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2019/05/fabio-comparelli-696510-unsplash-1.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
  <img src="http://blog.shanelee.name/content/images/2019/05/fabio-comparelli-696510-unsplash-1.jpg" alt="GraphQL API on the new Google Cloud Run"><p>Cloud Run is a layer that Google built on top of Knative to simplify deploying serverless applications on the Google Cloud Platform.</p>
</blockquote>

<p>So whats <strong>Knative</strong>?? </p>

<p><a href="https://cloud.google.com/knative/">Knative</a> provides an open API and runtime environment that enables you to run your serverless workloads anywhere you choose: fully managed on Google Cloud, on Google Kubernetes Engine (GKE), or on your own Kubernetes cluster.</p>

<p>Knative can be deployed on any Kubernetes cluster. It acts as the middleware bridging the gap between core infrastructure services and developer experience.</p>

<p>Cloud Run is Googles own implementation of Knative. <br>
 It enables you to run stateless containers that are invocable via web requests or Cloud Pub/Sub events.</p>

<p>Features include:</p>

<ul>
<li>Fast autoscaling</li>
<li>Managed</li>
<li>Redundancy</li>
<li>Integrated logging and monitoring</li>
<li>Custom domains</li>
<li>Built on knative</li>
</ul>

<p>There are <strong>two</strong> options when it comes to using Google Cloud Run. </p>

<h2 id="flavorsofcloudrun">Flavors of Cloud Run</h2>

<p>Currently in beta, Google Cloud Run is available as a standalone environment and within the Google Kubernetes Engine (GKE).</p>

<p>Developers can deploy apps to Cloud Run through the console or CLI. If there is a GKE cluster with Istio installation, apps targeting Cloud Run can be easy deployed to an existing Kubernetes cluster.</p>

<p>Each deployment to service creates a revision. A revision consists of a specific container image, along with environment settings such as environment variables, memory limits, or concurrency value.</p>

<p>Requests are automatically routed as soon as possible to the latest healthy service revision.</p>

<p>For more check out the video below on the differences.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/RVdhyprptTQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<h2 id="tutorial">Tutorial</h2>

<p>In this tutorial, we will deploy a graphql API based on Node.js and Postgres to the Cloud Run platform.</p>

<p>There are two steps involved in this workflow: provisioning a cloud sql postgres database instance, and deploying code to Cloud Run. This tutorial assumes you have an active account on Google Cloud Platform with the CLI and SDK installed on your development machine. </p>

<p><del>You also need Docker Desktop to build images locally.</del></p>

<p>Actually, that's not true! You don't even need docker locally!! You can use google cloud build to run it remotely for you ü§ì</p>

<h3 id="letsbegin">Lets begin</h3>

<p>To follow along, you can find the github project <a href="https://github.com/shavo007/graphql-playground/tree/master/api">here</a></p>

<p>In a previous <a href="https://blog.shanelee.name/2019/02/20/skaffold-for-local-kubernetes-development/">post</a>, I talked about running this example on kubernetes using skaffold</p>

<h4 id="googlecloudsetup">Google cloud setup</h4>

<p>First on your google cloud account you need to enable billing and <a href="http://console.cloud.google.com/apis/library/run.googleapis.com">cloud run API</a></p>

<p>Create your new google cloud project first and then run the following commands</p>

<pre><code class="language-bash">gcloud components install beta #install beta components  
gcloud components update #update components  
gcloud config set run/region us-central1 #set cloud run region  
gcloud services enable container.googleapis.com containerregistry.googleapis.com cloudbuild.googleapis.com  
gcloud config set project [PROJECT_ID] #set project id  
gcloud beta auth login  
</code></pre>

<h4 id="step1createcloudsqlforpostgres">Step 1: create cloud sql for postgres</h4>

<p>GraphQL APIs datasource is a postgres database. Cloud run supports <a href="https://cloud.google.com/run/docs/configuring/connect-cloudsql">cloud sql</a> service.</p>

<pre><code class="language-bash">gcloud sql instances create [INSTANCE_NAME]  --database-version=POSTGRES_9_6 \  
       --tier db-f1-micro --region us-central1 
#save on costs by using a shared-core instance 
gcloud sql users set-password postgres no-host --instance=[INSTANCE_NAME] \  
       --password=[PASSWORD]
</code></pre>

<p>For testing purposes, I am running a micro instance. </p>

<p><a href="https://cloud.google.com/sql/docs/postgres/create-instance">Refer to doc
</a> for more info</p>

<p><img src="http://blog.shanelee.name/content/images/2019/05/CloudSQL.png" alt="GraphQL API on the new Google Cloud Run"></p>

<h4 id="step2buildinganddeployingacloudrunservice">Step 2: Building and Deploying a Cloud Run Service</h4>

<p><mark>You can find the docker file and src in the github repo</mark></p>

<p>We will build the Docker image remotely and push it to Google Container Registry (GCR)</p>

<pre><code class="language-bash">gcloud builds submit --tag gcr.io/[PROJECT-ID]/graphql #build container image  
</code></pre>

<p>Verify the image exists in GCR</p>

<pre><code class="language-bash">gcloud container images list  
</code></pre>

<p>Deploy the container using cloud run and overwrite env vars to connect to the DB</p>

<pre><code class="language-bash">#overwrite the host to connect over a unix domain socket and db password
gcloud beta run deploy --image gcr.io/[PROJECT-ID]/graphql --add-cloudsql-instances [INSTANCE-NAME] --update-env-vars DB_HOST=/cloudsql/[CONNECTION NAME],name=graphql,DATABASE_PASSWORD=[PASSWORD] #respond y to allow unauthenticated invocations.  
</code></pre>

<p>The switch, <mark>‚Äìallow-unauthenticated</mark>, will let the service accept the traffic from the public internet. Notice that we are passing the Postgres connection string generated by cloudSQL as an environment variable. The code expects the connection string from the <mark>DBHOST</mark> environment variable.</p>

<p>See details of the running service by running the command below</p>

<pre><code class="language-bash">gcloud beta run services list  
</code></pre>

<p>Or you can view the UI console</p>

<p><img src="http://blog.shanelee.name/content/images/2019/05/CloudRunServiceGraphql.png" alt="GraphQL API on the new Google Cloud Run"></p>

<p>You can verify it works locally by using <a href="https://insomnia.rest/">Insomnia</a></p>

<p><img src="http://blog.shanelee.name/content/images/2019/05/insomnia.png" alt="GraphQL API on the new Google Cloud Run"></p>

<p>Some sample queries can be seen below to sign in a user and get back their details</p>

<pre><code class="language-json"> mutation
 {
   signUp(username: "shane", email: "slee@x.com",password: "xxx") {
     token
   }
 }

 query {
   users {
     username
     id
   }
 }
</code></pre>

<h4 id="finally">Finally</h4>

<p>Finally, delete all resources after.</p>

<p>Easiest way is to delete the test <a href="https://console.cloud.google.com/iam-admin/projects">project</a></p>

<p>Any questions feel free to comment below. </p>]]></content:encoded></item><item><title><![CDATA[Skaffold for local kubernetes development]]></title><description><![CDATA[skaffold for local kubernetes development]]></description><link>http://blog.shanelee.name/2019/02/20/skaffold-for-local-kubernetes-development/</link><guid isPermaLink="false">939f9b10-3288-4a62-83c4-8de5aa50b6f6</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[docker]]></category><category><![CDATA[node]]></category><category><![CDATA[skaffold]]></category><category><![CDATA[graphql]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Wed, 20 Feb 2019 06:37:09 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2019/02/jacek-dylag-680347-unsplash-1.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
  <img src="http://blog.shanelee.name/content/images/2019/02/jacek-dylag-680347-unsplash-1.jpg" alt="Skaffold for local kubernetes development"><p>Easy and Repeatable Kubernetes Development</p>
</blockquote>

<p><strong>TLDR</strong> Below, I will showcase how to install and use skaffold for local development with kubernetes.</p>

<p>Currently <strong>(20/02/2019)</strong>, skaffold has nearly <mark>6000</mark> ‚≠ê on github.</p>

<p>I have been using Skaffold for all my new projects that involve cloud native microservices, and it works like a charm on top of <code>Docker Desktop for Mac</code>/Minikube.</p>

<p><strong>Skaffold</strong> is fantastic for local development with kubernetes. I can test locally my changes without having to deploy remotely. This helps speed up my local development and gives me confidence in my changes.</p>

<h2 id="overview">Overview</h2>

<p>Skaffold is a tool to develop containerized applications locally or remotely while deploying them on Kubernetes. It automatically builds and deploys your apps as you change your source code.</p>

<p>Skaffold primarily simplifies the <mark>build ‚Üí deploy ‚Üí refactor ‚Üí repeat</mark> cycle.</p>

<h3 id="skaffoldmodes">Skaffold modes</h3>

<p><img src="http://blog.shanelee.name/content/images/2019/02/skaffold-cmds.jpg" alt="Skaffold for local kubernetes development"></p>

<p>In a single command, Skaffold can:</p>

<ul>
<li>Collects and watches your source code for changes</li>
<li>Syncs files directly to pods if user marks them as syncable</li>
<li>Builds artifacts from the source code</li>
<li>Tests the built artifacts using container-structure-tests</li>
<li>Tags the artifacts</li>
<li>Pushes the artifacts</li>
<li>Deploys the artifacts</li>
<li>Monitors the deployed artifacts</li>
<li>Cleans up deployed artifacts on exit (Ctrl+C)</li>
</ul>

<h2 id="skaffoldfeatures">Skaffold features</h2>

<ul>
<li><p><strong>Remote development:</strong> Skaffold doesn‚Äôt require you to run a local Kubernetes cluster (minikube or docker-for-desktop). It can build/push images locally with docker, and run them on the remote clusters (such as GKE). This is a laptop battery saver!</p></li>
<li><p><strong>More remote development:</strong> You actually don‚Äôt need to run a local docker either. Skaffold can do remote builds using services like Google Container Builder. Although it‚Äôll be slow.</p></li>
<li><p><strong>Tag management:</strong> In your Kubernetes manifests, you leave the image tags out in the ‚Äúimage:‚Äù field, and Skaffold automatically changes the manifests with the new tags as it rebuilds the images.</p></li>
<li><p><strong>Rebuild only what‚Äôs changed:</strong> If your microservices are on separate directories, changing source code for one will not cause rebuild for all images. Skaffold understands which images have been impacted by the change.</p></li>
<li><p><strong>Cleanup on exit:</strong> Terminating ‚Äúskaffold dev‚Äù runs a routine that cleans up the deployed k8s resources. If this fails, you can run ‚Äúskaffold delete‚Äù to clean up deployed artifacts.</p></li>
</ul>

<h2 id="letsgetstarted">Lets get started!</h2>

<p>On mac, you can install skaffold using brew</p>

<pre><code class="language-bash">brew install skaffold  
</code></pre>

<h3 id="localdevelopment">Local development</h3>

<p>Run <code>skaffold init</code> to bootstrap Skaffold config.</p>

<p>Once that is complete, define in the yaml file the location of where your kubernetes manifests are defined.</p>

<p>Sample skaffold yaml file</p>

<pre><code class="language-yaml">apiVersion: skaffold/v1beta5  
kind: Config  
build:  
  artifacts:
  - image: shanelee007/graphql
deploy:  
  kubectl:
    manifests:
    - kubernetes/config.yaml
    - kubernetes/deployment.yaml
    - kubernetes/secret.yaml
profiles:  
- name: dev
  build:
    artifacts:
    - image: shanelee007/graphql
      sync:
        '**/*.js': .
      docker:
        dockerfile: Dockerfile.dev
</code></pre>

<p>Here you can see where I defined my manifest files. Also for local development I have used a <code>profile</code> to define a development dockerfile and utilised the sync feature. </p>

<blockquote>
  <p>profiles feature grants you the freedom to switch tools as you see fit depending on the context.</p>
</blockquote>

<h3 id="localdevelopmentworkflow">Local development workflow</h3>

<p><img src="http://blog.shanelee.name/content/images/2019/02/skaffold_workflow_local.png" alt="Skaffold for local kubernetes development"></p>

<h5 id="syncfilestoyourpodswithskaffold">Sync files to your pods with Skaffold</h5>

<p>With even one change to a file, Skaffold rebuilds the images that depend on that file, pushes them to a registry, and then redeploys the relevant parts of your Kubernetes application. </p>

<p>The Skaffold file sync feature solves this problem. For each image, you can specify which files can be synced directly into a running container. Then, when you modify these files, Skaffold copies them directly into the running container rather than kicking off a full rebuild and redeploy. With Skaffold‚Äôs file sync feature, you can enjoy even faster development!</p>

<p><a href="https://skaffold.dev/docs/how-tos/filesync/">Sync</a> is quite a new feature. Think of it as similar to <code>nodemon</code></p>

<p>I have created my own demo <strong>github</strong> project <a href="https://github.com/shavo007/graphql-playground/tree/master/api#skaffold">here</a> if you want to follow along.</p>

<p>There was an issue with publishing docker image for local development every time I ran skaffold. To prevent this from happening there is global config to disable this.</p>

<pre><code class="language-bash">skaffold config set --global local-cluster true #do not push images after building  
</code></pre>

<h4 id="noteondockerfile">Note on Dockerfile</h4>

<p>In my github project, you can see I use multi-stage approach with my docker <a href="https://github.com/shavo007/graphql-playground/blob/master/api/Dockerfile">files</a></p>

<p>Think of it as a build pipeline as code.</p>

<p>It allows you to selectively copy artifacts from one stage to another, leaving behind everything you don‚Äôt want in the final image. </p>

<p>To analyse your final production image I found a useful tool called <a href="https://github.com/wagoodman/dive">dive</a></p>

<p>It allows you to explore and optimise your docker image size.</p>

<p><img src="http://blog.shanelee.name/content/images/2019/02/Screen-Shot-2019-02-20-at-7-23-39-pm.png" alt="Skaffold for local kubernetes development"></p>

<p>Now we can run skaffold!</p>

<pre><code class="language-bash">skaffold dev -p dev -v=info #run locally/watching changes dev mode  
</code></pre>

<p>If you want to try out the new experimental gui run instead </p>

<pre><code class="language-bash">skaffold dev -p dev -v=info --experimental-gui  
</code></pre>

<p>Console output will look like so:</p>

<p><a href="https://asciinema.org/a/220028" target="_blank"><img src="https://asciinema.org/a/220028.svg" alt="Skaffold for local kubernetes development"></a></p>

<p>Every time you make a src code change, skaffold will watch for these changes and update the pod on the fly. Pretty neat!!</p>

<p><img src="https://media.giphy.com/media/vEgtLzJo8n7qg/giphy.gif" alt="Skaffold for local kubernetes development"></p>

<h4 id="upgrade">Upgrade</h4>

<p>As of <strong>20/02/2019</strong> the latest version is <code>v1beta5</code>. To upgrade just run </p>

<pre><code class="language-bash">brew upgrade skaffold  
skaffold fix --overwrite  
</code></pre>

<p>Thats all for now. In my next post, I will discuss deploying remotely using skaffold.üòÉ</p>

<h2 id="learnmore">Learn more</h2>

<ul>
<li><p>Check out skaffold <a href="https://skaffold.dev/docs/">doc</a> to understand more</p></li>
<li><p>There is numerous <a href="https://github.com/GoogleContainerTools/skaffold/tree/master/examples">examples</a> at the github repo</p></li>
<li><p>My sample github <a href="https://github.com/shavo007/graphql-playground/tree/master/api">repo</a> showcasing graphql api with kubernetes</p></li>
</ul>]]></content:encoded></item><item><title><![CDATA[How I aced the Certified Kubernetes Administrator (CKA) Exam]]></title><description><![CDATA[<p>I took the CKA exam recently and would like to share my own preparation and tips.</p>

<p>The exam is intense as it requires 3 hours of concentrated effort during which you need to solve 24 problems. So timekeeping is very important.  If you are interested to take this exam and</p>]]></description><link>http://blog.shanelee.name/2018/10/17/how-i-aced-the-certified-kubernetes-administrator-cka-exam/</link><guid isPermaLink="false">ab6a2a07-ad36-4be5-ae74-d629d25a25bb</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[cka]]></category><category><![CDATA[exam]]></category><category><![CDATA[kubectl]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Wed, 17 Oct 2018 07:29:30 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2018/10/clement-h-544786-unsplash.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://blog.shanelee.name/content/images/2018/10/clement-h-544786-unsplash.jpg" alt="How I aced the Certified Kubernetes Administrator (CKA) Exam"><p>I took the CKA exam recently and would like to share my own preparation and tips.</p>

<p>The exam is intense as it requires 3 hours of concentrated effort during which you need to solve 24 problems. So timekeeping is very important.  If you are interested to take this exam and have second thought about it, my sincere advice is, just do it!</p>

<h2 id="gettingstarted">Getting started</h2>

<p>As part of black friday deal last year I bought from linux foundation the package deal. It consisted of the CKA exam and linux foundation kubernetes fundamentals <a href="https://training.linuxfoundation.org/training/kubernetes-fundamentals/">course</a>.</p>

<p>In a previous contract role I had worked with kubernetes also on a daily basis. So I was familiar with the basic concepts.</p>

<h2 id="preparation">Preparation</h2>

<ul>
<li><p>Understanding the kubernetes documentation is vital here as that is the only resource you are now allowed as part of the exam. Search ‚Äú<strong>What resources am I allowed to access during my exam?</strong>‚Äù at <a href="https://www.cncf.io/certification/cka/faq/">faq section</a></p></li>
<li><p>Familiarising yourself with the official <a href="https://kubernetes.io/docs/concepts/">documentation</a> is a <strong>must</strong>.</p></li>
<li><p>Practice each topic. Since this exam is all about solving problems, you need to practice a lot. You can use katacoda <a href="https://www.katacoda.com/courses/kubernetes/playground">playground</a> or this <a href="https://labs.play-with-k8s.com/">labs</a> site by Docker. These two tools helped me a lot during preparation. Katacoda also has several scenario based topics and it is important to practice <a href="https://www.katacoda.com/courses/kubernetes">these</a>.</p></li>
<li><p>For each topic I found this <a href="https://cka-exam.blog/">blog</a>  extremely useful to go through.
<em>Practice, practice, practice</em>. Get used to deploying on a cluster and using all the <strong>kubectl</strong> commands. Locally on my mac, <em>docker for mac</em> now incorporates kubernetes so no need for minikube anymore.</p></li>
<li><p>Kubernetes the hard way - Kelsey hightowers tutorial on <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">github</a> is vital to pass this exam. Managed services like AKS, EKS or kubeadm are not going to help here. You need to get right under the hood and understand how the control plane works.</p></li>
<li><p>Familiarise yourself with tools such as: openssl, cfssl, systemctl, etcdctl (for managing etcd)</p></li>
</ul>

<h2 id="examtime">Exam Time</h2>

<p>There are 24 problems and the exam duration is 3 hours. This means you can spend seven and half minutes per question. However, the difficulty range varies; nearly dozen problems are straight and super simple. So, have a strategy for the exam. My strategy was to complete 10 easy questions in the first hour, 8 medium in the second hour and leave final hour for remaining 6 (tough) questions. Once you are done with the question, just double check whether you are saving the output as described in the question and move on (don‚Äôt try to come back and check the answers again). <strong>You will not have time to go back and check!</strong></p>

<h3 id="progresstracking">Progress Tracking</h3>

<p>The CKA exam allows you to write notes to a notebook, which they provide in exam‚Äôs UI. Use it wisely. Once I had my exam available, I used their Notebook to keep track of my progress. It helps you to see your progress and how much left to be finished. </p>

<p>The format I used:</p>

<pre><code class="language-vim"># - scoring - total
1 4 4  
2 5 9  
3 3 12  
4  
5  
...
24  
</code></pre>

<p>The first column is just the number of question (24 in total). The second column‚Ää‚Äî‚Ääscoring, is how much the problem worth and you will get that from the problem description. The third column is ‚Äútotal‚Äù, where you put the total scoring as you go and it will let you know what problems are already completed and how many points you already have. This little bureaucracy will help you to understand how well you are doing and come back to some problems if you decided to skip them before. </p>

<h3 id="kubectlninja">Kubectl Ninja</h3>

<p>Use kubectl to create resources (such as deployment, service, cronjobs etc) instead of creating them from manifest files. It saves lot of time. <br>
I used the <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">cheatsheet</a> heavily during the exam. </p>

<p>First thing I did was copy in the below to notepad. It allowed me to create resources quickly from <code>stdin</code>  </p>

<pre><code class="language-bash">cat &lt;&lt;EOF | kubectl create -f -

EOF  
</code></pre>

<blockquote>
  <p>Take the exam in the morning. </p>
</blockquote>

<p>I took it at 11AM. I went for a quick walk beforehand to get some fresh air and then I was ready to go!</p>

<p>To reiterate, familiarise yourself with the official documentation. It has multiple sections (tasks, concepts and references) for the same topic. So, you should know what/where to look for quickly in the documentation during exam.</p>

<p>Best of luck! Remember you have one free retake if needed.üòÉüöÄ‚öì</p>

<p><img src="http://blog.shanelee.name/content/images/2018/10/CKAcertificate.png" alt="How I aced the Certified Kubernetes Administrator (CKA) Exam"></p>]]></content:encoded></item><item><title><![CDATA[Istio on Azure AKS]]></title><description><![CDATA[How to deploy istio service mesh on azure kubernetes service (AKS) and run bookinfo example application]]></description><link>http://blog.shanelee.name/2018/08/12/istio-on-azure-aks/</link><guid isPermaLink="false">1300cd8a-88a9-43ef-a409-66a2b74c5ad0</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[azure]]></category><category><![CDATA[aks]]></category><category><![CDATA[istio]]></category><category><![CDATA[google]]></category><category><![CDATA[service-mesh]]></category><category><![CDATA[k8s]]></category><category><![CDATA[microservice]]></category><category><![CDATA[grafana]]></category><category><![CDATA[jaeger]]></category><category><![CDATA[tracing]]></category><category><![CDATA[metrics]]></category><category><![CDATA[prometheus]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 12 Aug 2018 04:20:09 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2018/08/peter-gonzalez-621059-unsplash-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="http://blog.shanelee.name/content/images/2018/08/peter-gonzalez-621059-unsplash-1.jpg" alt="Istio on Azure AKS"><p><img src="http://blog.shanelee.name/content/images/2018/08/istio.png" alt="Istio on Azure AKS"></p>

<p>Istio recently <a href="https://istio.io/about/notes/1.0">announced</a> that they are production ready. Service meshes are becoming an important level of abstraction for a developer using kubernetes. And <a href="https://www.envoyproxy.io/">Envoy</a> is the heartbeat of this service mesh and continues its impressive growth. </p>

<blockquote>
  <p>Istio reduces complexity of managing microservice deployments by providing a uniform way to secure, connect, and monitor microservices.</p>
</blockquote>

<p>Google have also released a managed istio <a href="https://cloudplatform.googleblog.com/2018/07/cloud-services-platform-bringing-the-best-of-the-cloud-to-you.html">service</a></p>

<p>I have previously designed and built cloud native architectures (especially on AWS). But I found that AWS have <em>dropped the ball</em> in relation to kubernetes. Azure and Google managed kuberentes services are more mature and Azure kubernetes offering is even free! </p>

<p><strong>TLDR:</strong> I particulary like Azure AKS and below I will showcase how easy it is to create a cluster and run istio.</p>

<p>Brendan Burns (co-founder of kubernetes and leading the azure container team) and Microsoft have invested wisely I feel in the cloud and are kicking goals. Read the latest <a href="https://azure.microsoft.com/en-us/blog/azure-kubernetes-service-aks-ga-new-regions-new-features-new-productivity/">blog</a> from Brendan on the most recent releases and updates. </p>

<h2 id="launchkubernetesclusteronazureaks">Launch kubernetes cluster on azure (AKS)</h2>

<p>Below these commands assume you have azure cli installed. If not check out <a href="https://docs.microsoft.com/en-us/cli/azure/get-started-with-azure-cli?view=azure-cli-latest">azure cli setup</a></p>

<p>Like previous posts, I also use bash aliases for kubectl. Github project exists <a href="https://github.com/ahmetb/kubectl-aliases">here</a></p>

<p>Find the location you want to create the cluster and what version of kubernetes to run. Then create the cluster with three worker nodes.</p>

<pre><code class="language-bash">    az provider list --query "[?namespace=='Microsoft.ContainerService'].resourceTypes[] | [?resourceType=='managedClusters'].locations[]" -o tsv

    az aks get-versions --location "Australia East" --query "orchestrators[].orchestratorVersion" 

    az group create --name myResourceGroup1 --location "Australia East"

    az aks create --resource-group myResourceGroup1 --name myAKSCluster --node-count 3 --kubernetes-version 1.11.1 --generate-ssh-keys
</code></pre>

<p>It takes a few minutes to spin up the cluster. Once its up, download the kubeconfig to use kubectl locally </p>

<pre><code class="language-bash">  az aks get-credentials --resource-group myResourceGroup1 --name myAKSCluster
</code></pre>

<p>Verify the nodes are running <code>kgnoowide</code></p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-2-28-58-pm.png" alt="Istio on Azure AKS"></p>

<h2 id="deployistio">Deploy istio</h2>

<p>Istio is installed in two parts. The first part involves the CLI tooling that will be used to deploy and manage Istio backed services. The second part configures the Kubernetes cluster to support Istio.</p>

<pre><code>curl -L https://git.io/getLatestIstio | sh -  
cd istio-1.0.0  
export PATH=$PWD/bin:$PATH
</code></pre>

<h3 id="configureistiocrds">Configure Istio CRDs</h3>

<p><code>kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml
</code></p>

<h3 id="installistiowithdefaultmutualtlsauthentication">Install Istio with default mutual TLS authentication</h3>

<p><code>kubectl apply -f install/kubernetes/istio-demo-auth.yaml
</code></p>

<p>This will deploy Pilot, Mixer, Ingress-Controller, Egress-Controller and the Istio CA (Certificate Authority).</p>

<p>Check the status of the pods <br>
<code>kgpoowide -n istio-system</code></p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-2-44-04-pm.png" alt="Istio on Azure AKS"></p>

<h3 id="istioarchitecture">Istio architecture</h3>

<p><img src="http://blog.shanelee.name/content/images/2018/08/istio-architecture.png" alt="Istio on Azure AKS"></p>

<p>The previous step deployed the Istio Pilot, Mixer, Ingress-Controller, Egress-Controller and the Istio CA (Certificate Authority).</p>

<ul>
<li><p><strong>Pilot</strong> - Responsible for configuring the Envoy and Mixer at runtime.</p></li>
<li><p><strong>Envoy</strong> - Sidecar proxies per microservice to handle ingress/egress traffic between services in the cluster and from a service to external services. The proxies form a secure microservice mesh providing a rich set of functions like discovery, rich layer-7 routing, circuit breakers, policy enforcement and telemetry recording/reporting functions.</p></li>
<li><p><strong>Mixer</strong> - Create a portability layer on top of infrastructure backends. Enforce policies such as ACLs, rate limits, quotas, authentication, request tracing and telemetry collection at an infrastructure level.</p></li>
<li><p><strong>Ingress/Egress</strong> - Configure path based routing.</p></li>
<li><p><strong>Istio CA</strong> - Secures service to service communication over TLS. Providing a key management system to automate key and certificate generation, distribution, rotation, and revocation</p></li>
</ul>

<h3 id="deploybookinfoexampleapplication">Deploy BookInfo example application</h3>

<p>This sample deploys a simple application composed of four separate microservices which will be used to demonstrate various features of the Istio service mesh.</p>

<p>Enable default side-car injection <br>
<code>kubectl label namespace default istio-injection=enabled
</code></p>

<p>Deploy the services <br>
<code>kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml
</code></p>

<p>Verify the pods and services are running <br>
<code>kubectl get svc,pod</code></p>

<p>Deploy the ingress <a href="https://istio.io/docs/concepts/traffic-management/#gateways">gateway</a></p>

<p><code>kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml
</code></p>

<p>Now determine the ingress ip and port</p>

<p><code>kubectl get svc istio-ingressgateway -n istio-system
</code>
Set the ingress ip and port</p>

<pre><code class="language-bash">export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')  
export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')  
export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')  
</code></pre>

<p>Set the <code>gateway url</code></p>

<p><code>export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
</code></p>

<p>Verify the app is up and running <br>
<code>curl -o /dev/null -s -w "%{http_code}\n" http://${GATEWAY_URL}/productpage
</code></p>

<h5 id="applydefaultdestinationrules">Apply default destination rules</h5>

<p>Before you can use Istio to control the Bookinfo version routing, you need to define the available versions, called subsets, in destination rules.</p>

<p><code>kubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml
</code></p>

<blockquote>
  <p>Deploying a microservice-based application in an Istio service mesh allows one to externally control service monitoring and tracing, request (version) routing, resiliency testing, security and policy enforcement, etc., in a consistent way across the services, for the application as a whole.</p>
</blockquote>

<p>Werner Vogels (CTO of AWS) quoted at AWS Re:Invent  </p>

<blockquote>
  <p>"In the future, all the code you ever write will be business logic." </p>
</blockquote>

<p>Service mesh goes along way in helping you succeed that statement.</p>

<h5 id="controlrouting">Control Routing</h5>

<p>One of the main features of Istio is its traffic management. As a Microservice architectures scale, there is a requirement for more advanced service-to-service communication control.</p>

<h6 id="userbasedtestingrequestrouting">User Based Testing / Request Routing</h6>

<p>One aspect of traffic management is controlling traffic routing based on the HTTP request, such as user agent strings, IP address or cookies.</p>

<p>The example below will send all traffic for the user "jason" to the reviews:v2, meaning they'll only see the black stars.</p>

<p><code>kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml</code></p>

<p>Visit the product page and signin as a user jason (password jason)</p>

<h6 id="trafficshapingforcanaryreleases">Traffic Shaping for Canary Releases</h6>

<p>The ability to split traffic for testing and rolling out changes is important. This allows for A/B variation testing or deploying canary releases.</p>

<p>The rule below ensures that 50% of the traffic goes to reviews:v1 (no stars), or reviews:v3 (red stars).</p>

<p><code>kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml</code></p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-10-00-pm.png" alt="Istio on Azure AKS"></p>

<h4 id="newreleases">New Releases</h4>

<p>Given the above approach, if the canary release were successful then we'd want to move 100% of the traffic to reviews:v3.</p>

<p><code>kubectl apply -f samples/bookinfo/networking/virtual-service-reviews-v3.yaml</code></p>

<h5 id="listallroutes">List all routes</h5>

<pre><code class="language-bash">$ istioctl get virtualservices
VIRTUAL-SERVICE NAME   GATEWAYS           HOSTS     #HTTP     #TCP      NAMESPACE   AGE  
bookinfo               bookinfo-gateway   *             1        0      default     20m  
reviews                                   reviews       1        0      default     5m  
</code></pre>

<h5 id="accessmetrics">Access Metrics</h5>

<p><img alt="Istio on Azure AKS" src="http://blog.shanelee.name/content/images/2018/08/prometheus-icon-color-1.png" style="width: 250px; height:250px"></p>

<p>With Istio's insight into how applications communicate, it can generate profound insights into how applications are working and performance metrics.</p>

<p>Send traffic to the application</p>

<pre><code class="language-bash">while true; do  
  curl -s http://$GATEWAY_URL/productpage &gt; /dev/null
  echo -n .;
  sleep 0.2
done  
</code></pre>

<p>Setup port forwarding  </p>

<pre><code class="language-bash">kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=prometheus -o jsonpath='{.items[0].metadata.name}') 9090:9090 &amp;  
</code></pre>

<p>View metrics in <a href="http://localhost:9090/graph#%5B%7B%22range_input%22%3A%221h%22%2C%22expr%22%3A%22istio_double_request_count%22%2C%22tab%22%3A1%7D%5D">Prometheus UI</a></p>

<p>The provided link opens the Prometheus UI and executes a query for values of the <code>istio_double_request_count</code> metric.</p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-26-44-pm.png" alt="Istio on Azure AKS"></p>

<p>Prometheus was recently <a href="https://www.cncf.io/announcement/2018/08/09/prometheus-graduates/">promoted</a> from CNCF as a graduate project, following kubernetes.</p>

<h5 id="grafana">Grafana</h5>

<p>Verify the services are up  </p>

<pre><code>kubectl -n istio-system get svc grafana prometheus  
</code></pre>

<p>Open the Istio Dashboard via the Grafana UI.</p>

<p>In Kubernetes environments, execute the following command:</p>

<pre><code>$ kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath='{.items[0].metadata.name}') 3000:3000 &amp;
</code></pre>

<p>Visit <code>http://localhost:3000/dashboard/db/istio-mesh-dashboard</code> in your web browser.</p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-35-45-pm.png" alt="Istio on Azure AKS"></p>

<p>This gives the global view of the Mesh along with services and workloads in the mesh. </p>

<p>For more info checkout <a href="https://istio.io/docs/tasks/telemetry/using-istio-dashboard/">https://istio.io/docs/tasks/telemetry/using-istio-dashboard/</a></p>

<h5 id="distributedtracing">Distributed Tracing</h5>

<p><img alt="Istio on Azure AKS" src="http://blog.shanelee.name/content/images/2018/08/jaeger-icon-color.png" style="width: 250px; height:250px"></p>

<p>This task shows you how Istio-enabled applications can be configured to collect trace spans. After completing this task, you should understand all of the assumptions about your application and how to have it participate in tracing, regardless of what language/framework/platform you use to build your application.</p>

<h6 id="accessingthedashboard">Accessing the dashboard</h6>

<p>Setup access to the Jaeger dashboard by using port-forwarding:</p>

<pre><code>$ kubectl port-forward -n istio-system $(kubectl get pod -n istio-system -l app=jaeger -o jsonpath='{.items[0].metadata.name}') 16686:16686 &amp;
</code></pre>

<p>Access the Jaeger dashboard by opening your browser to <code>http://localhost:16686</code>.</p>

<p>From the left-hand pane of the Jaeger dashboard, select <code>productpage</code> from the Service drop-down list and click Find Traces. You should see something similar to the following:</p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-43-04-pm.png" alt="Istio on Azure AKS"></p>

<p>If you click on the top (most recent) trace, you should see the details corresponding to your latest refresh of the /productpage. The page should look something like this:</p>

<p><img src="http://blog.shanelee.name/content/images/2018/08/Screen-Shot-2018-08-12-at-3-43-16-pm.png" alt="Istio on Azure AKS"></p>

<p>And there you have it. For more information on istio check out <a href="https://istio.io/">https://istio.io/</a></p>

<p>Dont forget to delete your cluster after your finished!</p>

<p>Stay tuned for more posts on <strong>kubernetes</strong>.</p>]]></content:encoded></item><item><title><![CDATA[Test drive heptio sonobuoy diagnostic kubernetes tool]]></title><description><![CDATA[<blockquote>
  <p>Heptio Sonobuoy is a diagnostic tool that makes it easier to understand the state of a Kubernetes cluster by running a set of Kubernetes conformance tests in an accessible and non-destructive manner. </p>
</blockquote>

<p>Heptio have come out with some useful kubernetes tools; sonobuoy being one. </p>

<p>In this post, I will spin</p>]]></description><link>http://blog.shanelee.name/2018/02/03/test-drive-heptio-sonobuoy-diagnostic-kubernetes-tool/</link><guid isPermaLink="false">395f2c06-dbbf-46ae-a335-0c12203079ef</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[docker]]></category><category><![CDATA[aws]]></category><category><![CDATA[heptio]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 03 Feb 2018 09:33:23 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2018/08/joseph-barrientos-93565.jpg" medium="image"/><content:encoded><![CDATA[<blockquote>
  <img src="http://blog.shanelee.name/content/images/2018/08/joseph-barrientos-93565.jpg" alt="Test drive heptio sonobuoy diagnostic kubernetes tool"><p>Heptio Sonobuoy is a diagnostic tool that makes it easier to understand the state of a Kubernetes cluster by running a set of Kubernetes conformance tests in an accessible and non-destructive manner. </p>
</blockquote>

<p>Heptio have come out with some useful kubernetes tools; sonobuoy being one. </p>

<p>In this post, I will spin up a kubernetes cluster on AWS using latest version of kops <strong>(1.8)</strong> and test drive sonobuoy scanner tool.</p>

<p>Rather than install kops and kubectl locally, I have published a docker image that you can use as a utility container.</p>

<p><a href="https://hub.docker.com/r/shanelee007/alpine-kops/">Alpine kops</a> docker image includes kops, kubectl, terraform, aws cli and helm. The swiss army knife for kubernetes!! </p>

<h2 id="kubernetesinstallation">Kubernetes installation</h2>

<p>Run the container  </p>

<pre><code class="language-bash">docker run --rm -it \  
  -v "$HOME"/.ssh:/root/.ssh:ro \
  -v "$HOME"/.aws:/root/.aws:ro \
  -v "$HOME"/.kube:/root/.kube:rw \
  -v "$HOME"/.helm:/root/.helm:rw \
  -v "$(pwd)":/workdir \
  -w /workdir \
  shanelee007/alpine-kops
</code></pre>

<p>Then create the cluster on AWS. Here I am creating one master instance and two worker nodes</p>

<pre><code class="language-bash">  kops create cluster --v=0 \
    --cloud=aws \
    --node-count 2 \
    --master-size=m3.medium \
    --master-zones=ap-southeast-2a \
    --zones ap-southeast-2a,ap-southeast-2c \
    --name= ${NAME} \
    --node-size=m3.medium \
    --node-volume-size=20
</code></pre>

<p>Before scanning the cluster, I will deploy a few applications.</p>

<p>Bitnami have come out with kubeapps, to easily deploy apps on your cluster.</p>

<h2 id="bitnamikubeapps">Bitnami KubeApps</h2>

<blockquote>
  <p>Kubeapps is a Kubernetes dashboard that supercharges your Kubernetes cluster with simple browse and click deployment of apps in any format. </p>
</blockquote>

<h3 id="installation">Installation</h3>

<pre><code class="language-bash">sudo curl -L https://github.com/kubeapps/installer/releases/download/v0.2.0/kubeapps-linux-amd64 -o /usr/local/bin/kubeapps &amp;&amp; sudo chmod +x /usr/local/bin/kubeapps  
</code></pre>

<p>To see what it installs, run dry run first</p>

<pre><code class="language-bash">  kubeapps up --dry-run -o yaml
</code></pre>

<p>Once your happy, lets kick it off</p>

<pre><code class="language-bash"> kubeapps up
</code></pre>

<h3 id="dashboard">Dashboard</h3>

<p>Once Kubeapps is installed, securely access the Kubeapps Dashboard from your system by running:</p>

<pre><code class="language-bash">kubeapps dashboard  
</code></pre>

<p>This will start an HTTP proxy for secure access to the Kubeapps Dashboard and launch your default browser to access it. </p>

<h3 id="deploywordpress">Deploy wordpress</h3>

<p>Using the "Charts" menu from the Dashboard welcome page I will select wordpress application from the list of charts in the official Kubernetes chart repository.</p>

<h2 id="sonobuoy">Sonobuoy</h2>

<p>Now lets install sonobuoy on the cluster and run the diagnostics.</p>

<p>You will find the steps <a href="https://scanner.heptio.com">here</a></p>

<p>Once you run the command, you will see in the browser to see the conformance results.</p>

<p><img src="http://blog.shanelee.name/content/images/2018/02/Screen-Shot-2018-02-03-at-9-35-17-pm.png" alt="Test drive heptio sonobuoy diagnostic kubernetes tool"></p>

<p>You will see two pods running in the namespace heptio-sonobuoy</p>

<pre><code class="language-bash">NAME                                READY     STATUS    RESTARTS   AGE  
sonobuoy                            3/3       Running   0          5m  
sonobuoy-e2e-job-bf586487f2f64f0b   2/2       Running   0          4m  
</code></pre>

<p>It may take up to 60 mins to run the tests. So sit back and relax... üòâ</p>

<p>To see what is happening you can use kubetail to tail the logs</p>

<pre><code class="language-bash">kubetail sonobuoy -n heptio-sonobuoy  
kubetail sonobuoy-e2e-job-bf586487f2f64f0b -n heptio-sonobuoy
</code></pre>

<h3 id="sonobuoyresults">Sonobuoy results</h3>

<p>Once it finishes, you can download the results and keep the report by exporting as a pdf.</p>

<p><img src="http://blog.shanelee.name/content/images/2018/02/Screen-Shot-2018-02-03-at-10-19-17-pm.png" alt="Test drive heptio sonobuoy diagnostic kubernetes tool"></p>

<p>All tests passed. There you have it!</p>

<p>Stay tuned for more kubernetes goodness.. ‚öìÔ∏è</p>]]></content:encoded></item><item><title><![CDATA[Logging on kubernetes with fluentd and elasticsearch 6]]></title><description><![CDATA[How to configure EFK (Elastic-Fluentd-Kibana) stack on kubernetes for logging]]></description><link>http://blog.shanelee.name/2017/12/17/logging-on-kubernetes-with-fluentd-and-elasticsearch-6/</link><guid isPermaLink="false">cac30a75-368d-462f-aee8-9c510bfd9de1</guid><category><![CDATA[elasticsearch]]></category><category><![CDATA[kubernetes]]></category><category><![CDATA[docker]]></category><category><![CDATA[ingress]]></category><category><![CDATA[nginx]]></category><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[curator]]></category><category><![CDATA[fluentd]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 16 Dec 2017 13:06:59 GMT</pubDate><media:content url="http://blog.shanelee.name/content/images/2018/01/robert-larsson-64547-1.jpg" medium="image"/><content:encoded><![CDATA[<h1 id="tldr">TLDR</h1>

<img src="http://blog.shanelee.name/content/images/2018/01/robert-larsson-64547-1.jpg" alt="Logging on kubernetes with fluentd and elasticsearch 6"><p>The solution I have used in the past for logging in kubernetes clusters is EFK (Elastic-Fluentd-Kibana). Alot of you have probably heard of ELK stack but I find that logstash is more heavyweight and does not provide the same output plugins as fluentd.</p>

<p><img alt="Logging on kubernetes with fluentd and elasticsearch 6" src="http://blog.shanelee.name/content/images/2017/12/icon-elasticsearch-bb-1.svg" style="width: 300px; height:300px"></p>

<h2 id="elasticsearch">Elasticsearch</h2>

<p>Elasticsearch 6 has just been <a href="https://www.elastic.co/blog/elasticsearch-6-0-0-released">announced</a> with some major performance improvements.</p>

<p>You have a few options to deploy elasticsearch - elastic cloud or spin up your own ES cluster in kubernetes. But I have decided to go with the AWS managed service. It now has <a href="https://aws.amazon.com/blogs/aws/amazon-elasticsearch-service-now-supports-vpc/">VPC support</a> and the new release 6.0. </p>

<p>The reason being that administrating Elasticsearch can be a lot of work, as many people experienced with the system will tell you it can be tricky to keep running smoothly and that it‚Äôs a task better outsourced to an external service.</p>

<p>I recently read dubsmash story <a href="https://stackshare.io/dubsmash/dubsmash-scaling-to-200-million-users-with-3-engineers">https://stackshare.io/dubsmash/dubsmash-scaling-to-200-million-users-with-3-engineers</a> and how they focus on the product deliverables and not infrastructure. If you have a small team or not in the business of managing services such as ES, i find its best to outsource..</p>

<p>Following the 12 factor principles for logging <a href="https://12factor.net/logs">https://12factor.net/logs</a> all applications that are containerised (using docker) should log to STDOUT.</p>

<blockquote>
  <p>A twelve-factor app never concerns itself with routing or storage of its output stream. It should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout.</p>
</blockquote>

<p>Whether your using winston for nodeJS or console appender for spring boot, this is the recommended way. </p>

<p>Every pod in a <strong>K8S</strong> cluster has its standard output and standard error captured and stored in the /var/log/containers/ node directory. </p>

<p>Now you need a logging agent ( or logging shipper) to ingest these logs and output to a target. </p>

<h2 id="fluentd">FluentD</h2>

<p><img src="http://blog.shanelee.name/content/images/2017/12/container_logging-1024x536.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<p>Fluentd is an open-source framework for data collection that unifies the collection and consumption of data in a pluggable manner. There are several producer and consumer loggers for various kinds of applications. It has a huge suite of <a href="https://www.fluentd.org/plugins">plugins</a> to choose from. </p>

<p>Fluentd will be deployed as a <strong>daemonset</strong> on the kubernetes cluster.</p>

<p>Kubernetes logs the content of the stdout and stderr streams of a pod to a file. It creates one file for each container in a pod. The default location for these files is /var/log/containers . The filename contains the pod name, the namespace of the pod, the container name, and the container id. The file contains one JSON object per line of the two streams stdout and stderr. </p>

<p>A DaemonSet ensures that a certain pod is scheduled to each kubelet exactly once. The fluentd pod mounts the <strong>/var/lib/containers/</strong> host volume to access the logs of all pods scheduled to that kubelet.</p>

<h2 id="whatsmissinggeo">Whats missing - Geo</h2>

<p><img src="http://blog.shanelee.name/content/images/2017/12/joao-silas-72562.jpg" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<p>I found with the examples online for fluentd daesmonset there was none that supported <strong>geoip</strong>. Coreos offering and fluentd official kubernetes daemonset do not provide this feature. </p>

<p>Geoip is a very useful tool when inspecting access logs through kibana. I use nginx  ingress controller in kubernetes and I wanted to see where incoming requests arose geographically. </p>

<p>Knowing from where in the world people are accessing your website is important not only for troubleshooting and operational intelligence but also for other use cases such as business intelligence</p>

<h3 id="ingressandnginx">Ingress and Nginx</h3>

<p>I use nginx as a reverse proxy behind AWS ELB to manage my routing. By default, nginx does not output to json. And instead of figuring out the fluentd nginx parser I decided to configure nginx to enable json logging. </p>

<p>Sample conf can be seen below:</p>

<pre><code class="language-yaml">kind: ConfigMap  
apiVersion: v1  
metadata:  
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app: ingress-nginx
data:  
  use-proxy-protocol: "false"
  log-format-escape-json: "true"
  log-format-upstream: '{"proxy_protocol_addr": "$proxy_protocol_addr","remote_addr": "$remote_addr", "proxy_add_x_forwarded_for": "$proxy_add_x_forwarded_for",
   "request_id": "$request_id","remote_user": "$remote_user", "time_local": "$time_local", "request" : "$request", "status": "$status", "vhost": "$host","body_bytes_sent": "$body_bytes_sent",
   "http_referer":  "$http_referer", "http_user_agent": "$http_user_agent", "request_length" : "$request_length", "request_time" : "$request_time",
    "proxy_upstream_name": "$proxy_upstream_name", "upstream_addr": "$upstream_addr",  "upstream_response_length": "$upstream_response_length",
    "upstream_response_time": "$upstream_response_time", "upstream_status": "$upstream_status"}'
</code></pre>

<h3 id="fluentddockerimage">Fluentd docker image</h3>

<p>I then extended the fluentd debian elasticsearch docker image to install the geo-ip plugin and also update the max mind database.</p>

<p>Docker Image can be found <a href="https://hub.docker.com/r/shanelee007/fluentd-kubernetes/">here</a> on docker hub. Tag is <strong>v0.12-debian-elasticsearch-geo</strong></p>

<pre><code class="language-bash">docker pull shanelee007/fluentd-kubernetes:v0.12-debian-elasticsearch-geo  
</code></pre>

<p>Now I needed to amend the fluentd config to filter my nginx access logs and translate ip address to geo co-ordinates. </p>

<p>Sample config in the yaml file for daemonset with updated database is</p>

<pre><code class="language-yaml">  geoip-filter.conf: |
    &lt;filter kube.ingress-nginx.nginx-ingress-controller&gt;
        type geoip

        # Specify one or more geoip lookup field which has ip address (default: host)
        # in the case of accessing nested value, delimit keys by dot like 'host.ip'.
        geoip_lookup_key  remote_addr

        # Specify optional geoip database (using bundled GeoLiteCity databse by default)
        geoip_database    "/home/fluent/GeoLiteCity.dat"

        # Set adding field with placeholder (more than one settings are required.)
        &lt;record&gt;
          city            ${city["remote_addr"]}
          lat             ${latitude["remote_addr"]}
          lon             ${longitude["remote_addr"]}
          country_code3   ${country_code3["remote_addr"]}
          country         ${country_code["remote_addr"]}
          country_name    ${country_name["remote_addr"]}
          dma             ${dma_code["remote_addr"]}
          area            ${area_code["remote_addr"]}
          region          ${region["remote_addr"]}
          geoip           '{"location":[${longitude["remote_addr"]},${latitude["remote_addr"]}]}'
        &lt;/record&gt;

        # To avoid get stacktrace error with `[null, null]` array for elasticsearch.
        skip_adding_null_record  true

        # Set log_level for fluentd-v0.10.43 or earlier (default: warn)
        log_level         info

        # Set buffering time (default: 0s)
        flush_interval    1s
    &lt;/filter&gt;
</code></pre>

<p>I also updated the elasticsearch template to version 6 as there was issues with version 5.</p>

<p>Now to the fun part... testing it out!</p>

<h2 id="tryityourself">Try it yourself</h2>

<p>This tutorial can be executed in less than 15 minutes, as log as you already have:</p>

<ul>
<li><p>Kubernetes cluster up</p></li>
<li><p>Nginx ingress installed</p></li>
</ul>

<h3 id="installing">Installing</h3>

<p>Github project for creating all resources in kubernetes can be found at <a href="https://github.com/shavo007/k8s-ingress-letsencrypt/tree/master">https://github.com/shavo007/k8s-ingress-letsencrypt/tree/master</a></p>

<p>To create the namespace and manifests for logging, the only change you need is to update the elasticsearch endpoint in the configmap.</p>

<p>File is located at <a href="https://github.com/shavo007/k8s-ingress-letsencrypt/blob/master/resources/logging/fluentd-configmap.yaml#L291">https://github.com/shavo007/k8s-ingress-letsencrypt/blob/master/resources/logging/fluentd-configmap.yaml#L291</a></p>

<p>Then you can create the resources. </p>

<h4 id="kubectlaliases">Kubectl aliases</h4>

<p>For the commands below I am using bash aliases. Aliases save me alot of time when I am querying a cluster. You can find the github project <a href="https://github.com/ahmetb/kubectl-aliases">here</a></p>

<pre><code class="language-bash">ka resources/logging --record  
</code></pre>

<p>Verify the pods are created</p>

<pre><code class="language-bash">kgpon logging  
</code></pre>

<p>If you have the dashboard installed you can inspect the logs there or you can tail the logs from the command line using <a href="https://github.com/johanhaleby/kubetail">kubetail</a></p>

<pre><code class="language-bash">kubetail fluentd -n logging  
</code></pre>

<h2 id="kibana">Kibana</h2>

<p><img alt="Logging on kubernetes with fluentd and elasticsearch 6" src="http://blog.shanelee.name/content/images/2017/12/icon-kibana-bb.svg" style="width: 300px; height:300px"></p>

<p>Now access kibana - GUI for elasticsearch</p>

<p>You should now see your logs ingested in elasticsearch. </p>

<p><img src="http://blog.shanelee.name/content/images/2017/12/Screen-Shot-2017-12-26-at-3-10-18-pm.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<h3 id="tilemapvisualization">Tile map visualization</h3>

<p>There is a limitation with managed service and tile map view. I customized the settings to use WMS compliant map server. See below:</p>

<p><img src="http://blog.shanelee.name/content/images/2017/12/Screen-Shot-2017-12-26-at-3-14-43-pm.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<p>Once you pick the pattern, and assuming your geopoints are correctly mapped, Kibana will automatically populate the visualisation settings such as which field to aggregate on, and display the map almost instantly.</p>

<p>I have defined a visualisation for heatmap. This helps in visualizing geospatial data. You can go to management and import the json file from here  <a href="https://github.com/shavo007/k8s-ingress-letsencrypt/blob/master/resources/assets/export.json">https://github.com/shavo007/k8s-ingress-letsencrypt/blob/master/resources/assets/export.json</a></p>

<p>This includes multiple visualizations and a dashboard.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/12/Screen-Shot-2017-12-17-at-11-47-26-am.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<h4 id="inaccurategeolocation">Inaccurate Geolocation</h4>

<p>You may find the IP is matched to an inaccurate location. Be aware that the free Maxmind database that is used is ‚Äúcomparable to, but less accurate than, MaxMind‚Äôs GeoIP2 databases‚Äù, and, ‚ÄúIP geolocation is inherently imprecise. Locations are often near the center of the population.‚Äù See the MaxMind site for further details.</p>

<h4 id="widgets">Widgets</h4>

<p>You can build up more widgets, such as url count or count by country.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/12/Screen-Shot-2017-12-26-at-3-43-45-pm.png" alt="Logging on kubernetes with fluentd and elasticsearch 6"></p>

<h2 id="cleanupoflogindices">Cleanup of log indices</h2>

<p>For cleanup of old log indices there is a tool called <a href="https://github.com/elastic/curator">curator</a>. I found a serverless option <a href="https://github.com/cloudreach/aws-lambda-es-cleanup">https://github.com/cloudreach/aws-lambda-es-cleanup</a> and created the lambda function via <strong>terraform</strong>.</p>

<p>You can easily schedule the lambda function to cleanup log indices greater than x no. of days.</p>

<p>And thats it! Stay tuned for more posts on <strong>kubernetes</strong>. </p>]]></content:encoded></item><item><title><![CDATA[Kubernetes ingress and sticky sessions]]></title><description><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Recently I had to look at horizontally scaling a traditional web-app on kubernetes. Here i will explain how I achieved it and what ingress controller is and why to use it.</p>

<p>I  assume you know what pods are so I will quickly breakdown service and ingress resources.</p>

<h2 id="service">Service</h2>]]></description><link>http://blog.shanelee.name/2017/10/16/kubernetes-ingress-and-sticky-sessions/</link><guid isPermaLink="false">52c2d7b6-231c-4d3b-8fcd-7f4fa090d576</guid><category><![CDATA[kubernetes]]></category><category><![CDATA[docker]]></category><category><![CDATA[ingress]]></category><category><![CDATA[sticky]]></category><category><![CDATA[elb]]></category><category><![CDATA[nginx]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 15 Oct 2017 14:21:48 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Recently I had to look at horizontally scaling a traditional web-app on kubernetes. Here i will explain how I achieved it and what ingress controller is and why to use it.</p>

<p>I  assume you know what pods are so I will quickly breakdown service and ingress resources.</p>

<h2 id="service">Service</h2>

<blockquote>
  <p>Service is a logical abstraction communication layer to pods. During normal operations pods get‚Äôs created, destroyed, scaled out, etc.</p>
</blockquote>

<p>A Service make‚Äôs it easy to always connect to the pods by connecting to their service which stays stable during the pod life cycle. A important thing about services are what their type is, it determines how the service expose itself to the cluster or the internet. Some of the service types are :</p>

<ul>
<li><p>ClusterIP Your service is only expose internally to the cluster on the internal cluster IP. A example would be to deploy Hasicorp‚Äôs vault and expose it only internally.</p></li>
<li><p>NodePort Expose the service on the EC2 Instance on the specified port. This will be exposed to the internet. Off course it this all depends on your AWS Security group / VPN rules.</p></li>
<li><p>LoadBalancer Supported on Amazon and Google cloud, this creates the cloud providers your using load balancer. So on Amazon it creates a ELB that points to your service on your cluster.</p></li>
<li><p>ExternalName Create a CNAME dns record to a external domain.</p></li>
</ul>

<p>For more information about Services look at <a href="https://kubernetes.io/docs/concepts/services-networking/service/">https://kubernetes.io/docs/concepts/services-networking/service/</a></p>

<h2 id="ingress">Ingress</h2>

<blockquote>
  <p>An Ingress is a collection of rules that allow inbound connections to reach the cluster services</p>
</blockquote>

<p>You define a number of <strong>rules</strong> to access a <strong>service</strong></p>

<h2 id="scenario">Scenario</h2>

<p>Imagine this scenario, you have a cluster running, on Amazon, you have multiple applications deployed to it, some are jvm microservices (spring boot) running inside embedded tomcat, and to add to the mix, you have a couple of SPA sitting in a Apache web server that serves static content. </p>

<p>All applications needs to have TLS, some of the api‚Äôs endpoints have changed, but you still have to serve the old endpoint path, so you need to do some sort of path rewrite. How do you expose everything to the internet? The obvious answer is create a type <strong>LoadBalancer</strong> service for each, but, then multiple ELB‚Äôs will be created, you have to deal with TLS termination at each ELB, you have to CNAME your applications/api‚Äôs domain names to the right ELB‚Äôs, and in general just have very little control over the ELB.</p>

<p>Enter Ingress Controllers. üëç</p>

<h3 id="whatisaningresscontroller">What is an ingress controller?</h3>

<blockquote>
  <p>An Ingress Controller is a daemon, deployed as a Kubernetes Pod, that watches the apiserver's /ingresses endpoint for updates to the Ingress resource. Its job is to satisfy requests for Ingresses.</p>
</blockquote>

<p>You deploy a ingress controller, create a type LoadBalancer service for it, and it sits and monitors Kubernetes api server‚Äôs <strong>/ingresses</strong> endpoint and acts as a reverse proxy for the ingress rules it found there. </p>

<p>You then deploy your application and expose it‚Äôs service as a type NodePort, and create ingress rules for it. The ingress controller then picks up the new deployed service and proxy traffic to it from outside.</p>

<p>Following this setup, you only have one ELB then on Amazon, and a central place at the ingress controller to manage the traffic coming into your cluster to your applications.</p>

<p>To visualise how this works, check out this little guy! Traefik is one implementation you can use as an ingress.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/10/architecture.png" alt=""></p>

<p>But I have chosen nginx ingress controller instead as it supports sticky sessions and as a reverse proxy is extremely popular solution.</p>

<p>So lets get to the interesting part; <strong>coding</strong>!!!</p>

<h2 id="demo">Demo</h2>

<p>I am going to setup a kubernetes gossip cluster on AWS using kops. Then create nginx ingress controller and reverse proxy to a sample app called echoheader. </p>

<p>To setup a k8s cluster on AWS, follow the guide at <a href="https://github.com/shavo007/k8s-ingress">https://github.com/shavo007/k8s-ingress</a></p>

<p>If you do not want to install kops and the other tools needed, I have built a simple docker image that you can use instead. </p>

<p><a href="https://store.docker.com/community/images/shanelee007/alpine-kops">https://store.docker.com/community/images/shanelee007/alpine-kops</a></p>

<p>This includes:</p>

<ul>
<li>Kops</li>
<li>Kubectl</li>
<li>AWS CLI</li>
<li>Terraform</li>
</ul>

<p>Once you have the cluster what we need to do is setup a default backend service for nginx.</p>

<p>The default backend is the default service that nginx falls backs to if if cannot route a request successfully. The default backend needs to satisfy the following two requirements :</p>

<p>serves a 404 page at / <br>
serves 200 on a /healthz</p>

<p>See more at <a href="https://github.com/kubernetes/ingress-nginx/tree/master/deploy">https://github.com/kubernetes/ingress-nginx/tree/master/deploy</a></p>

<p>Run the mandatory commands and install without RBAC roles.</p>

<p>Then install layer 7 service on AWS <br>
<a href="https://github.com/kubernetes/ingress-nginx/tree/master/deploy#aws">https://github.com/kubernetes/ingress-nginx/tree/master/deploy#aws</a> or install the service defined in my repo</p>

<pre><code class="language-bash">kubectl apply -f ingress-nginx-svc.yaml  
</code></pre>

<p>When you run these commands, it created a deployment with one replica of the nginx-ingress-controller and a service for it of type LoadBalancer which created a ELB for us on AWS. Let‚Äôs confirm that. Get the service :</p>

<pre><code class="language-bash">kubectl get services -n ingress-nginx  -o wide | grep nginx  
</code></pre>

<p>We can now test the default back-end</p>

<pre><code class="language-bash">ELB=$(kubectl get svc ingress-nginx -n ingress-nginx  -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')

curl $ELB  
</code></pre>

<p>You should see the following:  </p>

<pre><code class="language-bash">default backend - 404  
</code></pre>

<p>All good so far.. </p>

<p>This means everything is working correctly and the ELB forwarded traffic to our nginx-ingress-controller and the nginx-ingress-controller passed it along to the default-backend-service that we deployed.</p>

<h4 id="deployourapplication">Deploy our application</h4>

<p>Now run</p>

<pre><code class="language-bash">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/images/echoheaders/echo-app.yaml

kubectl apply -f ingress.yaml  
</code></pre>

<p>This will create deployment and service for echo-header app. This app simply returns information about the http request as output.</p>

<p>If you look at the ingress resource, you will see annotations defined. </p>

<p><code>ingress.kubernetes.io/ssl-redirect: "true"</code>  will redirect http to https.</p>

<p>To view all annotations check out <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md">https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md</a></p>

<p>One ingress rule is to route all requests for virtual host foo.bar.com to service  <strong>echoheaders</strong> on path /backend. So lets test it out!</p>

<pre><code class="language-bash">curl $ELB/backend -H 'Host: foo.bar.com'
</code></pre>

<p>You should get 200 response back with request headers and other info.</p>

<h3 id="stickysessions">Sticky sessions</h3>

<p>Now to one of the main features that nginx provides.  nginx-ingress-controller can handle sticky sessions as it bypass the service level and route directly the pods. More info can be found here <br>
<a href="https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/affinity/cookie">https://github.com/kubernetes/ingress-nginx/tree/master/docs/examples/affinity/cookie</a></p>

<p><mark>Update (17/10/2017) examples have been removed from repo! To find out more on the annotations related to sticky session go to <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md#miscellaneous</mark>">https://github.com/kubernetes/ingress-nginx/blob/master/docs/annotations.md#miscellaneous==</a></mark></p>

<p>To test it out we need to first scale our app echo-headers: Lets scale echo-headers deployment to three pods</p>

<pre><code class="language-bash"> kubectl scale --replicas=3 deployment/echoheaders
</code></pre>

<p>Now lets create the sticky ingress</p>

<pre><code class="language-bash">apiVersion: extensions/v1beta1  
kind: Ingress  
metadata:  
  name: nginx-test-sticky
  annotations:
    kubernetes.io/ingress.class: "nginx"
    ingress.kubernetes.io/affinity: "cookie"
    ingress.kubernetes.io/session-cookie-name: "route"
    ingress.kubernetes.io/session-cookie-hash: "sha1"

spec:  
  rules:
  - host: stickyingress.example.com
    http:
      paths:
      - backend:
          serviceName: echoheaders
          servicePort: 80
        path: /foo
</code></pre>

<p>What this setting does it, instruct nginx to use the nginx-sticky-module-ng module (<a href="https://bitbucket.org/nginx-goodies/nginx-sticky-module-ng">https://bitbucket.org/nginx-goodies/nginx-sticky-module-ng</a>) that‚Äôs bundled with the controller to handle all sticky sessions for us.</p>

<pre><code class="language-bash">kubectl apply -f sticky-ingress.yaml  
</code></pre>

<p>There is a very useful tool called kubetail that you can use to tail the logs of a pod and verify the  sticky session behaviour. To install kubetail check out <a href="https://github.com/johanhaleby/kubetail">https://github.com/johanhaleby/kubetail</a></p>

<p>Now in one terminal window, we can tail the logs </p>

<pre><code class="language-bash">kubetail -l app=echoheaders  
</code></pre>

<p>and in another send in multiple requests to the virtual host stickyingress.example.com</p>

<pre><code class="language-bash">curl -D cookies.txt $ELB/foo -H 'Host: stickyingress.example.com'


while true; do sleep 1;curl -b cookies.txt $ELB/foo -H 'Host: stickyingress.example.com';done
</code></pre>

<p>When the backend server is removed, the requests are then re-routed to another upstream server and NGINX creates a new cookie, as the previous hash became invalid.</p>

<p>As, you can see, requests are sent to the same pod for every subsequent request.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/10/stickySession.gif" alt=""></p>

<h3 id="proxyprotocol">Proxy protocol</h3>

<p>Lots of times you need to pass a user‚Äôs IP address / hostname through to your application. A example would be, to have the hostname of the user in your application logs.</p>

<p>To enable passing along the hostname, enable the below annotation</p>

<pre><code class="language-bash"> service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'
</code></pre>

<p><strong>Update 1(7/10/2017) It looks like this is not needed anymore</strong></p>

<p>For more see <a href="https://github.com/kubernetes/ingress-nginx#source-ip-address">https://github.com/kubernetes/ingress-nginx#source-ip-address</a></p>

<p>To conclude, i have showcased above a subset of features for ingress. Others include path-rewrite, TLS termination, path routing, scaling, rbac, auth and prometheus metrics. For more info check out resources below.</p>

<h2 id="resources">Resources</h2>

<p>For more information visit:</p>

<p>Github project : <a href="https://github.com/shavo007/k8s-ingress">https://github.com/shavo007/k8s-ingress</a></p>

<p>Kubernetes nginx ingress: <a href="https://github.com/kubernetes/ingress-nginx">https://github.com/kubernetes/ingress-nginx</a></p>

<p>External DNS: <a href="https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/nginx-ingress.md">https://github.com/kubernetes-incubator/external-dns/blob/master/docs/tutorials/nginx-ingress.md</a></p>

<p>Kubernetes faqs: <br>
<a href="https://github.com/hubt/kubernetes-faq/blob/master/README.md#kubernetes-faq">https://github.com/hubt/kubernetes-faq/blob/master/README.md#kubernetes-faq</a></p>

<p>Alpine-kops: <a href="https://store.docker.com/community/images/shanelee007/alpine-kops">https://store.docker.com/community/images/shanelee007/alpine-kops</a></p>]]></content:encoded></item><item><title><![CDATA[How to save costs with serverless and AWS Lambda]]></title><description><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Recently AWS <a href="https://aws.amazon.com/about-aws/whats-new/2017/06/amazon-rds-supports-stopping-and-starting-of-database-instances/">announced</a> new feature to stop/start your RDS instances. This is something I have been looking forward to for a long while..</p>

<h2 id="awsrdsstopstart">AWS RDS stop/start</h2>

<p><img src="http://blog.shanelee.name/content/images/2017/07/rds.png" alt="">
Companies I have worked for in the past have scaled down their RDS instances for lower end envs to save</p>]]></description><link>http://blog.shanelee.name/2017/07/28/how-to-save-costs-with-serverless-and-aws-lambda/</link><guid isPermaLink="false">cd5587b1-d0bc-45cb-bda4-5436ed764cd8</guid><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[rds]]></category><category><![CDATA[serverless]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Fri, 28 Jul 2017 03:11:00 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Recently AWS <a href="https://aws.amazon.com/about-aws/whats-new/2017/06/amazon-rds-supports-stopping-and-starting-of-database-instances/">announced</a> new feature to stop/start your RDS instances. This is something I have been looking forward to for a long while..</p>

<h2 id="awsrdsstopstart">AWS RDS stop/start</h2>

<p><img src="http://blog.shanelee.name/content/images/2017/07/rds.png" alt="">
Companies I have worked for in the past have scaled down their RDS instances for lower end envs to save costs. Now you can schedule to stop/start, sweet!</p>

<blockquote>
  <p>The stop/start feature is available for database instances running in a Single-AZ deployment</p>
</blockquote>

<p>This may not suit your needs but if you have multiple RDS instances running in lower end envs in a single A-Z; this potentially could save you $100s if not $1000s per annum..</p>

<h2 id="rdspricingexample">RDS Pricing example</h2>

<p>Lets take a medium sized RDS instance <mark>db.m4.xlarge    (4CPU and 16GB RAM)</mark> in <strong>Sydney</strong> region for <strong>MySQL</strong> engine costs <strong>$0.492</strong> per hour</p>

<p>Now if we use the <a href="https://calculator.s3.amazonaws.com/index.html">calculator</a>, <br>
usage running 24/7, costs on average <strong>$360.15</strong> per month</p>

<p>Now if we only had that running business hours <strong>(9 hrs a day MON-FRI)</strong> , talking on average <strong>$92.99</strong>.</p>

<p>Thats a cost saving of <strong>$267.16</strong>. That is just for <strong>one</strong> <strong>instance</strong> <strong>per</strong> <strong>month</strong>/ OR <strong>$3,205.92 per annum</strong>!!!!! </p>

<p>What if you were working in a large enterprise with 10s or 100s of RDS instances... üòâ</p>

<p><img src="http://blog.shanelee.name/content/images/2017/07/neat.gif" alt=""></p>

<p>So now that we know how much you can save, how to automate the process of stopping/starting instances and run it at low cost.</p>

<p>Well, the answer is serverless!</p>

<h2 id="serverlessandawslambda">Serverless and AWS Lambda</h2>

<p><img src="http://blog.shanelee.name/content/images/2017/07/lambda1.png" alt=""></p>

<p>Over the past few years serverless (or FaaS - Function as a Service) computing has became more and more popular.</p>

<blockquote>
  <p>Run code without thinking about servers.
  Pay for only the compute time you consume.</p>
</blockquote>

<p><mark>In the past years, most of the cloud providers started to offer their own version of serverless: Microsoft launched Azure Functions while Google launched Cloud Functions. IBM released an open-source version of serverless, called OpenWhisk.</mark></p>

<h3 id="callinganewlambdafunctionforthefirsttime">Calling a new Lambda function for the first time</h3>

<p>When you deploy your Lambda function (or update an existing one), a new container will be created for it.</p>

<p>Your code will be moved into the container, and the initialization code will run before the first request arrives to the exposed handler function.</p>

<h4 id="consequentcallstoanexistinglambdafunction">Consequent calls to an existing Lambda function</h4>

<p>For the next calls, Lambda may decide to create new containers to serve your requests. In this case, the same process will happen as described above, with initialization.</p>

<p>However, if you have not changed your Lambda function and only a little time passed since the last call, Lambda may reuse the container. This way it saves the initialization time required to spin up the new container and your code inside it.</p>

<h4 id="benefits">Benefits</h4>

<p>AWS lambda has a number of benefits including:</p>

<ul>
<li>Reduced operational cost</li>
<li>Reduced scaling cost</li>
<li>Easier operational management</li>
<li>Cheap! 1,000,000 free requests per month.
Up to 3.2 million seconds of compute time per month</li>
</ul>

<p>To save all the headaches of building, zipping and deploying your function I will use serverless framework. </p>

<h2 id="serverlessframework">Serverless framework</h2>

<p><img src="http://blog.shanelee.name/content/images/2017/07/serverless.png" alt=""></p>

<blockquote>
  <p>Build auto-scaling, pay-per-execution, event-driven apps on AWS Lambda and more..</p>
</blockquote>

<p>The Serverless framework is an open-source, MIT-licensed solution which helps with creating and managing AWS Lambda functions easier.</p>

<p>Now there is other options such as <a href="https://github.com/awslabs/serverless-application-model">AWS SAM</a> but serverless is quite mature and releases often.</p>

<p>In my previous <a href="https://blog.shanelee.name/2016/12/17/serverless-and-scheduled-lambda-function/">post</a> I talked about serverless and how to install and configure it locally.</p>

<h3 id="codingtime">Coding time..</h3>

<p>Here I am going to show you how to use serverless to schedule two functions for stopping and starting RDS instances.</p>

<p>The src code for both functions exist on github:</p>

<ul>
<li><a href="https://github.com/shavo007/lambda-start-rds">https://github.com/shavo007/lambda-start-rds</a></li>
<li><a href="https://github.com/shavo007/lambda-stop-rds">https://github.com/shavo007/lambda-stop-rds</a></li>
</ul>

<p>Both lambda functions are developed using nodeJS. I decided to use <strong>yarn</strong> rather than npm. </p>

<blockquote>
  <p><a href="https://yarnpkg.com/">Yarn</a> is a Node.js package manager which is much faster than NPM, has offline support, and fetches dependencies more <a href="https://yarnpkg.com/en/docs/yarn-lock">predictably</a>.</p>
</blockquote>

<p>I use some <a href="http://es6-features.org/">ES6</a> syntax, which is a great improvement over the "old" ES5 syntax. There are too many ES6 features to list them here but typical ES6 code uses classes with <code>class, const and let, template strings, and arrow functions ((text) =&gt; { console.log(text) })</code>.</p>

<p>As of now, the current runtime supported by AWS Lambda is <strong>node 6.10</strong></p>

<h4 id="babel">Babel</h4>

<p><a href="https://babeljs.io/">Babel</a> is a compiler that transforms ES6 code into ES5 code. It is very modular and can be used in tons of different environments.</p>

<h4 id="eslint">ESLint</h4>

<p><a href="http://eslint.org/">ESLint</a> is the linter of choice for ES6 code. A linter gives you recommendations about code formatting, which enforces style consistency in your code, and code you share with your team. It's also a great way to learn about JavaScript by making mistakes that ESLint will catch.</p>

<p>Instead of configuring the rules we want for our code ourselves, I use the config created by <a href="https://www.npmjs.com/package/eslint-config-airbnb">Airbnb</a>.</p>

<h4 id="atom">Atom</h4>

<p>You can use any IDE of your choice. I use <a href="https://atom.io/">Atom</a> and it has many plugins such as support for <a href="https://atom.io/packages/linter-eslint">eslint</a></p>

<h2 id="functions">Functions</h2>

<p>Both functions are triggered by a cron job. You can view this config in the respective <code>serverless.yaml</code> file</p>

<p>I create my RDS instances with custom tags. <code>autoStopInstance</code> and <code>autoStartInstance</code>.</p>

<p>The start function runs at <strong>9PM SUN-THU (UTC time)</strong>  and starts any instances with <code>autoStartInstance=true</code></p>

<p>The stop function runs at  <strong>8AM MON-FRI (UTC time)</strong> and stops any instances with <code>autoStopInstance=true</code></p>

<p><mark><strong>NB:</strong> I am running both lambda fns in Sydney</mark></p>

<h2 id="readmore">Read more</h2>

<p>Check out the following extra resources:</p>

<ul>
<li><p><a href="https://martinfowler.com/articles/serverless.html">Serverless Architectures by Martin Fowler</a></p></li>
<li><p><a href="https://serverless.com/blog/">The Serverless blog</a></p></li>
<li><a href="https://github.com/dwyl/learn-aws-lambda">Learn AWS Lambda</a></li>
</ul>

<p>If you have any questions, let me know in the comments below!</p>]]></content:encoded></item><item><title><![CDATA[JVM Microservice with spring boot, docker and kubernetes]]></title><description><![CDATA[How to build and deploy a spring boot application using docker and kubernetes.]]></description><link>http://blog.shanelee.name/2017/07/15/jvm-microservice-with-spring-boot-docker-and-kubernetes/</link><guid isPermaLink="false">357eedf7-58d4-48b6-ab00-a8013d81dce5</guid><category><![CDATA[docker]]></category><category><![CDATA[spring-boot]]></category><category><![CDATA[kubernetes]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 15 Jul 2017 04:10:22 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>That title is a bit of a mouthful...</p>

<p>Over the last two weeks I have been playing with kubernetes. I have extensive experience building microservices and below I will demonstrate how to build a microservice, contain it using docker and deploy on kubernetes.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/07/kubernetes.png" alt="Kubernetes"></p>

<h2 id="creatingamicroserviceprojectusingspringboot">Creating a microservice project using spring boot</h2>

<p>Spring boot allows a developer to build a production-grade stand-alone application, like a typical CRUD application exposing a RESTful API, with minimal configuration, reducing the learning curve required for using the Spring Framework drastically. </p>

<blockquote>
  <p>Spring Boot favors convention over configuration and is designed to get you up and running as quickly as possible.</p>
</blockquote>

<h3 id="createspringbootapplication">Create spring boot application</h3>

<p>To create your spring boot app we will use  <a href="https://start.spring.io/">Spring Initializr</a> web page and generate a Gradle Project with the pre-selected Spring Boot Version. <br>
We define <strong>name.shanelee</strong> as Group (if applicable) and define the artifact name. From here you can choose whatever dependencies you need for your microservice. We use <strong>Web</strong> for supporting tomcat and restful API. <br>
 <strong>Actuator</strong> dependency which implements some production-grade features useful for monitoring and managing our application like health-checks and HTTP requests traces.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/07/Screen-Shot-2017-07-15-at-2-25-28-pm.png" alt="Spring boot"></p>

<p>Spring Initializr has already created everything for us. We just need to have a Java JDK 1.8 or later installed on our machine and the JAVA_HOME environment variable set accordingly.</p>

<pre><code class="language-bash">### Extracting and launching the application
shanelee at shanes-MacBook-Air in ~/java-projects  
$ unzip   ~/Downloads/demo.zip -d microservice
$ cd microservice/demo/
$ ./gradlew bootRun
</code></pre>

<p><strong>The application is up and running and we did not write one line of code!</strong></p>

<p>Spring Boot is opinionated and auto-configures the application with sane default values and beans. It also scans the classpath for known dependencies and initializes them. In our case, we immediately enjoy all the production-grade services offered by <a href="http://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html">Spring Actuator</a>.</p>

<pre><code class="language-bash">~$ curl http://localhost:8080/health
{"status":"UP","diskSpace":{"status":"UP","total":981190307840,"free":744776503296,"threshold":10485760}}
</code></pre>

<p><strong>NB: Actuator endpoints is important when we deploy the container in kubernetes. It needs to know when the microservice is ready to handle network traffic.</strong></p>

<p>For more information see <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</a></p>

<h2 id="packagingaspringbootapplicationasadockercontainer">Packaging a Spring Boot application as a Docker container</h2>

<p>Let's start by creating the Dockerfile in the root directory of our project.</p>

<pre><code class="language-Docker">FROM openjdk:8u131-jdk-alpine  
VOLUME /tmp  
WORKDIR /app  
COPY ./build/libs/demo-0.0.1-SNAPSHOT.jar .  
ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","/app/demo-0.0.1-SNAPSHOT.jar"]  
</code></pre>

<p>The FROM keyword defines the base Docker image of our container. We chose <a href="http://openjdk.java.net/">OpenJDK</a> installed on <a href="https://alpinelinux.org/">Alpine Linux</a> which is a lightweight Linux distribution. To understand why I use alpine as base image check out these <a href="https://diveintodocker.com/blog/the-3-biggest-wins-when-using-alpine-as-a-base-docker-image">benefits</a></p>

<p>The <strong>VOLUME</strong> instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from the native host or other containers. <strong>ENTRYPOINT</strong> defines the command to execute when the container is started. Since Spring Boot produces an executable JAR with embedded Tomcat, the command to execute is simply <code>java -jar microservice.jar</code>. The additional flag <code>java.security.edg=file:/dev/./urandom</code> is used to speed up the application start-up and avoid possible freezes. By default, Java uses <code>/dev/random</code> to seed its SecureRandom class which is known to block if its entropy pool is empty.</p>

<h2 id="logging">Logging</h2>

<blockquote>
  <p>Treat logs as event streams</p>
</blockquote>

<p>This is what is recommended by <strong>12factor</strong> principles. </p>

<p>Microservice should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. During local development, the developer will view this stream in the foreground of their terminal to observe the app‚Äôs behavior.</p>

<p>In staging or production deploys, each process‚Äô stream will be captured by the execution environment and routed to one or more final destinations for viewing and long-term archival. <br>
As I will be using <strong>kubernetes</strong>, I will define a daemonset logging shipper called <strong>fluentd</strong>. </p>

<h3 id="daemonsetandfluentd">Daemonset and Fluentd</h3>

<p><img src="http://blog.shanelee.name/content/images/2017/07/kubernetes-elastic-fluentd.png" alt="FluentD"></p>

<p>A <strong>DaemonSet</strong> ensures that a certain pod is scheduled to each kubelet exactly once. The <strong>fluentd</strong> pod mounts the <code>/var/lib/containers/</code> host volume to access the logs of all pods scheduled to that <strong>kubelet</strong></p>

<p>Daemonset for fluentd can be found <a href="https://github.com/fluent/fluentd-kubernetes-daemonset">here</a></p>

<p><strong>Kubernetes</strong> logs the content of the <code>stdout</code> and <code>stderr</code> streams of a pod to a file. It creates one file for each container in a pod. The default location for these files is <code>/var/log/containers</code> . The filename contains the pod name, the namespace of the pod, the container name, and the container id. The file contains one JSON object per line of the two streams stdout and stderr. </p>

<p>Fluentd is a flexible log data collector. It supports various inputs like log files or syslog and supports many outputs like <strong>elasticsearch</strong> or Hadoop. Fluentd converts each log line to an event. Those events can be processed and enriched in the fluentd pipeline.</p>

<h4 id="considerationsforproductiondeployments">Considerations for Production Deployments</h4>

<p>In a production environment you have to implement a log rotation of the stored log data. Since the above fluentd configuration generally will generate one index per day this is easy. <a href="https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html">Elasticsearch Curator</a> is a tool made for exactly this job. <br>
Curator can run as a container similar to one I defined <a href="https://hub.docker.com/r/shanelee007/docker-es-curator-cron/">here</a> also or a scheduled <a href="https://www.elastic.co/blog/serverless-elasticsearch-curator-on-aws-lambda">lambda</a> function. <br>
Logs to <code>stdout</code> have to be in <strong>JSON</strong> format. </p>

<h2 id="kubernetes">Kubernetes</h2>

<blockquote>
  <p>Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.</p>
</blockquote>

<p>I will discuss how to run kubernetes locally using minikube and how to define resource objects for the microservice above. In a later post I will talk about creating cluster on aws using kops.</p>

<h2 id="howtorunkuberneteslocally">How to run kubernetes locally</h2>

<p>To run kubernetes locally you need <a href="https://kubernetes.io/docs/getting-started-guides/minikube/">minikube</a>. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.</p>

<p>To install locally follow the steps <a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">here</a></p>

<p><mark>Increase the storage size when starting</mark></p>

<pre><code class="language-bash">minikube start --disk-size="10g" --memory="4096"  
#Switch to minikube context
kubectl config use-context minikube  
</code></pre>

<p>After cluster created, open the dashboard. Dashboard is an <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons">addon</a> for kubernetes. </p>

<p><code>minikube dashboard</code></p>

<h2 id="createdeploymentandservicefordemomicroservice">Create deployment and service for demo microservice</h2>

<p>To test locally build and tag your docker image <br>
You can point your docker client to the VM's docker daemon by running</p>

<p><code>eval $(minikube docker-env)</code></p>

<pre><code class="language-Docker">docker build -t demo .  
</code></pre>

<p>You should see output like below  </p>

<pre><code class="language-bash">Sending build context to Docker daemon  15.76MB  
Step 1/5 : FROM openjdk:8u131-jdk-alpine  
8u131-jdk-alpine: Pulling from library/openjdk  
88286f41530e: Pull complete  
009f6e766a1b: Pull complete  
86ed68184682: Pull complete  
Digest: sha256:2b1f15e04904dd44a2667a07e34c628ac4b239f92f413b587538f801a0a57c88  
Status: Downloaded newer image for openjdk:8u131-jdk-alpine  
 ---&gt; 478bf389b75b
Step 2/5 : VOLUME /tmp  
 ---&gt; Running in ff8bd4023ec3
 ---&gt; 61232f70a630
Removing intermediate container ff8bd4023ec3  
Step 3/5 : WORKDIR /app  
 ---&gt; 79ea27f4f4ea
Removing intermediate container 01fac4d0f9a3  
Step 4/5 : COPY ./build/libs/demo-0.0.1-SNAPSHOT.jar .  
 ---&gt; f9aa60a3ac4a
Removing intermediate container d90236650b23  
Step 5/5 : ENTRYPOINT java -Djava.security.egd=file:/dev/./urandom -jar /app/demo-0.0.1-SNAPSHOT.jar  
 ---&gt; Running in 7c8a6c01cef0
 ---&gt; abdcba6bf841
Removing intermediate container 7c8a6c01cef0  
Successfully built abdcba6bf841  
Successfully tagged demo:latest
</code></pre>

<h2 id="addons">Addons</h2>

<p>Minikube has a set of built in addons that can be used enabled, disabled, and opened inside of the local k8s environment. </p>

<p>To enable addon for minkube run <code>minikube addons enable &lt;addon&gt;</code></p>

<p>To verify get the list of addons  </p>

<pre><code class="language-bash">$ minikube addons list
- dashboard: enabled
- default-storageclass: enabled
- kube-dns: enabled
- heapster: disabled
- ingress: enabled
- registry: enabled
- registry-creds: disabled
- addon-manager: enabled
</code></pre>

<p>Below is a sample deployment config. Here I defined the service and deployment resource objects. </p>

<pre><code class="language-yaml">apiVersion: v1  
kind: Service  
metadata:  
  name: demo-microservice
  labels:
    app: demo
spec:  
  ports:
    - port: 8081
  selector:
    app: demo
    tier: microservice
  type: LoadBalancer
---

---
apiVersion: extensions/v1beta1  
kind: Deployment  
metadata:  
  name: demo-microservice
  creationTimestamp: null
  labels:
     app: demo
spec:  
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: demo
        tier: microservice
        env: dev
    spec:
      containers:
        - name: demo
          image: demo
          imagePullPolicy: Never
          ports:
          - containerPort: 8081
          env:
            - name: SERVER_PORT
              value: "8081"
</code></pre>

<p>This is the definition of a Kubernetes <a href="http://kubernetes.io/docs/user-guide/deployments/">Deployment</a> named <strong>demo-microservice</strong>. The replicas element defines the target number of Pods. Kubernetes performs automated binpacking and self-healing of the system to comply with the deployment specifications while achieving optimal utilization of compute resources. A Pod can be composed of multiple containers. In this scenario, I included one container: one for demo microservice image.</p>

<p>If using private docker registry  you need to set an entry under the <strong>imagePullSecrets</strong> which is used to authenticate to the docker Registry.</p>

<p><mark>For a detailed explanation of Kubernetes resources and concepts refer to the <a href="http://kubernetes.io/">official documentation</a>.</mark></p>

<h2 id="democreation">Demo creation</h2>

<p>Now create the service and deployment</p>

<pre><code>$ kubectl apply -f deployment.yml --record
service "demo-microservice" configured  
deployment "demo-microservice" created  
</code></pre>

<p>Kubernetes will create one pod with one container inside.</p>

<p>To verify the pod is running  </p>

<pre><code class="language-bash">kubectl get pods  
#To tail the logs of the microservice container run
kubectl logs -f &lt;pod&gt;  
</code></pre>

<p>Now that the container is running in the pod, we can verify the health of the microservice. <br>
The pod is exposed through a service. <br>
To get information about the service run <br>
<code>$ kubectl describe svc demo-microservice</code></p>

<p>To access locally get the public url  </p>

<pre><code class="language-bash">$ minikube service demo-microservice --url
http://192.168.99.100:31511  
</code></pre>

<h3 id="verifyhealth">Verify health</h3>

<p>Then hit the health endpoint to verify the status of the microservice  </p>

<pre><code class="language-bash">$ curl http://192.168.99.100:31511/health
{"status":"UP","diskSpace":{"status":"UP","total":19163156480,"free":9866866688,"threshold":10485760}}
</code></pre>

<h4 id="configurelivenessandreadinessprobes">Configure Liveness and Readiness Probes</h4>

<p>Now that we are happy with the deployment, we are going to add an additional feature. </p>

<p>The <a href="https://kubernetes.io/docs/admin/kubelet/">kubelet</a> uses liveness probes to know when to restart a Container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a Container in such a state can help to make the application more available despite bugs.</p>

<p>The kubelet uses readiness probes to know when a Container is ready to start accepting traffic. A Pod is considered ready when all of its Containers are ready. One use of this signal is to control which Pods are used as backends for Services. When a Pod is not ready, it is removed from Service load balancers.</p>

<p>The right combination of liveness and readiness probes used with Kubernetes deployments can:</p>

<ul>
<li><p>Enable zero downtime deploys</p></li>
<li><p>Prevent deployment of broken images</p></li>
<li><p>Ensure that failed containers are automatically restarted</p></li>
</ul>

<pre><code class="language-yaml"> livenessProbe:
              httpGet:
                path: /health
                port: 8081
                httpHeaders:
                  - name: X-Custom-Header
                    value: Awesome
              initialDelaySeconds: 30
              periodSeconds: 3
</code></pre>

<p>The <strong>livenessProbe</strong> field specifies that the kubelet should perform a liveness probe every 3 seconds for demo. The initialDelaySeconds field tells the kubelet that it should wait 30 seconds before performing the first probe. </p>

<p>To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8081. If the handler for the server‚Äôs /health path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it.</p>

<p>To try the HTTP liveness check, update the deployment</p>

<pre><code class="language-bash">$ kubectl apply -f kubernetes/deployment.yml --record
</code></pre>

<p>If you describe the pod, you will see the liveness http request <br>
 <code>Liveness: http-get http://:8081/health delay=30s timeout=1s period=3s #success=1 #failure=3</code></p>

<p>The readiness probe has similar configuration:  </p>

<pre><code class="language-yaml ">readinessProbe:  
   httpGet:
     path: /health
     port: 8081
   initialDelaySeconds: 30
   periodSeconds: 10
</code></pre>

<p>Readiness and liveness probes can be used in parallel for the same container. Using both can ensure that traffic does not reach a container that is not ready for it, and that containers are restarted when they fail.</p>

<p>To verify these changes, spring boot actuator has a production ready endpoint called trace.</p>

<blockquote>
  <p>Displays trace information (by default the last 100 HTTP requests).</p>
</blockquote>

<p>If you access this endpoint, you will see the health requests like below <br>
<a href="http://192.168.99.100:31511/trace">http://192.168.99.100:31511/trace</a></p>

<pre><code class="language-bash">{timestamp: 1499927068835,info: {method: "GET",path: "/health",headers: {request: {host: "172.17.0.3:8081",user-agent: "Go-http-client/1.1",x-custom-header: "Awesome",accept-encoding: "gzip",connection: "close"},response: {X-Application-Context: "application:local:8081",Content-Type: "application/vnd.spring-boot.actuator.v1+json;charset=UTF-8",Transfer-Encoding: "chunked",Date: "Thu, 13 Jul 2017 06:24:28 GMT",Connection: "close",status: "200"}},timeTaken: "4"}},
</code></pre>

<p>The actuator <a href="https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html">endpoints</a> provide a wealth of information for your microservice. Make sure you become accustomed to them. </p>

<p>There you have it!</p>

<p>A working example of using spring boot, docker and kubernetes.</p>

<p>Stay tuned for more kubernetes goodness... ;-)</p>

<p>If you want to view the sample code check out github repo <a href="https://github.com/shavo007/spring-boot-k8s">here</a></p>]]></content:encoded></item><item><title><![CDATA[Test drive docker health check]]></title><description><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Healthcheck in docker was introduced in docker compose version 2.1 and up.</p>

<p>In version <a href="https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/">1.12</a>, Docker added the ability to perform health checks directly in the Docker engine ‚Äî without needing external monitoring tools or sidecar containers.</p>

<blockquote>
  <p>Configure a check that‚Äôs run to determine whether or</p></blockquote>]]></description><link>http://blog.shanelee.name/2017/06/12/test-drive-docker-health-check/</link><guid isPermaLink="false">92dff96b-d487-442b-98bb-b77722028f90</guid><category><![CDATA[pact]]></category><category><![CDATA[docker]]></category><category><![CDATA[docker-compose]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Mon, 12 Jun 2017 05:06:53 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Healthcheck in docker was introduced in docker compose version 2.1 and up.</p>

<p>In version <a href="https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/">1.12</a>, Docker added the ability to perform health checks directly in the Docker engine ‚Äî without needing external monitoring tools or sidecar containers.</p>

<blockquote>
  <p>Configure a check that‚Äôs run to determine whether or not containers for this service are ‚Äúhealthy‚Äù</p>
</blockquote>

<p>This is a great addition because a container reporting status as Up 1 hour may return errors. The container may be up but there is no way for the application inside the container to provide a status.</p>

<h2 id="dockercomposeexample">Docker compose example</h2>

<p>I will guide you through an example of using healthcheck in my pact broker demo.</p>

<p>Github repo can be found at <a href="https://github.com/shavo007/pact-demo">https://github.com/shavo007/pact-demo</a></p>

<p>Pact broker has two containers:</p>

<ul>
<li>Postgres</li>
<li>Pact broker</li>
</ul>

<h3 id="healthcheckoptions">Health check options</h3>

<p>The health check related options are:</p>

<ul>
<li><p><strong>test</strong>: must be either a string or a list. If it‚Äôs a list, the first item must be either NONE, CMD or CMD-SHELL. Health check commands should return 0 if healthy and 1 if unhealthy. </p></li>
<li><p><strong>interval</strong>: this controls the initial delay before the first health check runs and then how often the health check command is executed thereafter. The default is 30 seconds.</p></li>
<li><strong>retries</strong>: the health check will retry up to this many times before marking the container as unhealthy. The default is 3 retries.</li>
<li><strong>timeout</strong>: if the health check command takes longer than this to complete, it will be considered a failure. The default timeout is 30 seconds.</li>
</ul>

<p>Below is the docker compose file</p>

<script src="https://gist.github.com/shavo007/754a23247826a346ca79593bef44c172.js"></script>

<blockquote>
  <p>The exit code has to be binary, which means 0 or 1 - any other value is not supported. The code || exit 1 makes sure we only get a binary exit code and nothing more exotic.</p>
</blockquote>

<h3 id="waitingforpostgresqltobehealthy">Waiting for PostgreSQL to be "healthy"</h3>

<p>A particularly common use case is a service that depends on a database, such as PostgreSQL. We can configure docker-compose to wait for the PostgreSQL container to startup and be ready to accept requests before continuing.</p>

<p>The following healthcheck has been configured to periodically check if PostgreSQL reponds to the \l list query.</p>

<p>Now that we have defined the instructions, lets kick it off:</p>

<pre><code class="language-bash">docker-compose up --build  
</code></pre>

<p>docker-compose waits for the PostgreSQL service to be "healthy" before starting pact broker.</p>

<h3 id="arewehealthythen">Are we healthy then?</h3>

<p>Once you start the container, you will be able to see the health status in the <code>docker ps</code> output.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/06/Screen-Shot-2017-06-12-at-3-03-03-pm.png" alt=""></p>

<p>You can see the health check status of the postgres container is healthy. Pact broker container depends on this container and waits until it is healthy.</p>

<h3 id="howdoweinspectit">How do we inspect it?</h3>

<p>Using  </p>

<pre><code class="language-bash"> docker inspect
</code></pre>

<p>we can view the output from the command.</p>

<pre><code class="language-bash">docker inspect --format "{{json .State.Health.Status }}" pactdemo_postgres_1  
</code></pre>

<pre><code class="language-bash">"healthy"
</code></pre>

<p>You can use <a href="https://stedolan.github.io/jq/">jq</a> if you find the docker inspect command verbose.</p>]]></content:encoded></item><item><title><![CDATA[How To Import and Export Databases in MySQL or MariaDB with Docker]]></title><description><![CDATA[<h1 id="introduction">Introduction</h1>

<p>You can use data dumps for backup and restoration purposes, or you can use them to migrate data to a new server or development environment.</p>

<p>Working with database dumps in MySQL and MariaDB is straightforward. This tutorial will cover how to export the database as well as import it</p>]]></description><link>http://blog.shanelee.name/2017/04/09/how-to-import-and-export-databases-in-mysql-or-mariadb-with-docker/</link><guid isPermaLink="false">293badb0-924e-402f-8236-eee1c16210de</guid><category><![CDATA[mariadb]]></category><category><![CDATA[docker]]></category><category><![CDATA[mysql]]></category><category><![CDATA[sql]]></category><category><![CDATA[dump]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 09 Apr 2017 04:09:25 GMT</pubDate><content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>

<p>You can use data dumps for backup and restoration purposes, or you can use them to migrate data to a new server or development environment.</p>

<p>Working with database dumps in MySQL and MariaDB is straightforward. This tutorial will cover how to export the database as well as import it from a dump file in MySQL and MariaDB.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/04/mariadb.png" alt="MariaDB Icon"></p>

<h1 id="prerequisites">Prerequisites</h1>

<p>To import and/or export a MySQL or MariaDB database, you will need:</p>

<ul>
<li>Access to the Linux server running MySQL or MariaDB</li>
<li>The database name and user credentials for it</li>
</ul>

<h1 id="exportingthedatabase">Exporting the Database</h1>

<p>The mysqldump console utility is used to export databases to SQL text files. These files can easily be transferred and moved around. You will need the database name itself as well as the username and password to an account with privileges allowing at least full read only access to the database.</p>

<p>Export your database using the following command.</p>

<pre><code class="language-bash">mysqldump -u username -p database_name &gt; dump.sql  
</code></pre>

<ul>
<li><p><strong>username</strong> is the username you can log in to the database with</p></li>
<li><p><strong>database_name</strong> is the name of the database that will be exported</p></li>
<li><p><strong>dump.sql</strong> is the file in the current directory that the output will be saved to</p></li>
</ul>

<p>The command will produce no visual output, but you can inspect the contents of sql file to check if it's a legitimate SQL dump file by using:</p>

<pre><code class="language-bash">head -n 5 dump.sql  
</code></pre>

<p>The top of the file should look similar to this, mentioning that it's a mariadb dump for a database named database_name.</p>

<p><strong>SQL dump fragment</strong></p>

<pre><code class="language-bash">-- MySQL dump 10.16  Distrib 10.1.20-MariaDB, for debian-linux-gnu (x86_64)
--
-- Host: localhost    Database: localhost
-- ------------------------------------------------------
-- Server version    10.1.20-MariaDB-1~jessie
</code></pre>

<p>If any errors happen during the export process, mysqldump will print them clearly to the screen instead.</p>

<h1 id="importingthedatabaseintodockercontainer">Importing the Database into docker container</h1>

<p>To import an existing dump file into MySQL or MariaDB, you will have to create the new database. This is where the contents of the dump file will be imported.</p>

<p>I have an existing mariadb docker container running already locally.</p>

<pre><code class="language-bash">docker run --name mariadb -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password1 -e MYSQL_DATABASE=db -d mariadb:latest  
</code></pre>

<p>Now verify it is running</p>

<pre><code class="language-bash">docker ps  
</code></pre>

<p>You will see output similar to below:</p>

<pre><code class="language-bash">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                            NAMES  
7cfaabda3346        mariadb:latest      "docker-entrypoint..."   5 days ago          Up 5 days           0.0.0.0:3306-&gt;3306/tcp                           mariadb  
</code></pre>

<p>First, start a bash session inside container.</p>

<pre><code class="language-bash">docker exec -it mariadb bash  
</code></pre>

<p>Second, log in to the database as root or another user with sufficient privileges to create new databases.</p>

<pre><code class="language-bash">mysql -u root -ppassword1  
</code></pre>

<p>This will bring you into the mariadb shell prompt. Next, create a new database called new_database.</p>

<pre><code class="language-sql">CREATE DATABASE new_database;  
</code></pre>

<p>Now exit the MySQL shell by pressing CTRL+D. Exit the docker container also.On the normal command line, you can import the dump file with the following command:</p>

<pre><code class="language-bash">docker exec -i mariadb mysql -uroot -ppassword1 --database=new_database &lt; dump.sql  
</code></pre>

<ul>
<li>username is the username you can log in to the database with</li>
<li>new_database is the name of the freshly created database</li>
<li>dump.sql is the data dump file to be imported, located in the current directory</li>
</ul>

<p>The successfully run command will produce no output. If any errors occur during the process, mysql will print them to the terminal instead. You can check that the database was imported by logging in to the MySQL shell again and inspecting the data.</p>

<h1 id="conclusion">Conclusion</h1>

<p>You now know how to create database dumps from MySQL databases as well as how to import them again. mysqldump has multiple additional settings that may be used to alter how the dumps are created, which you can learn more about from the <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html">official mysqldump documentation page</a>.</p>

<p>To find out more about docker commands check out the docker doc <a href="https://docs.docker.com/engine/reference/commandline/exec/">here</a></p>]]></content:encoded></item></channel></rss>