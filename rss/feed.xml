<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Tech Blog]]></title><description><![CDATA[Thoughts, stories and ideas.]]></description><link>http://blog.shanelee.name/</link><generator>Ghost 0.8</generator><lastBuildDate>Sat, 22 Jul 2017 06:42:19 GMT</lastBuildDate><atom:link href="http://blog.shanelee.name/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[JVM Microservice with spring boot, docker and kubernetes]]></title><description><![CDATA[How to build and deploy a spring boot application using docker and kubernetes.]]></description><link>http://blog.shanelee.name/2017/07/15/jvm-microservice-with-spring-boot-docker-and-kubernetes/</link><guid isPermaLink="false">357eedf7-58d4-48b6-ab00-a8013d81dce5</guid><category><![CDATA[docker]]></category><category><![CDATA[spring-boot]]></category><category><![CDATA[kubernetes]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 15 Jul 2017 04:10:22 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>That title is a bit of a mouthful...</p>

<p>Over the last two weeks I have been playing with kubernetes. I have extensive experience building microservices and below I will demonstrate how to build a microservice, contain it using docker and deploy on kubernetes.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/07/kubernetes.png" alt="Kubernetes"></p>

<h2 id="creatingamicroserviceprojectusingspringboot">Creating a microservice project using spring boot</h2>

<p>Spring boot allows a developer to build a production-grade stand-alone application, like a typical CRUD application exposing a RESTful API, with minimal configuration, reducing the learning curve required for using the Spring Framework drastically. </p>

<blockquote>
  <p>Spring Boot favors convention over configuration and is designed to get you up and running as quickly as possible.</p>
</blockquote>

<h3 id="createspringbootapplication">Create spring boot application</h3>

<p>To create your spring boot app we will use  <a href="https://start.spring.io/">Spring Initializr</a> web page and generate a Gradle Project with the pre-selected Spring Boot Version. <br>
We define <strong>name.shanelee</strong> as Group (if applicable) and define the artifact name. From here you can choose whatever dependencies you need for your microservice. We use <strong>Web</strong> for supporting tomcat and restful API. <br>
 <strong>Actuator</strong> dependency which implements some production-grade features useful for monitoring and managing our application like health-checks and HTTP requests traces.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/07/Screen-Shot-2017-07-15-at-2-25-28-pm.png" alt="Spring boot"></p>

<p>Spring Initializr has already created everything for us. We just need to have a Java JDK 1.8 or later installed on our machine and the JAVA_HOME environment variable set accordingly.</p>

<pre><code class="language-bash">### Extracting and launching the application
shanelee at shanes-MacBook-Air in ~/java-projects  
$ unzip   ~/Downloads/demo.zip -d microservice
$ cd microservice/demo/
$ ./gradlew bootRun
</code></pre>

<p><strong>The application is up and running and we did not write one line of code!</strong></p>

<p>Spring Boot is opinionated and auto-configures the application with sane default values and beans. It also scans the classpath for known dependencies and initializes them. In our case, we immediately enjoy all the production-grade services offered by <a href="http://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html">Spring Actuator</a>.</p>

<pre><code class="language-bash">~$ curl http://localhost:8080/health
{"status":"UP","diskSpace":{"status":"UP","total":981190307840,"free":744776503296,"threshold":10485760}}
</code></pre>

<p><strong>NB: Actuator endpoints is important when we deploy the container in kubernetes. It needs to know when the microservice is ready to handle network traffic.</strong></p>

<p>For more information see <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/</a></p>

<h2 id="packagingaspringbootapplicationasadockercontainer">Packaging a Spring Boot application as a Docker container</h2>

<p>Let's start by creating the Dockerfile in the root directory of our project.</p>

<pre><code class="language-Docker">FROM openjdk:8u131-jdk-alpine  
VOLUME /tmp  
WORKDIR /app  
COPY ./build/libs/demo-0.0.1-SNAPSHOT.jar .  
ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom","-jar","/app/demo-0.0.1-SNAPSHOT.jar"]  
</code></pre>

<p>The FROM keyword defines the base Docker image of our container. We chose <a href="http://openjdk.java.net/">OpenJDK</a> installed on <a href="https://alpinelinux.org/">Alpine Linux</a> which is a lightweight Linux distribution. To understand why I use alpine as base image check out these <a href="https://diveintodocker.com/blog/the-3-biggest-wins-when-using-alpine-as-a-base-docker-image">benefits</a></p>

<p>The <strong>VOLUME</strong> instruction creates a mount point with the specified name and marks it as holding externally mounted volumes from the native host or other containers. <strong>ENTRYPOINT</strong> defines the command to execute when the container is started. Since Spring Boot produces an executable JAR with embedded Tomcat, the command to execute is simply <code>java -jar microservice.jar</code>. The additional flag <code>java.security.edg=file:/dev/./urandom</code> is used to speed up the application start-up and avoid possible freezes. By default, Java uses <code>/dev/random</code> to seed its SecureRandom class which is known to block if its entropy pool is empty.</p>

<h2 id="logging">Logging</h2>

<blockquote>
  <p>Treat logs as event streams</p>
</blockquote>

<p>This is what is recommended by <strong>12factor</strong> principles. </p>

<p>Microservice should not attempt to write to or manage logfiles. Instead, each running process writes its event stream, unbuffered, to stdout. During local development, the developer will view this stream in the foreground of their terminal to observe the app’s behavior.</p>

<p>In staging or production deploys, each process’ stream will be captured by the execution environment and routed to one or more final destinations for viewing and long-term archival. <br>
As I will be using <strong>kubernetes</strong>, I will define a daemonset logging shipper called <strong>fluentd</strong>. </p>

<h3 id="daemonsetandfluentd">Daemonset and Fluentd</h3>

<p><img src="http://blog.shanelee.name/content/images/2017/07/kubernetes-elastic-fluentd.png" alt="FluentD"></p>

<p>A <strong>DaemonSet</strong> ensures that a certain pod is scheduled to each kubelet exactly once. The <strong>fluentd</strong> pod mounts the <code>/var/lib/containers/</code> host volume to access the logs of all pods scheduled to that <strong>kubelet</strong></p>

<p>Daemonset for fluentd can be found <a href="https://github.com/fluent/fluentd-kubernetes-daemonset">here</a></p>

<p><strong>Kubernetes</strong> logs the content of the <code>stdout</code> and <code>stderr</code> streams of a pod to a file. It creates one file for each container in a pod. The default location for these files is <code>/var/log/containers</code> . The filename contains the pod name, the namespace of the pod, the container name, and the container id. The file contains one JSON object per line of the two streams stdout and stderr. </p>

<p>Fluentd is a flexible log data collector. It supports various inputs like log files or syslog and supports many outputs like <strong>elasticsearch</strong> or Hadoop. Fluentd converts each log line to an event. Those events can be processed and enriched in the fluentd pipeline.</p>

<h4 id="considerationsforproductiondeployments">Considerations for Production Deployments</h4>

<p>In a production environment you have to implement a log rotation of the stored log data. Since the above fluentd configuration generally will generate one index per day this is easy. <a href="https://www.elastic.co/guide/en/elasticsearch/client/curator/current/index.html">Elasticsearch Curator</a> is a tool made for exactly this job. <br>
Curator can run as a container similar to one I defined <a href="https://hub.docker.com/r/shanelee007/docker-es-curator-cron/">here</a> also or a scheduled <a href="https://www.elastic.co/blog/serverless-elasticsearch-curator-on-aws-lambda">lambda</a> function. <br>
Logs to <code>stdout</code> have to be in <strong>JSON</strong> format. </p>

<h2 id="kubernetes">Kubernetes</h2>

<blockquote>
  <p>Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications.</p>
</blockquote>

<p>I will discuss how to run kubernetes locally using minikube and how to define resource objects for the microservice above. In a later post I will talk about creating cluster on aws using kops.</p>

<h2 id="howtorunkuberneteslocally">How to run kubernetes locally</h2>

<p>To run kubernetes locally you need <a href="https://kubernetes.io/docs/getting-started-guides/minikube/">minikube</a>. Minikube runs a single-node Kubernetes cluster inside a VM on your laptop for users looking to try out Kubernetes or develop with it day-to-day.</p>

<p>To install locally follow the steps <a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">here</a></p>

<p><mark>Increase the storage size when starting</mark></p>

<pre><code class="language-bash">minikube start --disk-size="10g" --memory="4096"  
#Switch to minikube context
kubectl config use-context minikube  
</code></pre>

<p>After cluster created, open the dashboard. Dashboard is an <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons">addon</a> for kubernetes. </p>

<p><code>minikube dashboard</code></p>

<h2 id="createdeploymentandservicefordemomicroservice">Create deployment and service for demo microservice</h2>

<p>To test locally build and tag your docker image <br>
You can point your docker client to the VM's docker daemon by running</p>

<p><code>eval $(minikube docker-env)</code></p>

<pre><code class="language-Docker">docker build -t demo .  
</code></pre>

<p>You should see output like below  </p>

<pre><code class="language-bash">Sending build context to Docker daemon  15.76MB  
Step 1/5 : FROM openjdk:8u131-jdk-alpine  
8u131-jdk-alpine: Pulling from library/openjdk  
88286f41530e: Pull complete  
009f6e766a1b: Pull complete  
86ed68184682: Pull complete  
Digest: sha256:2b1f15e04904dd44a2667a07e34c628ac4b239f92f413b587538f801a0a57c88  
Status: Downloaded newer image for openjdk:8u131-jdk-alpine  
 ---&gt; 478bf389b75b
Step 2/5 : VOLUME /tmp  
 ---&gt; Running in ff8bd4023ec3
 ---&gt; 61232f70a630
Removing intermediate container ff8bd4023ec3  
Step 3/5 : WORKDIR /app  
 ---&gt; 79ea27f4f4ea
Removing intermediate container 01fac4d0f9a3  
Step 4/5 : COPY ./build/libs/demo-0.0.1-SNAPSHOT.jar .  
 ---&gt; f9aa60a3ac4a
Removing intermediate container d90236650b23  
Step 5/5 : ENTRYPOINT java -Djava.security.egd=file:/dev/./urandom -jar /app/demo-0.0.1-SNAPSHOT.jar  
 ---&gt; Running in 7c8a6c01cef0
 ---&gt; abdcba6bf841
Removing intermediate container 7c8a6c01cef0  
Successfully built abdcba6bf841  
Successfully tagged demo:latest
</code></pre>

<h2 id="addons">Addons</h2>

<p>Minikube has a set of built in addons that can be used enabled, disabled, and opened inside of the local k8s environment. </p>

<p>To enable addon for minkube run <code>minikube addons enable &lt;addon&gt;</code></p>

<p>To verify get the list of addons  </p>

<pre><code class="language-bash">$ minikube addons list
- dashboard: enabled
- default-storageclass: enabled
- kube-dns: enabled
- heapster: disabled
- ingress: enabled
- registry: enabled
- registry-creds: disabled
- addon-manager: enabled
</code></pre>

<p>Below is a sample deployment config. Here I defined the service and deployment resource objects. </p>

<pre><code class="language-yaml">apiVersion: v1  
kind: Service  
metadata:  
  name: demo-microservice
  labels:
    app: demo
spec:  
  ports:
    - port: 8081
  selector:
    app: demo
    tier: microservice
  type: LoadBalancer
---

---
apiVersion: extensions/v1beta1  
kind: Deployment  
metadata:  
  name: demo-microservice
  creationTimestamp: null
  labels:
     app: demo
spec:  
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: demo
        tier: microservice
        env: dev
    spec:
      containers:
        - name: demo
          image: demo
          imagePullPolicy: Never
          ports:
          - containerPort: 8081
          env:
            - name: SERVER_PORT
              value: "8081"
</code></pre>

<p>This is the definition of a Kubernetes <a href="http://kubernetes.io/docs/user-guide/deployments/">Deployment</a> named <strong>demo-microservice</strong>. The replicas element defines the target number of Pods. Kubernetes performs automated binpacking and self-healing of the system to comply with the deployment specifications while achieving optimal utilization of compute resources. A Pod can be composed of multiple containers. In this scenario, I included one container: one for demo microservice image.</p>

<p>If using private docker registry  you need to set an entry under the <strong>imagePullSecrets</strong> which is used to authenticate to the docker Registry.</p>

<p><mark>For a detailed explanation of Kubernetes resources and concepts refer to the <a href="http://kubernetes.io/">official documentation</a>.</mark></p>

<h2 id="democreation">Demo creation</h2>

<p>Now create the service and deployment</p>

<pre><code>$ kubectl apply -f deployment.yml --record
service "demo-microservice" configured  
deployment "demo-microservice" created  
</code></pre>

<p>Kubernetes will create one pod with one container inside.</p>

<p>To verify the pod is running  </p>

<pre><code class="language-bash">kubectl get pods  
#To tail the logs of the microservice container run
kubectl logs -f &lt;pod&gt;  
</code></pre>

<p>Now that the container is running in the pod, we can verify the health of the microservice. <br>
The pod is exposed through a service. <br>
To get information about the service run <br>
<code>$ kubectl describe svc demo-microservice</code></p>

<p>To access locally get the public url  </p>

<pre><code class="language-bash">$ minikube service demo-microservice --url
http://192.168.99.100:31511  
</code></pre>

<h3 id="verifyhealth">Verify health</h3>

<p>Then hit the health endpoint to verify the status of the microservice  </p>

<pre><code class="language-bash">$ curl http://192.168.99.100:31511/health
{"status":"UP","diskSpace":{"status":"UP","total":19163156480,"free":9866866688,"threshold":10485760}}
</code></pre>

<h4 id="configurelivenessandreadinessprobes">Configure Liveness and Readiness Probes</h4>

<p>Now that we are happy with the deployment, we are going to add an additional feature. </p>

<p>The <a href="https://kubernetes.io/docs/admin/kubelet/">kubelet</a> uses liveness probes to know when to restart a Container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a Container in such a state can help to make the application more available despite bugs.</p>

<p>The kubelet uses readiness probes to know when a Container is ready to start accepting traffic. A Pod is considered ready when all of its Containers are ready. One use of this signal is to control which Pods are used as backends for Services. When a Pod is not ready, it is removed from Service load balancers.</p>

<p>The right combination of liveness and readiness probes used with Kubernetes deployments can:</p>

<ul>
<li><p>Enable zero downtime deploys</p></li>
<li><p>Prevent deployment of broken images</p></li>
<li><p>Ensure that failed containers are automatically restarted</p></li>
</ul>

<pre><code class="language-yaml"> livenessProbe:
              httpGet:
                path: /health
                port: 8081
                httpHeaders:
                  - name: X-Custom-Header
                    value: Awesome
              initialDelaySeconds: 30
              periodSeconds: 3
</code></pre>

<p>The <strong>livenessProbe</strong> field specifies that the kubelet should perform a liveness probe every 3 seconds for demo. The initialDelaySeconds field tells the kubelet that it should wait 30 seconds before performing the first probe. </p>

<p>To perform a probe, the kubelet sends an HTTP GET request to the server that is running in the Container and listening on port 8081. If the handler for the server’s /health path returns a success code, the kubelet considers the Container to be alive and healthy. If the handler returns a failure code, the kubelet kills the Container and restarts it.</p>

<p>To try the HTTP liveness check, update the deployment</p>

<pre><code class="language-bash">$ kubectl apply -f kubernetes/deployment.yml --record
</code></pre>

<p>If you describe the pod, you will see the liveness http request <br>
 <code>Liveness: http-get http://:8081/health delay=30s timeout=1s period=3s #success=1 #failure=3</code></p>

<p>The readiness probe has similar configuration:  </p>

<pre><code class="language-yaml ">readinessProbe:  
   httpGet:
     path: /health
     port: 8081
   initialDelaySeconds: 30
   periodSeconds: 10
</code></pre>

<p>Readiness and liveness probes can be used in parallel for the same container. Using both can ensure that traffic does not reach a container that is not ready for it, and that containers are restarted when they fail.</p>

<p>To verify these changes, spring boot actuator has a production ready endpoint called trace.</p>

<blockquote>
  <p>Displays trace information (by default the last 100 HTTP requests).</p>
</blockquote>

<p>If you access this endpoint, you will see the health requests like below <br>
<a href="http://192.168.99.100:31511/trace">http://192.168.99.100:31511/trace</a></p>

<pre><code class="language-bash">{timestamp: 1499927068835,info: {method: "GET",path: "/health",headers: {request: {host: "172.17.0.3:8081",user-agent: "Go-http-client/1.1",x-custom-header: "Awesome",accept-encoding: "gzip",connection: "close"},response: {X-Application-Context: "application:local:8081",Content-Type: "application/vnd.spring-boot.actuator.v1+json;charset=UTF-8",Transfer-Encoding: "chunked",Date: "Thu, 13 Jul 2017 06:24:28 GMT",Connection: "close",status: "200"}},timeTaken: "4"}},
</code></pre>

<p>The actuator <a href="https://docs.spring.io/spring-boot/docs/current/reference/html/production-ready-endpoints.html">endpoints</a> provide a wealth of information for your microservice. Make sure you become accustomed to them. </p>

<p>There you have it!</p>

<p>A working example of using spring boot, docker and kubernetes.</p>

<p>Stay tuned for more kubernetes goodness... ;-)</p>

<p>If you want to view the sample code check out github repo <a href="https://github.com/shavo007/spring-boot-k8s">here</a></p>]]></content:encoded></item><item><title><![CDATA[Test drive docker health check]]></title><description><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Healthcheck in docker was introduced in docker compose version 2.1 and up.</p>

<p>In version <a href="https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/">1.12</a>, Docker added the ability to perform health checks directly in the Docker engine — without needing external monitoring tools or sidecar containers.</p>

<blockquote>
  <p>Configure a check that’s run to determine whether or</p></blockquote>]]></description><link>http://blog.shanelee.name/2017/06/12/test-drive-docker-health-check/</link><guid isPermaLink="false">92dff96b-d487-442b-98bb-b77722028f90</guid><category><![CDATA[pact]]></category><category><![CDATA[docker]]></category><category><![CDATA[docker-compose]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Mon, 12 Jun 2017 05:06:53 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Healthcheck in docker was introduced in docker compose version 2.1 and up.</p>

<p>In version <a href="https://blog.docker.com/2016/06/docker-1-12-built-in-orchestration/">1.12</a>, Docker added the ability to perform health checks directly in the Docker engine — without needing external monitoring tools or sidecar containers.</p>

<blockquote>
  <p>Configure a check that’s run to determine whether or not containers for this service are “healthy”</p>
</blockquote>

<p>This is a great addition because a container reporting status as Up 1 hour may return errors. The container may be up but there is no way for the application inside the container to provide a status.</p>

<h2 id="dockercomposeexample">Docker compose example</h2>

<p>I will guide you through an example of using healthcheck in my pact broker demo.</p>

<p>Github repo can be found at <a href="https://github.com/shavo007/pact-demo">https://github.com/shavo007/pact-demo</a></p>

<p>Pact broker has two containers:</p>

<ul>
<li>Postgres</li>
<li>Pact broker</li>
</ul>

<h3 id="healthcheckoptions">Health check options</h3>

<p>The health check related options are:</p>

<ul>
<li><p><strong>test</strong>: must be either a string or a list. If it’s a list, the first item must be either NONE, CMD or CMD-SHELL. Health check commands should return 0 if healthy and 1 if unhealthy. </p></li>
<li><p><strong>interval</strong>: this controls the initial delay before the first health check runs and then how often the health check command is executed thereafter. The default is 30 seconds.</p></li>
<li><strong>retries</strong>: the health check will retry up to this many times before marking the container as unhealthy. The default is 3 retries.</li>
<li><strong>timeout</strong>: if the health check command takes longer than this to complete, it will be considered a failure. The default timeout is 30 seconds.</li>
</ul>

<p>Below is the docker compose file</p>

<script src="https://gist.github.com/shavo007/754a23247826a346ca79593bef44c172.js"></script>

<blockquote>
  <p>The exit code has to be binary, which means 0 or 1 - any other value is not supported. The code || exit 1 makes sure we only get a binary exit code and nothing more exotic.</p>
</blockquote>

<h3 id="waitingforpostgresqltobehealthy">Waiting for PostgreSQL to be "healthy"</h3>

<p>A particularly common use case is a service that depends on a database, such as PostgreSQL. We can configure docker-compose to wait for the PostgreSQL container to startup and be ready to accept requests before continuing.</p>

<p>The following healthcheck has been configured to periodically check if PostgreSQL reponds to the \l list query.</p>

<p>Now that we have defined the instructions, lets kick it off:</p>

<pre><code class="language-bash">docker-compose up --build  
</code></pre>

<p>docker-compose waits for the PostgreSQL service to be "healthy" before starting pact broker.</p>

<h3 id="arewehealthythen">Are we healthy then?</h3>

<p>Once you start the container, you will be able to see the health status in the <code>docker ps</code> output.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/06/Screen-Shot-2017-06-12-at-3-03-03-pm.png" alt=""></p>

<p>You can see the health check status of the postgres container is healthy. Pact broker container depends on this container and waits until it is healthy.</p>

<h3 id="howdoweinspectit">How do we inspect it?</h3>

<p>Using  </p>

<pre><code class="language-bash"> docker inspect
</code></pre>

<p>we can view the output from the command.</p>

<pre><code class="language-bash">docker inspect --format "{{json .State.Health.Status }}" pactdemo_postgres_1  
</code></pre>

<pre><code class="language-bash">"healthy"
</code></pre>

<p>You can use <a href="https://stedolan.github.io/jq/">jq</a> if you find the docker inspect command verbose.</p>]]></content:encoded></item><item><title><![CDATA[How To Import and Export Databases in MySQL or MariaDB with Docker]]></title><description><![CDATA[<h1 id="introduction">Introduction</h1>

<p>You can use data dumps for backup and restoration purposes, or you can use them to migrate data to a new server or development environment.</p>

<p>Working with database dumps in MySQL and MariaDB is straightforward. This tutorial will cover how to export the database as well as import it</p>]]></description><link>http://blog.shanelee.name/2017/04/09/how-to-import-and-export-databases-in-mysql-or-mariadb-with-docker/</link><guid isPermaLink="false">293badb0-924e-402f-8236-eee1c16210de</guid><category><![CDATA[mariadb]]></category><category><![CDATA[docker]]></category><category><![CDATA[mysql]]></category><category><![CDATA[sql]]></category><category><![CDATA[dump]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 09 Apr 2017 04:09:25 GMT</pubDate><content:encoded><![CDATA[<h1 id="introduction">Introduction</h1>

<p>You can use data dumps for backup and restoration purposes, or you can use them to migrate data to a new server or development environment.</p>

<p>Working with database dumps in MySQL and MariaDB is straightforward. This tutorial will cover how to export the database as well as import it from a dump file in MySQL and MariaDB.</p>

<p><img src="http://blog.shanelee.name/content/images/2017/04/mariadb.png" alt="MariaDB Icon"></p>

<h1 id="prerequisites">Prerequisites</h1>

<p>To import and/or export a MySQL or MariaDB database, you will need:</p>

<ul>
<li>Access to the Linux server running MySQL or MariaDB</li>
<li>The database name and user credentials for it</li>
</ul>

<h1 id="exportingthedatabase">Exporting the Database</h1>

<p>The mysqldump console utility is used to export databases to SQL text files. These files can easily be transferred and moved around. You will need the database name itself as well as the username and password to an account with privileges allowing at least full read only access to the database.</p>

<p>Export your database using the following command.</p>

<pre><code class="language-bash">mysqldump -u username -p database_name &gt; dump.sql  
</code></pre>

<ul>
<li><p><strong>username</strong> is the username you can log in to the database with</p></li>
<li><p><strong>database_name</strong> is the name of the database that will be exported</p></li>
<li><p><strong>dump.sql</strong> is the file in the current directory that the output will be saved to</p></li>
</ul>

<p>The command will produce no visual output, but you can inspect the contents of sql file to check if it's a legitimate SQL dump file by using:</p>

<pre><code class="language-bash">head -n 5 dump.sql  
</code></pre>

<p>The top of the file should look similar to this, mentioning that it's a mariadb dump for a database named database_name.</p>

<p><strong>SQL dump fragment</strong></p>

<pre><code class="language-bash">-- MySQL dump 10.16  Distrib 10.1.20-MariaDB, for debian-linux-gnu (x86_64)
--
-- Host: localhost    Database: localhost
-- ------------------------------------------------------
-- Server version    10.1.20-MariaDB-1~jessie
</code></pre>

<p>If any errors happen during the export process, mysqldump will print them clearly to the screen instead.</p>

<h1 id="importingthedatabaseintodockercontainer">Importing the Database into docker container</h1>

<p>To import an existing dump file into MySQL or MariaDB, you will have to create the new database. This is where the contents of the dump file will be imported.</p>

<p>I have an existing mariadb docker container running already locally.</p>

<pre><code class="language-bash">docker run --name mariadb -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password1 -e MYSQL_DATABASE=db -d mariadb:latest  
</code></pre>

<p>Now verify it is running</p>

<pre><code class="language-bash">docker ps  
</code></pre>

<p>You will see output similar to below:</p>

<pre><code class="language-bash">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                                            NAMES  
7cfaabda3346        mariadb:latest      "docker-entrypoint..."   5 days ago          Up 5 days           0.0.0.0:3306-&gt;3306/tcp                           mariadb  
</code></pre>

<p>First, start a bash session inside container.</p>

<pre><code class="language-bash">docker exec -it mariadb bash  
</code></pre>

<p>Second, log in to the database as root or another user with sufficient privileges to create new databases.</p>

<pre><code class="language-bash">mysql -u root -ppassword1  
</code></pre>

<p>This will bring you into the mariadb shell prompt. Next, create a new database called new_database.</p>

<pre><code class="language-sql">CREATE DATABASE new_database;  
</code></pre>

<p>Now exit the MySQL shell by pressing CTRL+D. Exit the docker container also.On the normal command line, you can import the dump file with the following command:</p>

<pre><code class="language-bash">docker exec -i mariadb mysql -uroot -ppassword1 --database=new_database &lt; dump.sql  
</code></pre>

<ul>
<li>username is the username you can log in to the database with</li>
<li>new_database is the name of the freshly created database</li>
<li>dump.sql is the data dump file to be imported, located in the current directory</li>
</ul>

<p>The successfully run command will produce no output. If any errors occur during the process, mysql will print them to the terminal instead. You can check that the database was imported by logging in to the MySQL shell again and inspecting the data.</p>

<h1 id="conclusion">Conclusion</h1>

<p>You now know how to create database dumps from MySQL databases as well as how to import them again. mysqldump has multiple additional settings that may be used to alter how the dumps are created, which you can learn more about from the <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html">official mysqldump documentation page</a>.</p>

<p>To find out more about docker commands check out the docker doc <a href="https://docs.docker.com/engine/reference/commandline/exec/">here</a></p>]]></content:encoded></item><item><title><![CDATA[Serverless and scheduled lambda function]]></title><description><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Continuing on from my previous post, I decided to try out serverless to build my scheduled lambda function.</p>

<blockquote>
  <p>Serverless is an open-source, application framework to easily build serverless architectures on AWS Lambda.</p>
</blockquote>

<p>The Serverless Framework is the world’s leading development framework for building serverless architectures.</p>

<h2 id="australiapubliccalendarcustomalexaskill">Australia public</h2>]]></description><link>http://blog.shanelee.name/2016/12/17/serverless-and-scheduled-lambda-function/</link><guid isPermaLink="false">337574ed-d9ac-4efb-a0ac-a8a248ef6545</guid><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[serverless]]></category><category><![CDATA[alexa]]></category><category><![CDATA[node]]></category><category><![CDATA[australia]]></category><category><![CDATA[calendar]]></category><category><![CDATA[dot]]></category><category><![CDATA[amazon echo]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 17 Dec 2016 10:19:04 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Continuing on from my previous post, I decided to try out serverless to build my scheduled lambda function.</p>

<blockquote>
  <p>Serverless is an open-source, application framework to easily build serverless architectures on AWS Lambda.</p>
</blockquote>

<p>The Serverless Framework is the world’s leading development framework for building serverless architectures.</p>

<h2 id="australiapubliccalendarcustomalexaskill">Australia public calendar custom alexa skill</h2>

<p><img src="http://blog.shanelee.name/content/images/2016/12/AustraliaCalendar.png" alt="Amazon Alexa"></p>

<p>You can find the skill on the amazon marketplace <a href="https://www.amazon.com/shane-Australia-Public-Holiday-Calendar/dp/B01N0MY1B6/ref=cm_rdp_product">here</a></p>

<h2 id="quickrecap">Quick recap</h2>

<p>There is the pattern leveraging calendar files where the data is relatively static, and it’s more important to organize it in such a manner that allows navigation through voice commands. There's an ongoing process that can refresh it over time. Going deeper, let's explore the following utterance.</p>

<blockquote>
  <p>“Alexa, ask Australia Calendar to find next public holiday by state Victoria”</p>
</blockquote>

<p>The dialog with this question will be determining what calendar file to get. In this example, the data is static (there aren't new holidays being created every day), and the interaction will be around navigating a list of calendar events.</p>

<p>In this use case, we can invoke the API ahead of an individual user request, and organize and cache the data in s3 bucket. This improves performance, and simplifies the runtime model. Here’s a view of how this looks using the websites calendar files and how the data is staged.</p>

<p><img src="http://blog.shanelee.name/content/images/2016/12/AustraliaCalendarFlow--1-.svg" alt=""></p>

<p>An S3 bucket is used to store the data, and is persisted in a ical file object. Given the durability of S3, this ensures that the data is always accessible at runtime for the skill and we don't have to hammer the website again and again for what the types of calendar events are in each state.</p>

<h2 id="scheduledlambdafunction">Scheduled lambda function</h2>

<p>Below I will define the steps needed to build my scheduled function using serverless.</p>

<h3 id="installation">Installation</h3>

<p>Install serverless using npm <br>
<code>
npm install serverless -g <br>
</code></p>

<p>The current version is 1.4.0</p>

<p>Now create a new node.js service via the command below: <br>
<code>
serverless create --template aws-nodejs <br>
</code></p>

<p>Here you have defined the runtime to be nodejs. Now for my function I want it to run every 7 days and to put the ical files onto s3 bucket.</p>

<p>You will see in the serverless.yml I have defined the schedule and resource access.</p>

<pre><code class="language-yaml"># Welcome to Serverless!
#
# This file is the main config file for your service.
# It's very minimal at this point and uses default values.
# You can always add more config options for more control.
# We've included some commented out config examples here.
# Just uncomment any of them to get that config option.
#
# For full config options, check the docs:
#    docs.serverless.com
#
# Happy Coding!

service: aus-calendar-cron # NOTE: update this with your service name

provider:  
  name: aws
  runtime: nodejs4.3

  iamRoleStatements:
    - Effect: Allow
      Action:
        - s3:*
      Resource: "*"


# you can define service wide environment variables here
environment:  
   BUCKET: slee-calendar

functions:  
  cron:
    handler: handler.hello

#    The following are a few example events you can configure
#    NOTE: Please make sure to change your handler code to work with those events
#    Check the event documentation for details
    events:
     - schedule: rate(7 days)


#    Define function environment variables here
    environment:
       BUCKET: slee-calendar
</code></pre>

<p>Once I defined the handler logic, all I had to do then is call:</p>

<p><code>
serverless deploy --verbose <br>
</code></p>

<h3 id="howitworks">How it works</h3>

<ul>
<li>An AWS CloudFormation template is created from your serverless.yml.</li>
<li>If a Stack has not yet been created, then it is created with no resources except for an S3 Bucket, which will store zip files of your Function code.</li>
<li>The code of your Functions is then packaged into zip files.</li>
<li>Zip files of your Functions' code are uploaded to your Code S3 Bucket.</li>
<li>Any IAM Roles, Functions, Events and Resources are added to the AWS CloudFormation template.</li>
<li>The CloudFormation Stack is updated with the new CloudFormation template.</li>
<li>Each deployment publishes a new version for each function in your service.</li>
</ul>

<p>To test your function out you can then run <br>
<code>
serverless invoke --function cron --log <br>
</code></p>

<p>It allows to send event data to the function, read logs and display other important information of the function invocation.</p>

<p>And voilà. Thats your scheduled lambda function up and running! 😋</p>

<p>Stay tuned for more!!</p>]]></content:encoded></item><item><title><![CDATA[Alexa, ask australia calendar for next public holiday in state Victoria]]></title><description><![CDATA[Alexa skill to find next public holiday by state in Australia. ]]></description><link>http://blog.shanelee.name/2016/12/13/alexa-ask-australia-calendar-for-next-public-holiday-in-state-victoria/</link><guid isPermaLink="false">5e87a980-7a41-41be-a620-f0218a7ae396</guid><category><![CDATA[alexa]]></category><category><![CDATA[node]]></category><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[australia]]></category><category><![CDATA[calendar]]></category><category><![CDATA[dot]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 13 Dec 2016 08:52:13 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>Voice interfaces are taking off, but how advanced are they becoming? Are we at a point where they can become an automated agent, allowing us to put down our keypads and have a dialog just as if they were a friend or trusted colleague? I’ve set out to test this using Alexa, and building a custom skill (that  recently got certified on amazon marketplace).</p>

<p><img src="http://blog.shanelee.name/content/images/2016/12/AustraliaCalendar.png" alt="Amazon Alexa"></p>

<p>You can find the skill on the amazon marketplace <a href="https://www.amazon.com/shane-Australia-Public-Holiday-Calendar/dp/B01N0MY1B6/ref=cm_rdp_product">here</a></p>

<h2 id="alexamachinelearning">Alexa Machine Learning</h2>

<p>Amazon has enabled software developers to write custom skills that are published to the Alexa platform. These custom skills act as an application that can be invoked from the voice interface similar to basic features that come with the Alexa. </p>

<p>Here is an example of a phrase that invokes the Australia calendar application.  </p>

<blockquote>
  <p>Alexa, ask Australia Calendar whats the next holiday in state Victoria?</p>
</blockquote>

<p>The Alexa Skills Kit establishes the framework for how the powerful machine learning algorithms that Amazon has developed can be leveraged. <br>
A key concept within the platform is around the learning algorithms, and teaching Alexa what “might” be the expected outcome of a particular phrase. A simple example to understand this pattern is the following.</p>

<blockquote>
  <p>State {AUSPOST_STATE-SLOT}</p>
</blockquote>

<p>In Alexa terminology, the overall phrase is called an “Utterance”, and what’s in the brackets is referred to as a “Slot”. So when someone states “State Victoria” or “State Queensland”, both of these have the same intent, it’s just that the state is a variable that is defined in the slot. When modeling the application, the developer will establish possible choices that might be in the slot (i.e. Victoria, Tasmania, Queensland etc.) <br>
This structure is outlined by the developer as part of writing a custom skill, and once approved by the Alexa team, the model (including custom slots) is ingested into the platform which then influences the algorithms. This type of teaching is common in machine learning, and is useful for establishing patterns.</p>

<p>Once Alexa deciphers the spoken word, it translates into one of these patterns, then invokes the API provided by the developer, passing over which pattern was uttered, along with any variables from slots.</p>

<p>More on custom slots can be seen <a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/migrating-to-the-improved-built-in-and-custom-slot-types">here</a></p>

<p>When authoring an Alexa Skill (the custom “app”) the challenge is how to establish all the different ways in which a question or statement could be made, then building custom slots for variables within the utterance. That’s where the engineering comes in by the developer (me!) to take advantage of the underlying machine learning in the Alexa platform. If the pattern matching isn’t effective, that’s a problem with the machine learning and the platform itself.</p>

<h2 id="voiceuserinterfacedesignthoughtfulcodewritingmatters">Voice User Interface Design (Thoughtful code writing matters)</h2>

<p>Once the processing is done on Alexa, an API call is made to a micro-service developed by me using aws lambda (serverless compute service). The quality of the user interaction is very dependent upon how “flexible” the skill is written, and that effort has been put into understanding the difference between a visual/keyboard interface and a voice driven one.</p>

<h2 id="alexaflow">Alexa flow</h2>

<p>Trying to create a good skill requires picking some potential flows that a narrative may go through between a user and Alexa. The current version of the skill splits into one direction.</p>

<h3 id="architectingthesolution">Architecting the solution</h3>

<p>There is the pattern leveraging calendar files where the data is relatively static, and it’s more important to organize it in such a manner that allows navigation through voice commands. There's an ongoing process that can refresh it over time. Going deeper, let's explore the following utterance.</p>

<blockquote>
  <p>“Alexa, ask Australia Calendar to find next public holiday by state Victoria”</p>
</blockquote>

<p>The dialog with this question will be determining what calendar file to get. In this example, the data is static (there aren't new holidays being created every day), and the interaction will be around navigating a list of calendar events.</p>

<p>In this use case, we can invoke the API ahead of an individual user request, and organize and cache the data in s3 bucket. This improves performance, and simplifies the runtime model. Here’s a view of how this looks using the websites calendar files and how the data is staged.</p>

<p><img src="http://blog.shanelee.name/content/images/2016/12/AustraliaCalendarFlow--1-.svg" alt=""></p>

<p>An S3 bucket is used to store the data, and is persisted in a ical file object. Given the durability of S3, this ensures that the data is always accessible at runtime for the skill and we don't have to hammer the website again and again for what the types of calendar events are in each state.</p>

<h2 id="buildingtheskill">Building the Skill</h2>

<p>The code is organized into a series of functions that are invoked based on the intent. Here are sample mappings from utterances to intent for this skill.</p>

<pre><code class="language-javascript">GetNextHolidayIntent what is the next holiday for {State}  
GetNextHolidayIntent what is the next holiday for state {State}  
GetNextHolidayIntent what is the next public holiday for {State}  
GetNextHolidayIntent what is the next public holiday for state {State}  
GetNextHolidayIntent find next public holiday in state {State}  
GetNextHolidayIntent find next public holiday by state {State}  
GetNextHolidayIntent find next public holiday by state
</code></pre>

<p>Each of the intents have their own functions, and then gather the data for the response by leveraging cached data in the local memory of the skill, or by calling out to a S3 bucket. Here's a mapping of where the data is retrieved from:</p>

<table>  
  <tr>
    <th>Intent</th>
    <th>Function</th>
    <th>Data</th>
  </tr>
  <tr>
    <td>GetNextHolidayIntent</td>
    <td>GetNextHolidayIntent()</td>
    <td>S3 Bucket</td>
  </tr>
</table>

<h2 id="homecards">Home Cards</h2>

<p>Interactions between a user and an Alexa device can include home cards displayed in the Amazon Alexa App, the companion app available for Fire OS, Android, iOS, and desktop web browsers. These are graphical cards that describe or enhance the voice interaction. A custom skill can include these cards in its responses.</p>

<p>I have created a card for the returned holiday response for future reference. You can see an example below:</p>

<p><img src="http://blog.shanelee.name/content/images/2016/12/Screen-Shot-2016-12-13-at-6-09-21-pm.png" alt=""></p>

<p>Stay tuned for more skills!!</p>]]></content:encoded></item><item><title><![CDATA[Alexa, ask auspost for post offices by postcode]]></title><description><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>I will describe below my custom alexa skill.</p>

<h2 id="amazonalexa">Amazon ALexa</h2>

<p><img src="http://blog.shanelee.name/content/images/2016/11/26452717290_b51f1a1f04_z-1.jpg" alt="Amazon Alexa"></p>

<p>Recently, I have started developing alexa skills.</p>

<p>Alexa, the voice service that powers Echo, provides capabilities, or skills, that enable customers to interact with devices in a more intuitive way using voice. Examples of these skills include the</p>]]></description><link>http://blog.shanelee.name/2016/11/15/alexa-ask-auspost-for-post-offices-by-postcode/</link><guid isPermaLink="false">fcfc1f6e-dfe2-4002-83a4-2de0af5720e5</guid><category><![CDATA[alexa]]></category><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[node]]></category><category><![CDATA[amazon]]></category><category><![CDATA[amazon echo]]></category><category><![CDATA[serverless]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 15 Nov 2016 11:53:25 GMT</pubDate><content:encoded><![CDATA[<h1 id="tldr">TL;DR</h1>

<p>I will describe below my custom alexa skill.</p>

<h2 id="amazonalexa">Amazon ALexa</h2>

<p><img src="http://blog.shanelee.name/content/images/2016/11/26452717290_b51f1a1f04_z-1.jpg" alt="Amazon Alexa"></p>

<p>Recently, I have started developing alexa skills.</p>

<p>Alexa, the voice service that powers Echo, provides capabilities, or skills, that enable customers to interact with devices in a more intuitive way using voice. Examples of these skills include the ability to play music, answer general questions, set an alarm or timer and more.</p>

<p>Amazon says:</p>

<blockquote>
  <p>Natural user interfaces, like those based on speech, represent the next major disruption in computing.</p>
</blockquote>

<p>You can now find all alexa skills on amazon.com. See the <a href="https://www.amazon.com/b?ie=UTF8&amp;node=13727921011">experience</a></p>

<p>To see some examples of skills to use check out this <a href="http://www.wired.co.uk/article/the-best-features-of-amazon-echo">article</a></p>

<p>There is now over 3000 skills in the marketplace and growing <a href="https://techcrunch.com/2016/09/13/amazons-alexa-app-store-hits-3000-skills-up-from-1000-in-june/">fast</a></p>

<h2 id="skill">Skill</h2>

<p><img src="http://blog.shanelee.name/content/images/2016/11/noun_666106_cc-2.png" alt="Auspost post office locations"></p>

<p>I decided to build a skill that allows consumers of an alexa enabled device to find post offices by post code in Australia.</p>

<h2 id="alexaflow">Alexa flow</h2>

<p><img src="http://blog.shanelee.name/content/images/2016/11/alexaFlow.svg" alt=""></p>

<h2 id="interactwithauspostskill">Interact with auspost skill</h2>

<p>Now the fun part... 😛</p>

<p>Use the service simulator to test it out <a href="https://echosim.io/">here</a> or use an amazon echo or dot.</p>

<blockquote>
  <p>Alexa, open auspost</p>
</blockquote>

<p>Alexa will guide you in interacting with the skill.</p>

<p>Sample utterances are:</p>

<p>Try to find post offices by post code</p>

<blockquote>
  <p>Alexa, ask auspost to find post offices by postcode</p>
</blockquote>

<p>Follow the dialog flow to get post offices in your vicinity and the opening hours for today.</p>

<p>Here is a sneak preview!!</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Lf1UUJm0R4M" frameborder="0" allowfullscreen></iframe>

<h3 id="homecards">Home Cards</h3>

<p>Interactions between a user and an Alexa device can include home cards displayed in the Amazon Alexa App, the companion app available for Fire OS, Android, iOS, and desktop web browsers. These are graphical cards that describe or enhance the voice interaction. A custom skill can include these cards in its responses.</p>

<p>I have created a card for the post offices in the response for future reference. You can see an example below:</p>

<p><img src="http://blog.shanelee.name/content/images/2016/11/Screenshot_20161115-230725-1.png" alt=""></p>

<h3 id="disclaimer">Disclaimer</h3>

<p>This skill is not currently on the marketplace.</p>

<p>Stay tuned for more skills!!</p>]]></content:encoded></item><item><title><![CDATA[Alexa, ask home theater to put on Mr. Robot]]></title><description><![CDATA[Integrate alexa skill with plex media server using serverless technology aws lambda (FAAS)]]></description><link>http://blog.shanelee.name/2016/09/06/alexa-ask-home-theater-to-put-on-mr-robot/</link><guid isPermaLink="false">49266c7f-733b-46eb-b62f-0b73847f2b46</guid><category><![CDATA[alexa]]></category><category><![CDATA[plex]]></category><category><![CDATA[lambda]]></category><category><![CDATA[aws]]></category><category><![CDATA[node]]></category><category><![CDATA[dynamodb]]></category><category><![CDATA[amazon]]></category><category><![CDATA[amazon echo]]></category><category><![CDATA[serverless]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 06 Sep 2016 10:45:03 GMT</pubDate><content:encoded><![CDATA[<h1 id="plexforalexa">Plex for Alexa</h1>

<p><img src="http://blog.shanelee.name/content/images/2016/09/maxresdefault.jpg" alt=""></p>

<p>Recently, I have started playing around with Alexa skill services. I currently use plex for my media content and found a project on github that has built an alexa skill already. (<strong>thanks to @overloadut</strong>)</p>

<p><a href="https://overloadut.github.io/alexa-plex/">https://overloadut.github.io/alexa-plex/</a></p>

<p>As a pet project, I decided to get this up and running.</p>

<p>Below I will go into detail on the steps involved. </p>

<h1 id="process">Process</h1>

<p><img src="http://blog.shanelee.name/content/images/2016/09/PlexToAlexa.png" alt="Alexa flow"></p>

<h3 id="prerequisites">Prerequisites</h3>

<ul>
<li>Plex media server running</li>
<li>AWS account</li>
<li>Plex Home theater (PHT) client </li>
<li>Have basic knowledge of aws services, nodeJS and setting IAM roles.</li>
<li><a href="https://support.plex.tv/hc/en-us/articles/200484543-Enabling-Remote-Access-for-a-Server">Enable remote access for Plex</a></li>
</ul>

<h3 id="locally">Locally</h3>

<p>Install node, npm and git</p>

<p>Clone the github repo <a href="https://github.com/OverloadUT/alexa-plex">https://github.com/OverloadUT/alexa-plex</a></p>

<p>Run  </p>

<pre><code class="language-bash">npm install  
</code></pre>

<p>to install dependencies
Create a .env file in project root with similar properties as below:</p>

<pre><code class="language-bash">APP_PRODUCT=Alexa Plex  
APP_VERSION=2.0  
APP_DEVICE=Amazon Echo  
APP_DEVICE_NAME=Alexa  
APP_IDENTIFIER=  
ALEXA_APP_ID=  
AWS_ACCESS_KEY_ID=&lt;your own key&gt;  
AWS_SECRET_ACCESS_KEY=&lt;your own secret key&gt;  
</code></pre>

<p>There is a deploy script at the root of the project. Before you run this and update your lambda function, you need to first create it.</p>

<h3 id="awslambda">AWS Lambda</h3>

<p><img src="http://blog.shanelee.name/content/images/2016/09/AWS_Simple_Icons_Compute_AWSLambda-svg--1-.png" alt=""></p>

<p>Create IAM role that has access to lambda and to dynamodb. </p>

<p>If you have aws cli setup try the sample script below. <br>
<strong><em>NB: Make sure and change the role val.</em></strong></p>

<pre><code class="language-bash">aws lambda create-function --function-name alexa-plex --region us-east-1 --runtime  nodejs4.3 --role arn:aws:iam::xxxxxx:role/lambda_basic_execution --handler  index.handler  
</code></pre>

<p>Verify you can view the lambda function in the aws console. </p>

<p>Change to trigger to be "Alexa skills kit".</p>

<h3 id="dynamodb">DynamoDB</h3>

<p><img src="http://blog.shanelee.name/content/images/2016/09/AWS_Simple_Icons_Database_AmazonDynamoDB-svg.png" alt=""></p>

<p>Create a table named <em>AlexaPlexUsers</em> with a primary string key of <em>userid</em>.</p>

<h3 id="createalexaskill">Create alexa skill</h3>

<p><img src="http://blog.shanelee.name/content/images/2016/09/icon_ask_120x120-_CB295607968_--1-.png" alt=""></p>

<p>Navigate to <a href="https://developer.amazon.com/">Amazon Developer Portal</a> to begin the Alexa Skill process. </p>

<p>Sign In >> Alexa >> Alexa Skills Kit >> Add new skill</p>

<h4 id="skillinformation">Skill Information</h4>

<p>Fill in the Alexa Skill store details. Define the invocation name as home theater.  </p>

<h4 id="interactionmodel">Interaction Model</h4>

<p>This will setup what commands "Plex" can process. The sample utterances enable Alexa to process a variety of phrases for the same interaction.</p>

<p>Copy in the intent schema, sample utterances and custom slot. The files are located under directory ask_configuration. </p>

<h4 id="configuration">Configuration</h4>

<p>You'll want to fill in your Lambda <strong>ARN</strong> here. <br>
Set Account Linking to No.</p>

<h4 id="test">Test</h4>

<p>Now the fun part... 😛</p>

<p>Use the service simulator to test it out.</p>

<p>You'll need to link Alexa to your Plex account. Open your web browser to <a href="http://plex.tv/link">http://plex.tv/link</a> and ask Alexa:</p>

<blockquote>
  <p>"Alexa, tell home theater to continue setup"</p>
</blockquote>

<p>Alexa will guide you through the steps to link your account. Simply keep saying that same command and it should move to the next step.</p>

<p>Once all is good, try playing a tv show</p>

<blockquote>
  <p>"Alexa, tell home theater to play Mr. Robot"</p>
</blockquote>

<h2 id="troubleshooting">Troubleshooting</h2>

<ul>
<li><p>If you receive no response from lambda function, make sure you increase the timeout greater than ten seconds.</p></li>
<li><p>Any errors in the function, review the cloudwatch logs.</p></li>
<li><p>I had an issue where the default player was Plex Web for Chrome and not my Rasplex.
So I needed to change this entry in the dynamodb table. <br>
You will see there is a player object. You need to change the machine identifier to the client that is running plex home theater. How you find that out, is by logging into plex.tv, launching the webapp and inspecting the network requests. <br>
You can do this in Chrome by launching developer tools. You will see a XHR request similar to  <a href="https://plex.tv/devices.xml">https://plex.tv/devices.xml</a>. If you review the response you will find the client identifier value for the device you need. For me it was rasplex.</p></li>
</ul>

<h2 id="versions">Versions</h2>

<ul>
<li>PMS Version 1.0.3.2461  </li>
<li>Rasplex Version 1.6.2.123-e23a7eef</li>
</ul>

<p><strong>TODO</strong>
Add in video showing how it plays</p>]]></content:encoded></item><item><title><![CDATA[Consumer driven contract testing using PACT]]></title><description><![CDATA[pact framework to implement consumer driven contract testing against internal restful APIs]]></description><link>http://blog.shanelee.name/2016/07/19/consumer-driven-contract-testing-using-pact/</link><guid isPermaLink="false">1b17dbf5-5c11-4bfd-b1b7-18199f9e7fbd</guid><category><![CDATA[docker]]></category><category><![CDATA[docker-compose]]></category><category><![CDATA[pact]]></category><category><![CDATA[contract]]></category><category><![CDATA[testing]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 19 Jul 2016 11:26:24 GMT</pubDate><content:encoded><![CDATA[<h1 id="allaboutpact">All about PACT....</h1>

<p><img src="https://tech.affinitas.de/wp-content/uploads/2015/04/pact_two_parts.png" alt="The PACT Logo"></p>

<p>PACT enables consumer driven contract testing, providing a mock service and DSL for the consumer project, and interaction playback and verification for the service provider project.</p>

<p>The Pact family of testing frameworks (Pact-JVM, Pact Ruby, Pact .NET, Pact Go, Pact.js, Pact Swift etc.) provide support for Consumer Driven <br>
Contract Testing between dependent systems where the integration is based on HTTP (or message queues for some of the implementations).</p>

<p>See <a href="https://github.com/realestate-com-au/pact/wiki#implementations-in-other-languages">implementations</a></p>

<h2 id="dockerise">Dockerise...</h2>

<p><img src="https://www.docker.com/sites/default/files/docker_banner_image_12312.svg" alt="Docker"></p>

<p>To host your pacts, you need a pact broker. <br>
The Pact Broker provides a repository for consumer driven contracts created using the pact gem.</p>

<p>It:</p>

<ul>
<li>solves the problem of how to share pacts between consumer and provider projects</li>
<li>allows you to decouple your service release cycles</li>
<li>provides API documentation that is guaranteed to be up-to date</li>
<li>shows you real examples of how your services interact</li>
<li>allows you to visualise the relationships between your services </li>
</ul>

<p>There is a docker image already for pact broker, so i decided to create an image for the datastore; postgres. </p>

<p>You can find the image <a href="https://hub.docker.com/r/shanelee007/docker-pact-postgres/">here</a></p>

<p>I also defined a docker compose file to <em>orchestrate</em> starting the two containers within the same network. </p>

<p>Docker Compose is an orchestration tool that makes spinning up multi-container applications effortless.</p>

<p>See the file below:</p>

<pre><code class="language-Docker">version: '2'  
services:  
  postgres:
    image: shanelee007/docker-pact-postgres
    environment:
      -  POSTGRES_PASSWORD=ThePostgresPassword
      -  POSTGRES_USER=admin
    ports:
      - "5432:5432"
  pact:
    image: dius/pact_broker
    environment:
      -  PACT_BROKER_DATABASE_NAME=pactbroker
      -  PACT_BROKER_DATABASE_PASSWORD=TheUserPassword
      -  PACT_BROKER_DATABASE_HOST=postgres
      -  PACT_BROKER_DATABASE_USERNAME=pactbrokeruser
    ports:
      - "80:80"
    depends_on:
        - postgres
#    entrypoint: ./wait-for-it.sh postgres:5432 -- echo "postgres is up"
</code></pre>

<p>Docker command to run is  </p>

<pre><code class="language-bash">docker-compose --file docker-compose-pact.yml  up --build
</code></pre>

<p>One simple command and you have a pact broker up and running locally at <a href="http://localhost/ui/relationships">http://localhost/ui/relationships</a> 😎</p>

<h2 id="acloserlookatpact">A closer look at PACT</h2>

<p>I have created a github project that demonstrates Pact end-to-end.</p>

<p>You can find the project <a href="https://github.com/shavo007/pact-demo">here</a></p>

<p>Steps are:</p>

<ul>
<li>Spin up pact broker using docker containers</li>
<li>Start the microservice</li>
<li>Run js consumer test using pact mock service and publish to pact broker</li>
<li>Run jvm consumer and publish to pact broker</li>
</ul>

<p>You should then see the contracts published at <br>
<a href="http://localhost/ui/relationships">http://localhost/ui/relationships</a></p>

<h3 id="pactbrokerrelationships">Pact broker relationships</h3>

<p><img src="http://blog.shanelee.name/content/images/2016/07/Screen-Shot-2016-07-30-at-8-18-09-PM.png" alt="Pact broker"></p>

<p>You can then verify the contracts against the running microservice by running  </p>

<pre><code class="language-bash">./gradlew pactVerify
</code></pre>

<p><img src="http://blog.shanelee.name/content/images/2016/07/Screen-Shot-2016-07-30-at-8-03-16-PM.png" alt="Microservice console"></p>

<h3 id="consoleverification">Console verification</h3>

<p><img src="http://blog.shanelee.name/content/images/2016/07/Screen-Shot-2016-07-30-at-8-20-00-PM.png" alt=""></p>

<p>Reports are outputted in json and markdown form. Which is very useful in your CI build pipeline.  </p>

<h2 id="benefits">Benefits</h2>

<p>Pact is most valuable for designing and testing integrations where you (or your team/organisation/partner organisation) control the development of both the consumer and the provider, and the requirements of the consumer are going to be used to drive the features of the provider. It is a fantastic tool for developing and testing intra-organisation microservices.</p>

<h2 id="usefulresources">Useful resources</h2>

<ul>
<li><p><a href="http://dius.com.au/2014/05/19/simplifying-micro-service-testing-with-pacts/">http://dius.com.au/2014/05/19/simplifying-micro-service-testing-with-pacts/</a></p></li>
<li><p><a href="http://thoughtworks.github.io/pacto/patterns/cdc/">http://thoughtworks.github.io/pacto/patterns/cdc/</a></p></li>
<li><p><a href="http://docs.pact.io/">http://docs.pact.io/</a></p></li>
<li><p>Recently discussed at microservice meetup : <a href="http://www.meetup.com/Melbourne-Microservices/events/231874304/">http://www.meetup.com/Melbourne-Microservices/events/231874304/</a></p></li>
<li><p><a href="http://martinfowler.com/articles/microservice-testing/#testing-contract-introduction">http://martinfowler.com/articles/microservice-testing/#testing-contract-introduction</a></p></li>
</ul>]]></content:encoded></item><item><title><![CDATA[Elasticsearch curator 4.0 and docker]]></title><description><![CDATA[elasticsearch curator 4.0 and docker]]></description><link>http://blog.shanelee.name/2016/06/30/elasticsearch-curator-4-0-and-docker-2/</link><guid isPermaLink="false">ee2b6929-ab27-4c85-91d9-17548cbe95d0</guid><category><![CDATA[docker]]></category><category><![CDATA[elasticsearch]]></category><category><![CDATA[curator]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Thu, 30 Jun 2016 10:45:54 GMT</pubDate><content:encoded><![CDATA[<p><img src="http://blog.shanelee.name/content/images/2016/07/icon-elasticsearch-3.svg" alt="The Elasticsearch Logo"></p>

<p>Recently, I have worked on elasticsearch. There is a very useful tool called curator. </p>

<blockquote>
  <p>Elasticsearch Curator helps you curate, or manage, your Elasticsearch indices and snapshots.</p>
</blockquote>

<p>They recently released a new version of curator. You can find the blog post <a href="https://www.elastic.co/blog/curator_v4_release">here</a></p>

<h2 id="dockerise">Dockerise...</h2>

<p><img src="https://www.docker.com/sites/default/files/Engine.png" alt="The Docker Logo"></p>

<p>Based on this post, I set myself a challenge to build my first docker image. The base of this image is apache alpine (which is very lightweight)</p>

<p>You can find it on <a href="https://hub.docker.com/r/shanelee007/docker-es-curator-cron/">dockerhub</a></p>

<h3 id="tryitout">Try it out</h3>

<pre><code class="language-bash">docker run shanelee007/docker-es-curator-cron  
</code></pre>

<p>You can mount your own configuration files if you want:  </p>

<pre><code class="language-bash"> docker run -d -v "$PWD/config":/usr/share/curator/config shanelee007/curator4
</code></pre>

<p>The dockerfile is below:</p>

<pre><code class="language-Docker">FROM alpine:latest


RUN apk --update add python py-pip &amp;&amp; \  
    pip install elasticsearch-curator &amp;&amp; \
     rm -rf /var/cache/apk/*

ADD entrypoint.sh /entrypoint.sh

WORKDIR /usr/share/curator  
COPY config ./config

RUN chmod +x /entrypoint.sh

#run every minute
ENV CRON */1  *  *  * *  
ENV ES_HOST 127.0.0.1  
ENV CONFIG_FILE /usr/share/curator/config/curator.yml  
ENV COMMAND /usr/share/curator/config/delete_log_files_curator.yml

ENTRYPOINT ["/entrypoint.sh"]
</code></pre>

<p>Try it out and any questions or feedback, please feel free to comment. </p>]]></content:encoded></item><item><title><![CDATA[ElasticSearch Part1]]></title><link>http://blog.shanelee.name/2016/06/07/elasticsearch-part1/</link><guid isPermaLink="false">8b880ffb-97d6-4d39-9f7f-6984681c2afe</guid><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 07 Jun 2016 12:37:03 GMT</pubDate><content:encoded/></item><item><title><![CDATA[Playing around with Docker for Mac Beta]]></title><link>http://blog.shanelee.name/2016/06/07/playing-around-with-docker-for-mac-beta/</link><guid isPermaLink="false">b1525dc3-280b-4d6d-a109-76f78244c295</guid><category><![CDATA[docker]]></category><category><![CDATA[mac]]></category><category><![CDATA[beta]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Tue, 07 Jun 2016 12:33:12 GMT</pubDate><content:encoded/></item><item><title><![CDATA[Building a microservice using docker]]></title><description><![CDATA[tutorial on building a spring boot app using docker]]></description><link>http://blog.shanelee.name/2016/06/05/building-a-microservice-using-docker/</link><guid isPermaLink="false">efc343c5-2778-4902-bbea-d336184f2d14</guid><category><![CDATA[docker]]></category><category><![CDATA[spring]]></category><category><![CDATA[spring-boot]]></category><category><![CDATA[docker-compose]]></category><category><![CDATA[elasticsearch]]></category><category><![CDATA[kibana]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sun, 05 Jun 2016 11:14:28 GMT</pubDate><content:encoded/></item><item><title><![CDATA[Welcome to Ghost]]></title><description><![CDATA[<p>You're live! Nice. We've put together a little post to introduce you to the Ghost editor and get you started. You can manage your content by signing in to the admin area at <code>&lt;your blog URL&gt;/ghost/</code>. When you arrive, you can select this post from a list</p>]]></description><link>http://blog.shanelee.name/2016/06/04/welcome-to-ghost/</link><guid isPermaLink="false">bd6cd519-fce9-4139-b2bb-bd1bd346c59a</guid><category><![CDATA[Getting Started]]></category><dc:creator><![CDATA[Shane Lee]]></dc:creator><pubDate>Sat, 04 Jun 2016 06:24:13 GMT</pubDate><content:encoded><![CDATA[<p>You're live! Nice. We've put together a little post to introduce you to the Ghost editor and get you started. You can manage your content by signing in to the admin area at <code>&lt;your blog URL&gt;/ghost/</code>. When you arrive, you can select this post from a list on the left and see a preview of it on the right. Click the little pencil icon at the top of the preview to edit this post and read the next section!</p>

<h2 id="gettingstarted">Getting Started</h2>

<p>Ghost uses something called Markdown for writing. Essentially, it's a shorthand way to manage your post formatting as you write!</p>

<p>Writing in Markdown is really easy. In the left hand panel of Ghost, you simply write as you normally would. Where appropriate, you can use <em>shortcuts</em> to <strong>style</strong> your content. For example, a list:</p>

<ul>
<li>Item number one</li>
<li>Item number two
<ul><li>A nested item</li></ul></li>
<li>A final item</li>
</ul>

<p>or with numbers!</p>

<ol>
<li>Remember to buy some milk  </li>
<li>Drink the milk  </li>
<li>Tweet that I remembered to buy the milk, and drank it</li>
</ol>

<h3 id="links">Links</h3>

<p>Want to link to a source? No problem. If you paste in a URL, like <a href="http://ghost.org">http://ghost.org</a> - it'll automatically be linked up. But if you want to customise your anchor text, you can do that too! Here's a link to <a href="http://ghost.org">the Ghost website</a>. Neat.</p>

<h3 id="whataboutimages">What about Images?</h3>

<p>Images work too! Already know the URL of the image you want to include in your article? Simply paste it in like this to make it show up:</p>

<p><img src="https://ghost.org/images/ghost.png" alt="The Ghost Logo"></p>

<p>Not sure which image you want to use yet? That's ok too. Leave yourself a descriptive placeholder and keep writing. Come back later and drag and drop the image in to upload:</p>

<h3 id="quoting">Quoting</h3>

<p>Sometimes a link isn't enough, you want to quote someone on what they've said. Perhaps you've started using a new blogging platform and feel the sudden urge to share their slogan? A quote might be just the way to do it!</p>

<blockquote>
  <p>Ghost - Just a blogging platform</p>
</blockquote>

<h3 id="workingwithcode">Working with Code</h3>

<p>Got a streak of geek? We've got you covered there, too. You can write inline <code>&lt;code&gt;</code> blocks really easily with back ticks. Want to show off something more comprehensive? 4 spaces of indentation gets you there.</p>

<pre><code>.awesome-thing {
    display: block;
    width: 100%;
}
</code></pre>

<h3 id="readyforabreak">Ready for a Break?</h3>

<p>Throw 3 or more dashes down on any new line and you've got yourself a fancy new divider. Aw yeah.</p>

<hr>

<h3 id="advancedusage">Advanced Usage</h3>

<p>There's one fantastic secret about Markdown. If you want, you can write plain old HTML and it'll still work! Very flexible.</p>

<p><input type="text" placeholder="I'm an input field!"></p>

<p>That should be enough to get you started. Have fun - and let us know what you think :)</p>]]></content:encoded></item></channel></rss>
